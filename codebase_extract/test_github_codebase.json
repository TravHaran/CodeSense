{
    "name": "",
    "type": "folder",
    "keywords": [],
    "children": [
        {
            "name": "README.md",
            "type": "file",
            "keywords": [],
            "annotation": "",
            "content": "# Project Codesense\n\n## Breakdown\n\n### 1. CodeBase Tree Extraction\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\n### 2. Call Graph Extraction\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\n### 3. Annotation Generation\n    - for a fucntion defined in code generate a text summarization\n### 4. Annotation Aggregation\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\n### 5. Keyword Extraction\n    - from the aggregated annotation report extract a list of keywords\n    - from a usery query extract a list of keywords\n### 6. Tree Traversal\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\n    - once the target node has been found, return it's corresponding aggregated annotation report\n### 7. Question Answering\n    - given the aggregated annoation report as context, provide an answer to the user's query."
        },
        {
            "name": "annotation_aggregate",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "annotation_aggregate.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import sys\n\nsys.path.insert(0, \"..\")\nfrom utilities.utility import json_to_obj\n\n'''\nCreate a class to aggregate the annotations of some target nodes\n- input:\n    - search_result object containing most relevant nodes with annotations \n- output:\n    - an aggregate of all the relevant annotations in string format\n    - optionally save output as txt file\n'''\n\nclass AnnotationAggregate:\n    def __init__(self, traverse_obj):\n        self.result_model = traverse_obj\n        self.annotations = []\n    \n    def aggregate_annotations(self):\n        for entry in self.result_model[\"results\"]:\n            node = entry[\"node\"]\n            self.annotations.append((node[\"name\"], node[\"annotation\"]))\n        return self.format_output()\n    \n    def format_output(self):\n        output = \"Relevant Files: \\n\\n\"\n        count = 0\n        for entry in self.annotations:\n            count += 1\n            name = entry[0]\n            annotation = entry[1]\n            output += str(f\"FILENAME: {name}\\nDESCRIPTION: \\\"{annotation}\\\"\\n\\n\")\n        return output\n\n\nclass TestAnnotationAggregate:\n    def test_aggreagate_top_1_results(self):\n        test_traverse_obj = json_to_obj(\"top_1.json\")\n        aggregator = AnnotationAggregate(test_traverse_obj)\n        print(\"\\nTesting Aggregation of top 1 results:\")\n        result = aggregator.aggregate_annotations()\n        print(result)\n    def test_aggreagate_top_3_results(self):\n        test_traverse_obj = json_to_obj(\"top_3.json\")\n        aggregator = AnnotationAggregate(test_traverse_obj)\n        print(\"\\nTesting Aggregation of top 3 results:\")\n        print(aggregator.aggregate_annotations())\n\nif __name__ == \"__main__\":\n    testAnnotationAggregate = TestAnnotationAggregate()\n    testAnnotationAggregate.test_aggreagate_top_1_results()\n    testAnnotationAggregate.test_aggreagate_top_3_results()"
                },
                {
                    "name": "top_1.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        }\n    ]\n}"
                },
                {
                    "name": "top_3.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        },\n        {\n            \"score\": 0.5,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"testkeywordextract\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"keyword_extract.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"English\",\n                    \"Language\",\n                    \"Natural\",\n                    \"Python\",\n                    \"TestKeywordExtract\",\n                    \"Toolkit\",\n                    \"annotated\",\n                    \"based\",\n                    \"class\",\n                    \"code\",\n                    \"contains\",\n                    \"create\",\n                    \"description\",\n                    \"ensuring\",\n                    \"expected\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"filtering\",\n                    \"filters\",\n                    \"identifies\",\n                    \"includes\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"list\",\n                    \"lists\",\n                    \"method\",\n                    \"nltk\",\n                    \"nouns\",\n                    \"output\",\n                    \"pieces\",\n                    \"processing\",\n                    \"provided\",\n                    \"query\",\n                    \"running\",\n                    \"script\",\n                    \"selecting\",\n                    \"stopwords\",\n                    \"tagging\",\n                    \"test\",\n                    \"text\",\n                    \"tokenizes\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"info.txt\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Automatic\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"RAKE\",\n                    \"Rapid\",\n                    \"SSL\",\n                    \"algorithm\",\n                    \"certificate\",\n                    \"changing\",\n                    \"command\",\n                    \"commands\",\n                    \"consists\",\n                    \"downloading\",\n                    \"downloads\",\n                    \"environment\",\n                    \"error\",\n                    \"gensim\",\n                    \"install\",\n                    \"installing\",\n                    \"instructs\",\n                    \"involves\",\n                    \"issue\",\n                    \"language\",\n                    \"model\",\n                    \"occurs\",\n                    \"packages\",\n                    \"processing\",\n                    \"provided\",\n                    \"setting\",\n                    \"shell\",\n                    \"suggests\",\n                    \"text\",\n                    \"tokenization\",\n                    \"use\",\n                    \"version\",\n                    \"words\"\n                ],\n                \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n            }\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "annotation_generate",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "annotation_generate.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "from openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n'''\nCreate a class to annotate a piece of given code\n- input:\n    - Code (Function, Class, etc.)\n- output:\n    - Summary of code in text\n'''\n\nclass AnnotationGeneration:\n    def __init__(self):\n        self.res = \"\"\n\n    def snippet_summary(self, snippet):\n            ## Set the API Key\n            load_dotenv()\n            API_KEY = os.getenv('OPENAI_SECRET_API_KEY')\n            client = OpenAI(api_key=API_KEY)\n\n            #GPT4o REPONSE REQUEST\n            MODEL=\"gpt-4o\"\n\n            completion = client.chat.completions.create(\n            model=MODEL,\n            #Prompt modelling, grounding the model to provide a more concise and clear summary when given a piece of code\n            messages=[\n                {\"role\": \"system\", \"content\": '''You are an AI designed to explain code clearly and concisely. When given a piece of code, your task is to provide a quick summary without giving a detailed breakdown. Your summary should include the programming language, the purpose of the code, a brief explanation of its key components functionality and logic, and the expected output. Respond in a single blurb of text.\n\n            Here is the piece of code for you to explain:\n\n            python\n            Copy code\n            def is_prime(n):\n                if n <= 1:\n                    return False\n                for i in range(2, int(n**0.5) + 1):\n                    if n % i == 0:\n                        return False\n                return True\n\n            number = 7\n            result = is_prime(number)\n            print(f\"Is {number} a prime number? {result}\")\n            Expected Explanation:\n\n            The code is written in Python and checks if a given number is a prime number. The function is_prime(n) returns True if n is prime by testing divisibility from 2 to the square root of n, otherwise it returns False. The variable number is set to 7, and the function is called to check if 7 is prime, with the result printed. The output will be: \"Is 7 a prime number? True\".'''},\n                \n                {\"role\": \"user\", \"content\": f'''With that said. Explain the given code:\n                    {snippet}\n                '''}\n            ]\n            )\n            return completion.choices[0].message.content\n    \n\n### TESTING \nclass TestSnippetSummary:\n     def __init__(self):\n        self.summarizer = AnnotationGeneration()\n        print(\"Testing Snippet Summarizer... \\n\")\n    \n        #High-depth code is code that has many variables, refers to many functions, changes variables, and is overall complex to understand from a quick view\n     def test_snippet_summarizer_from_high_depth_code(self):\n          print(\"Testing code snippet summarizer with a high depth function... \\n\")\n          code_snippet = '''\n          const handleCreateEvent = async(e) => {\n            e.preventDefault()\n            // console.log(user)\n            // const dateNow = new Date()\n            // var date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\"\\\"\", \"\").replace(\"\\\"\", \"\");\n                if (isDateRange == false){\n                    await addDoc(collection(db, 'schools', completeSchoolName, 'events'), {\n                      title: title,\n                      description: description,\n                      dateTime: dateTime,\n                      location: location,\n                      club: club,\n                      dateAdded: Date().toLocaleString(),\n                      // createdBy: {name: user.displayName, email: user.email}\n                  }).then(setTitle(\"\")).then(setDescription(\"\")).then(setDateTime(\"\")).then(setStartDateTime(\"\")).then(setEndDateTime(\"\")).then(setLocation(\"\")).then(setClub(\"\")).then(fetchUser())\n                  \n                  const chosenDate = new Date(dateTime)\n                  var dateNow = new Date(chosenDate)\n                  dateNow.setDate(chosenDate.getDate() + 1)\n                  const date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\"\\\"\", \"\").replace(\"\\\"\", \"\")\n                  await setDoc(doc(db, 'schools', completeSchoolName, 'announcements', date), {\n                    notes: [{\n                      title: title,\n                      description: description,\n                    club: club}],  \n                    createdBy: {name: user.name, email: user.email},\n                      dateAdded: Date().toLocaleString(),\n                      // createdBy: {name: user.displayName, email: user.email}\n                  }).then(setTitle(\"\")).then(setDescription(\"\")).then(setDateTime(\"\")).then(setLocation(\"\")).then(setClub(\"\")).then(fetchUser())\n\n                } else {\n                  await addDoc(collection(db, 'schools', completeSchoolName, 'events'), {\n                    title: title,\n                    description: description,\n                    startDate: startDateTime,\n                    endDate: endDateTime,\n                    location: location,\n                    club: club,\n                    dateAdded: Date().toLocaleString(),\n                    // createdBy: {name: user.displayName, email: user.email}\n                }).then(setTitle(\"\")).then(setDescription(\"\")).then(setDateTime(\"\")).then(setStartDateTime(\"\")).then(setEndDateTime(\"\")).then(setLocation(\"\")).then(setClub(\"\")).then(fetchUser())\n                \n                const chosenDate = new Date(startDateTime)\n                var dateNow = new Date(chosenDate)\n                dateNow.setDate(chosenDate.getDate() + 1)\n                const date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\"\\\"\", \"\").replace(\"\\\"\", \"\")\n                await setDoc(doc(db, 'schools', completeSchoolName, 'announcements', date), {\n                  notes: [{\n                    title: title,\n                    description: description,\n                  club: club}],  \n                  createdBy: {name: user.name, email: user.email},\n                    dateAdded: Date().toLocaleString(),\n                    // createdBy: {name: user.displayName, email: user.email}\n                }).then(setTitle(\"\")).then(setDescription(\"\")).then(setDateTime(\"\")).then(setLocation(\"\")).then(setClub(\"\")).then(fetchUser())\n\n                }\n                \n      }'''\n          output = self.summarizer.snippet_summary(code_snippet)\n          print(f\"CODE SUMMARY: \\n{output} \\n\\n\")\n          assert type(output) == str\n\n          '''\n          EXPECTED OUTPUT:The code is written in JavaScript, specifically using the async/await syntax to handle asynchronous operations with Firestore, a cloud database from Firebase. It defines an event-handling function `handleCreateEvent` meant to create and save event data into the Firestore database. When a form submission event triggers the function, it first prevents the default behavior with `e.preventDefault()`. \n          The function checks if `isDateRange` is false and, based on this, either adds or updates single or range-dated event documents in the Firestore under the 'events' collection. It also conditionally updates the 'announcements' collection based on the existence of date ranges. After database operations, it resets multiple state variables (title, description, dateTime, etc.) and fetches user data. \n          The function ensures newly created or modified data incorporates the current date and time and user metadata. The expected result includes adding appropriate entries in the Firestore under both 'events' and 'announcements' collections and resetting the form's state. \n          '''\n\n\n     #Low-depth code is code that is fairly simple, not many variables, and easy to follow \n     def test_snippet_summarizer_from_low_depth_code(self):\n          print(\"Testing code snippet summarizer with a low depth function... \\n\")\n          code_snippet = '''\n          class Solution:\n            def topK(self, nums: List[int], k: int) -> List[int]:\n                count = {}\n                freq = [[] for i in range (len(nums) + 1)]\n       \n            for n in nums:\n                count[n] = 1 + count.get(n, 0)\n            for n, c in count.items():\n                freq[c].append(n)\n       \n            res = []\n       \n            for i in range(len(freq) - 1, 0, -1):\n                for n in freq[i]:\n                    res.append(n)\n                    if len(res) == k:\n                        return res'''\n          output = self.summarizer.snippet_summary(code_snippet)\n          print(f\"CODE SUMMARY: \\n{output} \\n\\n\")\n          assert type(output) == str\n          '''\n          EXPECTED OUTPUT:\n          The code is written in Python and defines a method `topK` within a class `Solution`. The purpose of this method is to return the top k most frequent integers from a given list `nums`. \n          It first creates a `count` dictionary to tally the frequency of each number in `nums`, then organizes these frequencies into a list of lists `freq`. \n          The method then iterates through `freq` in reverse order to gather the k most frequent numbers into the result list `res`, which is returned once it reaches the desired length k. \n          '''\n    \n\nif __name__ == \"__main__\":\n    TestSnippetSummary = TestSnippetSummary()\n    TestSnippetSummary.test_snippet_summarizer_from_low_depth_code()\n    TestSnippetSummary.test_snippet_summarizer_from_high_depth_code()\n"
                },
                {
                    "name": "info.txt",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "pip install:\n    python-dotenv\n    openai\n\nmake sure you have the .env file containing the openai secret key in this directory"
                }
            ]
        },
        {
            "name": "codebase_extract",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "codebase_extract.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import os\nimport sys\nsys.path.insert(0, \"..\")\n\nfrom utilities.utility import obj_to_json, file_to_string\n\n'''\nCreate a class to extract a model of a codebase as a tree\n- input: local directory path as a string\n- output: \n    - object containing tree structure of directory\n    - at leaf nodes store content of file as a string (if it's content is readable)\n'''\n\n\nclass CodebaseExtract:\n    def __init__(self, path):\n        # Initialize the output dictionary model with folder contents\n        # name, type, keywords, and empty list for children\n        self.path = path\n        self.model = {}\n    \n    def get_model(self):\n        self.model = self._build_model(self.path)\n        return self.model\n\n    def _build_model(self, path):  # extracts a directory as a json object\n        model = {'name': os.path.basename(path),\n                 'type': 'folder', 'keywords': [], 'children': []}\n        # Check if the path is a directory\n        if not os.path.isdir(path):\n            return model\n        # Iterate over the entries in the directory\n        for entry in os.listdir(path):\n            if not entry.startswith('.'): # ignore hidden folders & files\n                # Create the file path for current entry\n                entry_path = os.path.join(path, entry)\n                # if the entry is a directory, recursively call the function\n                if os.path.isdir(entry_path):\n                    model['children'].append(self._build_model(entry_path))\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\n                else:\n                    content = \"\"\n                    # save file content as string\n                    try:\n                        content = file_to_string(entry_path)\n                    except Exception: # handle unreadable file content\n                        content = \"n/a\"\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\n                    ], 'annotation': \"\", 'content': content})\n        return model\n\n\nclass TestCodebaseExtract:\n    def __init__(self):\n        self.test_path = \"../../codesense\"\n        self.extractor = CodebaseExtract(self.test_path)\n        print(\"Testing Codebase Extractor...\\n\")\n\n    def test_extract_codebase(self):\n        print(\"Testing codebase extraction of current project directory...\\n\")\n        output = self.extractor.get_model()\n        obj_to_json(\"./\", \"test_codebase\", output)\n        assert type(output) == dict\n\n\nif __name__ == \"__main__\":\n    testCodebaseExtract = TestCodebaseExtract()\n    testCodebaseExtract.test_extract_codebase()\n"
                },
                {
                    "name": "test_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "keyword_extract",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "keyword_extract.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n\n'''\nCreate a class to extract keywords from text\n- input:\n    - sample text as a string\n-output: \n    - list of keywords\n'''\n\n\nclass KeywordExtract:\n    def __init__(self):\n        self.keywords = []\n        # common english stopwords\n        self.stop_words = set(stopwords.words(\"english\"))\n\n    def extract(self, text):\n        tokens = word_tokenize(text)  # tokenize text\n        filtered_tokens = [word for word in tokens if word.lower(\n        ) not in self.stop_words]  # filter out stopwords\n        # identify keywords with part of speech tagging\n        pos_tags = nltk.pos_tag(filtered_tokens)\n        # keep only nouns, verbs\n        for word, pos in pos_tags:\n            if pos.startswith(\"NN\") or pos.startswith(\"VB\"):\n                self.keywords.append(word)\n        self.keywords = list(set(self.keywords))  # remove duplicates\n        return self.keywords\n\n\nclass TestKeywordExtract:\n    def __init__(self):\n        self.extractor = KeywordExtract()\n        print(\"Testing Keyword Extractor...\\n\")\n\n    def test_extract_keywords_from_query(self):\n        print(\"Testing keywword extraction of user query...\\n\")\n        text = \"I want to modify the maxProfit function to have an initial maxP value of 10\"\n        output = self.extractor.extract(text)\n        print(f\"Keywords from query: {output}\\n\")\n        assert type(output) == list\n\n    def test_extract_keywords_from_annotation(self):\n        print(\"Testing keywword extraction of code annotation...\\n\")\n        text = \"\"\"\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\n\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\n            \"\"\"\n        output = self.extractor.extract(text)\n        print(f\"Keywords from annotation: {output}\\n\")\n        assert type(output) == list\n\n\nif __name__ == \"__main__\":\n    testKeywordExtract = TestKeywordExtract()\n    testKeywordExtract.test_extract_keywords_from_query()\n    testKeywordExtract.test_extract_keywords_from_annotation()\n"
                }
            ]
        },
        {
            "name": "populate_annotations",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "ignore.txt",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "codesense/keyword_extract\ncodesense/extras/codebase_extraction/codebase.json\ncodesense/README.md\n"
                },
                {
                    "name": "populate_annotations.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import os\nimport sys\n\nsys.path.insert(0, \"..\")\nfrom annotation_generate.annotation_generate import AnnotationGeneration\nfrom utilities.utility import file_to_string, obj_to_json, json_to_obj\n\n'''\nCreate a class to populate the codebase json with annotations\n- input: \n    - codebase model object\n- output:\n    - codebase model object with updated annotation fields\n'''\n\nclass PopulateAnnotations:\n    def __init__(self, model_obj, ignore_paths_file):\n        #ignore_paths is a txt file containing file_paths to ignore\n        self.annotator = AnnotationGeneration()\n        self.model = model_obj\n        self.ignore_list = self.build_ignore_list(ignore_paths_file)\n    \n    def build_ignore_list(self, txt_file):\n        # read txt file as string\n        ignore_list = file_to_string(txt_file).splitlines()\n        return ignore_list\n        \n    \n    def annotate(self, content_str):\n        formated_str = content_str.replace(\"\\n\", \"\") # remove newline characters\n        output = self.annotator.snippet_summary(formated_str) # comment this out to stub API call for testing purposes\n        # output = \"test\"\n        return output\n    \n    def populate_model(self):\n        self._populate(self.model, self.model[\"name\"])\n        return self.model\n        \n    def _populate(self, model, cur_path):\n        if model[\"type\"] == \"file\":\n            if model[\"content\"] not in [\"n/a\", \"\"]:\n                annotation = self.annotate(model[\"content\"])\n                model[\"annotation\"] = annotation \n                return model  \n        else:\n            for child in model[\"children\"]:\n                #build path string of traversal\n                new_path = os.path.join(cur_path, child[\"name\"])\n                if new_path not in self.ignore_list:\n                    self._populate(child, new_path)\n\nclass TestPopulateAnnotations:\n    def __init__(self):\n        self.test_model = json_to_obj(\"test_codebase_original.json\") \n        self.test_ignore_file = \"ignore.txt\"\n        self.populator = PopulateAnnotations(self.test_model, self.test_ignore_file)\n        \n    def test_populate_annotations(self):\n        print(\"Testing annotation population\")\n        updated_model = self.populator.populate_model()\n        obj_to_json(\"./\", \"test\", updated_model)\n        assert type(updated_model) == dict\n    \n        \n\nif __name__ == \"__main__\":\n    testPopulateAnnotations = TestPopulateAnnotations()\n    testPopulateAnnotations.test_populate_annotations()\n\n                \n        \n"
                },
                {
                    "name": "test.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"test\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"test\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "test_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\"test_codebase.json\\\" representing the directory structure of `self.test_path`.\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"This document outlines a project called \\\"Codesense,\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\n\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\n3. Annotation Generation: Produces text summaries for functions in the code.\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\n\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\"Once upon a time\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "test_codebase_original.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "populate_keywords",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "populate_keywords.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import sys\n\nsys.path.insert(0, \"..\")\nfrom keyword_extract.keyword_extract import KeywordExtract\nfrom utilities.utility import obj_to_json, json_to_obj\n\n'''\nCreate a class to populate the codebase json with keywords\n- input: \n    - codebase model object\n- output:\n    - codebase model object with updated keywords fields\n'''\n\nclass PopulateKeywords:\n    def __init__(self, model_obj):\n        self.model = model_obj\n    \n    def extractKeywords(self, content_str):\n        formated_str = content_str.replace(\"\\n\", \"\") # remove newline characters\n        extractor = KeywordExtract()\n        output = extractor.extract(formated_str)\n        # output = \"test\"\n        return output\n    \n    def populate_model(self):\n        self._populate(self.model)\n        return self.model\n        \n    def _populate(self, model):\n        if model[\"type\"] == \"file\":\n            annotation = model[\"annotation\"]\n            keywords = self.extractKeywords(annotation)\n            model['keywords'] = keywords\n            return model  \n        else:\n            for child in model[\"children\"]:\n                self._populate(child)\n    \n    \nclass TestPopulateKeyWords:\n    def __init__(self):\n        self.test_model = json_to_obj(\"test_codebase_original.json\")\n        self.populator = PopulateKeywords(self.test_model)\n\n    def test_populate_keywords(self):\n        print(\"Testing annotation population\")\n        updated_model = self.populator.populate_model()\n        obj_to_json(\"./\", \"test\", updated_model)\n        assert type(updated_model) == dict\n\nif __name__ == \"__main__\":\n    testPopulateKeyWords = TestPopulateKeyWords()\n    testPopulateKeyWords.test_populate_keywords()\n    "
                },
                {
                    "name": "test.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"annotated\",\n                        \"pieces\",\n                        \"method\",\n                        \"Python\",\n                        \"keyword\",\n                        \"query\",\n                        \"stopwords\",\n                        \"ensuring\",\n                        \"processing\",\n                        \"extracted\",\n                        \"English\",\n                        \"list\",\n                        \"tokenizes\",\n                        \"output\",\n                        \"keywords\",\n                        \"class\",\n                        \"test\",\n                        \"input\",\n                        \"text\",\n                        \"extraction\",\n                        \"selecting\",\n                        \"running\",\n                        \"Natural\",\n                        \"filters\",\n                        \"provided\",\n                        \"lists\",\n                        \"TestKeywordExtract\",\n                        \"expected\",\n                        \"description\",\n                        \"nouns\",\n                        \"filtering\",\n                        \"Language\",\n                        \"includes\",\n                        \"written\",\n                        \"tagging\",\n                        \"identifies\",\n                        \"based\",\n                        \"contains\",\n                        \"nltk\",\n                        \"script\",\n                        \"Toolkit\",\n                        \"extracts\",\n                        \"code\",\n                        \"create\"\n                    ],\n                    \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"Python\",\n                        \"damage\",\n                        \"Character\",\n                        \"named\",\n                        \"initialized\",\n                        \"health\",\n                        \"character\",\n                        \"updated\",\n                        \"game\",\n                        \"ninja\",\n                        \"output\",\n                        \"class\",\n                        \"speeds\",\n                        \"created\",\n                        \"doubled\",\n                        \"showcase\",\n                        \"attributes\",\n                        \"doubles\",\n                        \"code\",\n                        \"printed\",\n                        \"written\",\n                        \"speed\",\n                        \"using\",\n                        \"models\",\n                        \"double_speed\",\n                        \"parameters\",\n                        \"instances\",\n                        \"includes\",\n                        \"warrior\"\n                    ],\n                    \"annotation\": \"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"string\",\n                        \"directories\",\n                        \"converts\",\n                        \"method\",\n                        \"Expected\",\n                        \"Python\",\n                        \"named\",\n                        \"defines\",\n                        \"writes\",\n                        \"tree\",\n                        \"output\",\n                        \"reads\",\n                        \"class\",\n                        \"test_codebase.json\",\n                        \"model\",\n                        \"stores\",\n                        \"designed\",\n                        \"treating\",\n                        \"files\",\n                        \"traversing\",\n                        \"save_model_json\",\n                        \"leaf\",\n                        \"file_to_string\",\n                        \"provided\",\n                        \"structure\",\n                        \"content\",\n                        \"tests\",\n                        \"generates\",\n                        \"contents\",\n                        \"directory\",\n                        \"given\",\n                        \"JSON\",\n                        \"nodes\",\n                        \"representing\",\n                        \"self.test_path\",\n                        \"model_to_str\",\n                        \"file\",\n                        \"functionality\",\n                        \"code\",\n                        \"create\"\n                    ],\n                    \"annotation\": \"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\"test_codebase.json\\\" representing the directory structure of `self.test_path`.\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"project\",\n                \"involves\",\n                \"showing\",\n                \"Codesense\",\n                \"document\",\n                \"tree\",\n                \"Extraction\",\n                \"Graph\",\n                \"answer\",\n                \"keywords\",\n                \"summaries\",\n                \"Tree\",\n                \"CodeBase\",\n                \"Searches\",\n                \"Produces\",\n                \"call\",\n                \"implementations\",\n                \"report\",\n                \"outlines\",\n                \"Answering\",\n                \"called\",\n                \"codebase.2\",\n                \"Keyword\",\n                \"flows\",\n                \"structure\",\n                \"Traversal\",\n                \"aggregated\",\n                \"target\",\n                \"Creates\",\n                \"Identifies\",\n                \"annotation.7\",\n                \"serves\",\n                \"objectives\",\n                \"Generates\",\n                \"directed\",\n                \"Question\",\n                \"providing\",\n                \"codebase\",\n                \"Compiles\",\n                \"tasks\",\n                \"components\",\n                \"source\",\n                \"based\",\n                \"include\",\n                \"queries.6\",\n                \"nodes\",\n                \"matching\",\n                \"codebases\",\n                \"file.3\",\n                \"Annotation\",\n                \"Generation\",\n                \"representing\",\n                \"annotations\",\n                \"user\",\n                \"returns\",\n                \"related\",\n                \"Uses\",\n                \"Aggregation\",\n                \"functions\",\n                \"code.4\",\n                \"analyze\",\n                \"graph.5\",\n                \"code\",\n                \"Call\",\n                \"function\"\n            ],\n            \"annotation\": \"This document outlines a project called \\\"Codesense,\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\n\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\n3. Annotation Generation: Produces text summaries for functions in the code.\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\n\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Python\",\n                                \"involves\",\n                                \"installing\",\n                                \"packages\",\n                                \"certificate\",\n                                \"processing\",\n                                \"install\",\n                                \"language\",\n                                \"Extraction\",\n                                \"downloads\",\n                                \"tokenization\",\n                                \"SSL\",\n                                \"shell\",\n                                \"text\",\n                                \"Automatic\",\n                                \"instructs\",\n                                \"model\",\n                                \"use\",\n                                \"Gensim\",\n                                \"consists\",\n                                \"issue\",\n                                \"provided\",\n                                \"Keyword\",\n                                \"error\",\n                                \"command\",\n                                \"NLTK\",\n                                \"algorithm\",\n                                \"suggests\",\n                                \"downloading\",\n                                \"changing\",\n                                \"commands\",\n                                \"setting\",\n                                \"environment\",\n                                \"RAKE\",\n                                \"Rapid\",\n                                \"occurs\",\n                                \"version\",\n                                \"NLP\",\n                                \"words\",\n                                \"gensim\"\n                            ],\n                            \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"similarity\",\n                                \"embeddings\",\n                                \"Python\",\n                                \"keyword\",\n                                \"*\",\n                                \"uses\",\n                                \"Text\",\n                                \"processes\",\n                                \"texts\",\n                                \"output\",\n                                \"processing\",\n                                \"extracted\",\n                                \"language\",\n                                \"Extraction\",\n                                \"comparing\",\n                                \"tokenizes\",\n                                \"list\",\n                                \"extraction\",\n                                \"keywords\",\n                                \"Word\",\n                                \"input\",\n                                \"text\",\n                                \"reads\",\n                                \"library\",\n                                \"focuses\",\n                                \"model\",\n                                \"Gensim\",\n                                \"Comparison\",\n                                \"modeling.1\",\n                                \"Embeddings\",\n                                \"sentences\",\n                                \"Keyword\",\n                                \"lists\",\n                                \"performs\",\n                                \"verbs\",\n                                \"compare_keywords\",\n                                \"NLTK\",\n                                \"console\",\n                                \"Word2Vec\",\n                                \"written\",\n                                \"tagging\",\n                                \"create\",\n                                \"using\",\n                                \"removes\",\n                                \"calculates\",\n                                \"score\",\n                                \"libraries\",\n                                \"word\",\n                                \"Processing\",\n                                \"compare_words\",\n                                \"employs\",\n                                \"vector\",\n                                \"extract_keywords\",\n                                \"returned\",\n                                \"Similarity\",\n                                \"words.3\",\n                                \"context\",\n                                \"NLP\",\n                                \"computes\",\n                                \"extracts\",\n                                \"file\",\n                                \"code\",\n                                \"words\",\n                                \"techniques\",\n                                \"function\",\n                                \"keywords.The\"\n                            ],\n                            \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"method\",\n                                \"refers\",\n                                \"list\",\n                                \"class\",\n                                \"price\",\n                                \"profit\",\n                                \"stock\",\n                                \"increase\",\n                                \"captures\",\n                                \"iterates\",\n                                \"end\",\n                                \"sets\",\n                                \"Solution\",\n                                \"description\",\n                                \"element\",\n                                \"calculates\",\n                                \"maxP\",\n                                \"C++\",\n                                \"returns\",\n                                \"prices\",\n                                \"representing\",\n                                \"adds\",\n                                \"maxProfit\",\n                                \"difference\",\n                                \"opportunities\",\n                                \"function\"\n                            ],\n                            \"annotation\": \"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"project\",\n                                \"Transformers\",\n                                \"Python\",\n                                \"keyword\",\n                                \"generating\",\n                                \"Face\",\n                                \"Additionally\",\n                                \"generation\",\n                                \"character\",\n                                \"game\",\n                                \"scripts\",\n                                \"extraction\",\n                                \"summaries\",\n                                \"annotation\",\n                                \"text\",\n                                \"structured\",\n                                \"model\",\n                                \"detailing\",\n                                \"designed\",\n                                \"working\",\n                                \"structures\",\n                                \"utilities\",\n                                \"called\",\n                                \"provided\",\n                                \"structure\",\n                                \"attributes\",\n                                \"provide\",\n                                \"expected\",\n                                \"employing\",\n                                \"lists\",\n                                \"creating\",\n                                \"NLTK\",\n                                \"README\",\n                                \"methods\",\n                                \"codebase\",\n                                \"representations\",\n                                \"using\",\n                                \"codesense\",\n                                \"directory\",\n                                \"components\",\n                                \"tasks\",\n                                \"defined\",\n                                \"include\",\n                                \"JSON\",\n                                \"performing\",\n                                \"object\",\n                                \"codebases\",\n                                \"setting\",\n                                \"environment\",\n                                \"representing\",\n                                \"instructions\",\n                                \"annotations\",\n                                \"modeling\",\n                                \"outputs\",\n                                \"involve\",\n                                \"Hugging\",\n                                \"functionalities\",\n                                \"code\"\n                            ],\n                            \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"directories\",\n                                \"converts\",\n                                \"string\",\n                                \"read\",\n                                \"Python\",\n                                \"codebase.json\",\n                                \"navigates\",\n                                \"folder\",\n                                \"named\",\n                                \"indentation\",\n                                \"nested\",\n                                \"types\",\n                                \"create_folder_structure_json\",\n                                \"list\",\n                                \"ignored\",\n                                \"output\",\n                                \"reads\",\n                                \"folders\",\n                                \"specifies\",\n                                \"prints\",\n                                \"names\",\n                                \"children\",\n                                \"designed\",\n                                \"saves\",\n                                \"files\",\n                                \"file_path\",\n                                \"path\",\n                                \"file_to_string\",\n                                \"called\",\n                                \"structure\",\n                                \"Hidden\",\n                                \"creating\",\n                                \"contents\",\n                                \"create\",\n                                \"calls\",\n                                \"directory\",\n                                \"given\",\n                                \"JSON\",\n                                \"returns\",\n                                \"object\",\n                                \"representing\",\n                                \"starting\",\n                                \"including\",\n                                \"found\",\n                                \"script\",\n                                \"representation\",\n                                \"file\",\n                                \"code\",\n                                \"dictionary\",\n                                \"function\"\n                            ],\n                            \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Python\",\n                                \"packages\",\n                                \"install\",\n                                \"transformers\",\n                                \"CodeLlama\",\n                                \"English\",\n                                \"accelerate\",\n                                \"run\",\n                                \"snippet\",\n                                \"installed\",\n                                \"model\",\n                                \"running\",\n                                \"working\",\n                                \"required\",\n                                \"instruction\",\n                                \"command\",\n                                \"pip\",\n                                \"provides\",\n                                \"libraries\",\n                                \"environment\",\n                                \"plain\",\n                                \"code\"\n                            ],\n                            \"annotation\": \"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Transformers\",\n                                \"Python\",\n                                \"Face\",\n                                \"First\",\n                                \"uses\",\n                                \"time\",\n                                \"generation\",\n                                \"enabled\",\n                                \"language\",\n                                \"transformers\",\n                                \"library\",\n                                \"output\",\n                                \"generated\",\n                                \"prints\",\n                                \"text\",\n                                \"imports\",\n                                \"model\",\n                                \"use\",\n                                \"characters\",\n                                \"prompt\",\n                                \"load\",\n                                \"generate\",\n                                \"provided\",\n                                \"sampling\",\n                                \"expected\",\n                                \"continuation\",\n                                \"pipeline\",\n                                \"provides\",\n                                \"Llama\",\n                                \"written\",\n                                \"length\",\n                                \"Hugging\",\n                                \"code\",\n                                \"function\"\n                            ],\n                            \"annotation\": \"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\"Once upon a time\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "test_codebase_original.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\"test_codebase.json\\\" representing the directory structure of `self.test_path`.\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"This document outlines a project called \\\"Codesense,\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\n\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\n3. Annotation Generation: Produces text summaries for functions in the code.\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\n\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\"Once upon a time\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "question_answering",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "question_answer.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "from openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nimport sys\n\nsys.path.insert(0, \"..\")\nfrom annotation_aggregate.annotation_aggregate import AnnotationAggregate\nfrom utilities.utility import json_to_obj\n\n'''\nCreate a class that responds to a user query given context from the codebase\n- input:\n    - traversal result object\n- output:\n    - Response to user query as string\n'''\n\n\nclass QueryAnswer:\n    def __init__(self, traverse_obj):\n        self.res = \"\"\n        self.traversal = traverse_obj\n        aggregator = AnnotationAggregate(self.traversal)\n        self.context = aggregator.aggregate_annotations()\n\n    ## Set the API Key\n    def get_response(self, query):\n        load_dotenv()\n        API_KEY = os.getenv('OPENAI_SECRET_API_KEY')\n        client = OpenAI(api_key=API_KEY)\n\n        #GPT4o REPONSE REQUEST\n        MODEL=\"gpt-4o\"\n\n        completion = client.chat.completions.create(\n        model=MODEL,\n        #Prompt modelling, grounding the model to provide a more concise and clear summary when given a piece of code\n        messages=[\n            {\"role\": \"system\", \"content\": '''\n            You are a developer assistant designed to provide detailed answers and assistance based on contextual explanations of code in a codebase. Your input consists of explanations of code files and their respective file directories within the codebase. Users will provide queries related to the codebase, seeking clarification, assistance, or suggestions. Your task is to utilize the provided context to generate clear and structured responses to the user queries. Your responses should be informative, accurate, and tailored to the specific query. Additionally, you may suggest potential actions or direct the user to relevant code files within the codebase for further reference. Your responses should solely rely on the provided context, avoiding external knowledge or assumptions. Remember to maintain clarity and coherence in your responses, ensuring that users can easily understand and follow your guidance. Make sure to keep your responses as short as possible as well so that the developer can quickly view an answer their question.\n\n            Example:\n\n            Query: How does the event-handling function handle errors during Firestore database operations?\n\n            Context:\n            The code is written in JavaScript, specifically using the async/await syntax to handle asynchronous operations with Firestore, a cloud database from Firebase. It defines an event-handling function `handleCreateEvent` meant to create and save event data into the Firestore database. When a form submission event triggers the function, it first prevents the default behavior with `e.preventDefault()`. The function checks if `isDateRange` is false and, based on this, either adds or updates single or range-dated event documents in the Firestore under the 'events' collection. It also conditionally updates the 'announcements' collection based on the existence of date ranges. After database operations, it resets multiple state variables (title, description, dateTime, etc.) and fetches user data. The function ensures newly created or modified data incorporates the current date and time and user metadata. The expected result includes adding appropriate entries in the Firestore under both 'events' and 'announcements' collections and resetting the form's state. \n            File Directory: NewsFlash/pages/events.js\n            \n\n            Response:\n            The event-handling function `handleCreateEvent` in the file events.js employs error handling mechanisms to manage errors during Firestore database operations. Within the async function, try-catch blocks are utilized to capture and handle any potential errors that may occur during asynchronous database transactions. Specifically, when performing Firestore operations such as adding or updating event documents, the try block encapsulates these operations, allowing for graceful error handling. In the event of an error, the catch block is triggered, enabling the function to handle the error appropriately, which may include logging the error, displaying a user-friendly message, or initiating corrective actions. Additionally, the function may utilize Firebase's error handling features, such as error codes or error objects, to provide more detailed information about the nature of the error and facilitate troubleshooting. Overall, the event-handling function is designed to handle errors robustly, ensuring the reliability and stability of database operations.\n            '''},\n\n            {\"role\": \"user\", \"content\": f'''With that said. The query and context is given below:\n            QUERY: {query}\n            \n            CODEBASE CONTEXT: {self.context}\n            '''}\n        ]\n        )\n        return completion.choices[0].message.content\n\n### TESTING \nclass TestQueryAnswering:\n     def __init__(self):\n        self.test_model = json_to_obj(\"top_5.json\")\n        self.responder = QueryAnswer(self.test_model)\n        print(\"Testing Query Response... \\n\")\n    \n     def test_keyword_extract_explanation(self):\n        query=\"How does keyword extraction work in this project?\"\n        output = self.responder.get_response(query)\n        print(output)\n        assert type(output) == str\n    \n\nif __name__ == \"__main__\":\n    TestQueryAnswering = TestQueryAnswering()\n    TestQueryAnswering.test_keyword_extract_explanation()"
                },
                {
                    "name": "top_5.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\",\n                \"word2vec\",\n                \"extract_keywords\",\n                \"function\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        },\n        {\n            \"score\": 0.5,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\",\n                \"testkeywordextract\"\n            ],\n            \"node\": {\n                \"name\": \"keyword_extract.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"English\",\n                    \"Language\",\n                    \"Natural\",\n                    \"Python\",\n                    \"TestKeywordExtract\",\n                    \"Toolkit\",\n                    \"annotated\",\n                    \"based\",\n                    \"class\",\n                    \"code\",\n                    \"contains\",\n                    \"create\",\n                    \"description\",\n                    \"ensuring\",\n                    \"expected\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"filtering\",\n                    \"filters\",\n                    \"identifies\",\n                    \"includes\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"list\",\n                    \"lists\",\n                    \"method\",\n                    \"nltk\",\n                    \"nouns\",\n                    \"output\",\n                    \"pieces\",\n                    \"processing\",\n                    \"provided\",\n                    \"query\",\n                    \"running\",\n                    \"script\",\n                    \"selecting\",\n                    \"stopwords\",\n                    \"tagging\",\n                    \"test\",\n                    \"text\",\n                    \"tokenizes\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"info.txt\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Automatic\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"RAKE\",\n                    \"Rapid\",\n                    \"SSL\",\n                    \"algorithm\",\n                    \"certificate\",\n                    \"changing\",\n                    \"command\",\n                    \"commands\",\n                    \"consists\",\n                    \"downloading\",\n                    \"downloads\",\n                    \"environment\",\n                    \"error\",\n                    \"gensim\",\n                    \"install\",\n                    \"installing\",\n                    \"instructs\",\n                    \"involves\",\n                    \"issue\",\n                    \"language\",\n                    \"model\",\n                    \"occurs\",\n                    \"packages\",\n                    \"processing\",\n                    \"provided\",\n                    \"setting\",\n                    \"shell\",\n                    \"suggests\",\n                    \"text\",\n                    \"tokenization\",\n                    \"use\",\n                    \"version\",\n                    \"words\"\n                ],\n                \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"codebase.json\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Additionally\",\n                    \"Face\",\n                    \"Hugging\",\n                    \"JSON\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"README\",\n                    \"Transformers\",\n                    \"annotation\",\n                    \"annotations\",\n                    \"attributes\",\n                    \"called\",\n                    \"character\",\n                    \"code\",\n                    \"codebase\",\n                    \"codebases\",\n                    \"codesense\",\n                    \"components\",\n                    \"creating\",\n                    \"defined\",\n                    \"designed\",\n                    \"detailing\",\n                    \"directory\",\n                    \"employing\",\n                    \"environment\",\n                    \"expected\",\n                    \"extraction\",\n                    \"functionalities\",\n                    \"game\",\n                    \"generating\",\n                    \"generation\",\n                    \"include\",\n                    \"instructions\",\n                    \"involve\",\n                    \"keyword\",\n                    \"lists\",\n                    \"methods\",\n                    \"model\",\n                    \"modeling\",\n                    \"object\",\n                    \"outputs\",\n                    \"performing\",\n                    \"project\",\n                    \"provide\",\n                    \"provided\",\n                    \"representations\",\n                    \"representing\",\n                    \"scripts\",\n                    \"setting\",\n                    \"structure\",\n                    \"structured\",\n                    \"structures\",\n                    \"summaries\",\n                    \"tasks\",\n                    \"text\",\n                    \"using\",\n                    \"utilities\",\n                    \"working\"\n                ],\n                \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"function\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Hidden\",\n                    \"JSON\",\n                    \"Python\",\n                    \"called\",\n                    \"calls\",\n                    \"children\",\n                    \"code\",\n                    \"codebase.json\",\n                    \"contents\",\n                    \"converts\",\n                    \"create\",\n                    \"create_folder_structure_json\",\n                    \"creating\",\n                    \"designed\",\n                    \"dictionary\",\n                    \"directories\",\n                    \"directory\",\n                    \"file\",\n                    \"file_path\",\n                    \"file_to_string\",\n                    \"files\",\n                    \"folder\",\n                    \"folders\",\n                    \"found\",\n                    \"function\",\n                    \"given\",\n                    \"ignored\",\n                    \"including\",\n                    \"indentation\",\n                    \"list\",\n                    \"named\",\n                    \"names\",\n                    \"navigates\",\n                    \"nested\",\n                    \"object\",\n                    \"output\",\n                    \"path\",\n                    \"prints\",\n                    \"read\",\n                    \"reads\",\n                    \"representation\",\n                    \"representing\",\n                    \"returns\",\n                    \"saves\",\n                    \"script\",\n                    \"specifies\",\n                    \"starting\",\n                    \"string\",\n                    \"structure\",\n                    \"types\"\n                ],\n                \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n            }\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "template",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "template.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "'''\nCreate a class to model a character in a video game\n- initialize the class with three parameters\n    - Health\n    - Damage\n    - Speed\n\n- define a mathod to double the speed of the character\n'''\n\nclass Character:\n    def __init__(self, health, damage, speed):\n        self.health = health\n        self.damage = damage\n        self.speed = speed\n    \n    def double_speed(self):\n            self.speed *= 2\n\n\n\n\nwarrior = Character(100, 50, 10)\nninja = Character(80, 40, 40)\n\nprint(f\"Warrior speed: {warrior.speed}\")\nprint(f\"Ninja speed: {ninja.speed}\")\n\nwarrior.double_speed()\n\nprint(f\"Warrior speed: {warrior.speed}\")\n  "
                }
            ]
        },
        {
            "name": "test",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "integration_test",
                    "type": "folder",
                    "keywords": [],
                    "children": [
                        {
                            "name": "codebase.json",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "{\n    \"name\": \"rust-calculator\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"Cargo.toml\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"crate\",\n                \"TOML\",\n                \"code\",\n                \"version\",\n                \"dependency\",\n                \"manage\",\n                \"dependencies\",\n                \"Henry\",\n                \"edition\",\n                \"file\",\n                \"written\",\n                \"calc\",\n                \"Boisdequin\",\n                \"serves\",\n                \"Obvious\",\n                \"package\",\n                \"configuration\",\n                \"Cargo\",\n                \"information\",\n                \"declares\",\n                \"project\",\n                \"manager\",\n                \"used\",\n                \"author\",\n                \"Minimal\",\n                \"specifying\",\n                \"Tom\",\n                \"Language\",\n                \"name\",\n                \"Rust\"\n            ],\n            \"annotation\": \"The code is written in TOML (Tom's Obvious, Minimal Language) and serves as a Cargo manifest file for a Rust project. It defines the package information by specifying the name \\\"calc\\\", version \\\"0.1.0\\\", author \\\"Henry Boisdequin\\\", and the Rust edition \\\"2018\\\". Additionally, it declares a dependency on the \\\"itertools\\\" crate, version \\\"0.10\\\". This configuration will be used by Cargo, Rust's package manager, to manage the project and its dependencies.\",\n            \"content\": \"[package]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\nauthors = [\\\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\\\"]\\nedition = \\\"2018\\\"\\n\\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\\n\\n[dependencies]\\nitertools = \\\"0.10\\\"\\n\"\n        },\n        {\n            \"name\": \"Cargo.lock\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"registry\",\n                \"TOML\",\n                \"ensure\",\n                \"snippet\",\n                \"provided\",\n                \"version\",\n                \"pulled\",\n                \"packages\",\n                \"dependencies\",\n                \"specifies\",\n                \"versions\",\n                \"reproducibility\",\n                \"file\",\n                \"written\",\n                \"calc\",\n                \"depends\",\n                \"Obvious\",\n                \"Cargo.toml.lock\",\n                \"lists\",\n                \"package\",\n                \"indicates\",\n                \"Cargo\",\n                \"checksums\",\n                \"includes\",\n                \"index\",\n                \"project\",\n                \"manager\",\n                \"used\",\n                \"Minimal\",\n                \"Tom\",\n                \"projects\",\n                \"itertools\",\n                \"Language\",\n                \"Rust\"\n            ],\n            \"annotation\": \"The provided content is a snippet from a `Cargo.toml.lock` file, written in TOML (Tom's Obvious, Minimal Language), which is used by the Cargo package manager for Rust projects. This file specifies the dependencies for a Rust project and includes exact versions and checksums to ensure reproducibility. It lists three packages: `calc` version 0.1.0, `either` version 1.6.1, and `itertools` version 0.10.0. The `itertools` package depends on the `either` package. The file indicates that these dependencies are pulled from the crates.io index, which is the official package registry for Rust.\",\n            \"content\": \"# This file is automatically @generated by Cargo.\\n# It is not intended for manual editing.\\n[[package]]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\ndependencies = [\\n \\\"itertools\\\",\\n]\\n\\n[[package]]\\nname = \\\"either\\\"\\nversion = \\\"1.6.1\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\\\"\\n\\n[[package]]\\nname = \\\"itertools\\\"\\nversion = \\\"0.10.0\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"37d572918e350e82412fe766d24b15e6682fb2ed2bbe018280caa810397cb319\\\"\\ndependencies = [\\n \\\"either\\\",\\n]\\n\"\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"ensure\",\n                \"command\",\n                \"provided\",\n                \"code\",\n                \"execute\",\n                \"cargo\",\n                \"outlines\",\n                \"enabling\",\n                \"installed\",\n                \"run\",\n                \"line\",\n                \"navigate\",\n                \"need\",\n                \"cases\",\n                \"directory\",\n                \"test\",\n                \"perform\",\n                \"use\",\n                \"package\",\n                \"operations\",\n                \"system\",\n                \"application\",\n                \"compile\",\n                \"explanation\",\n                \"included\",\n                \"project\",\n                \"calculator\",\n                \"manager\",\n                \"repository\",\n                \"setup\",\n                \"implemented\",\n                \"containing\",\n                \"Rust\"\n            ],\n            \"annotation\": \"The provided explanation outlines a simple command-line calculator implemented in Rust. To run this calculator, you need to clone the repository containing the code, ensure that Rust and the cargo package manager are installed on your system, navigate to the project directory, and run the command `cargo run`. To execute any test cases included in the project, use the command `cargo test`. This setup will compile and run the calculator application, enabling you to perform basic arithmetic operations via the command line.\",\n            \"content\": \"Simple command-line calculator in Rust.\\n\\n## To Run\\n\\n1. Clone this repository\\n\\n2. Make sure you have Rust and cargo installed\\n\\n3. Cd into the project directory and type `cargo run`\\n\\n4. To test: run `cargo test`\\n\"\n        },\n        {\n            \"name\": \"src\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"main.rs\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"result\",\n                        \"splits\",\n                        \"ends\",\n                        \"list\",\n                        \"code\",\n                        \"program\",\n                        \"addition\",\n                        \"equation\",\n                        \"predefined\",\n                        \"subtraction\",\n                        \"applies\",\n                        \"q\",\n                        \"include\",\n                        \"message\",\n                        \"numbers\",\n                        \"reads\",\n                        \"written\",\n                        \"serves\",\n                        \"prompts\",\n                        \"breaks\",\n                        \"perform\",\n                        \"operators\",\n                        \"operations\",\n                        \"operator\",\n                        \"operation\",\n                        \"using\",\n                        \"quit\",\n                        \"printed\",\n                        \"multiplication\",\n                        \"substrings\",\n                        \"calculator\",\n                        \"Calc\",\n                        \"converts\",\n                        \"used\",\n                        \"checks\",\n                        \"division\",\n                        \"input\",\n                        \"valid\",\n                        \"corresponding\",\n                        \"module\",\n                        \"Rust\"\n                    ],\n                    \"annotation\": \"The code is written in Rust and serves as a basic calculator program. It repeatedly prompts the user to enter a mathematical equation or \\\"q\\\" to quit. The `Calc` module is used to perform arithmetic operations, which include addition, subtraction, multiplication, and division. The program reads user input, checks for valid operators from a predefined list, splits the input by the operator, converts the substrings into floating-point numbers, and applies the corresponding arithmetic operation using the `Calc` module. The result is then printed. If the input is \\\"q\\\", the loop breaks, and the program ends with a thank-you message.\",\n                    \"content\": \"mod calc;\\nuse calc::Calc;\\nuse std::io;\\n\\nfn main() {\\n    println!(\\\"Welcome to the a basic calculator built with Rust.\\\");\\n\\n    loop {\\n        println!(\\\"Please enter an equation or \\\\\\\"q\\\\\\\" to quit: \\\");\\n\\n        let mut input = String::new();\\n        io::stdin()\\n            .read_line(&mut input)\\n            .expect(\\\"Failed to read input\\\");\\n\\n        if input.trim() == \\\"q\\\" {\\n            println!(\\\"Thanks for using this program.\\\");\\n            break;\\n        }\\n\\n        let valid_operators = vec![\\\"+\\\", \\\"-\\\", \\\"*\\\", \\\"/\\\"];\\n\\n        for operator in valid_operators {\\n            match input.find(operator) {\\n                Some(_) => {\\n                    let parts: Vec<&str> = input.split(operator).collect();\\n\\n                    if parts.len() < 2 {\\n                        panic!(\\\"Invalid equation.\\\");\\n                    }\\n\\n                    let mut number_array = vec![];\\n                    let mut counter = 0;\\n\\n                    while counter != parts.len() {\\n                        let val: f64 = parts[counter].trim().parse().ok().expect(\\\"Enter a number.\\\");\\n                        number_array.push(val);\\n                        counter += 1;\\n                    }\\n\\n                    match operator {\\n                        \\\"+\\\" => println!(\\\"{}\\\", Calc::add(number_array)),\\n                        \\\"-\\\" => println!(\\\"{}\\\", Calc::sub(number_array)),\\n                        \\\"*\\\" => println!(\\\"{}\\\", Calc::mul(number_array)),\\n                        \\\"/\\\" => println!(\\\"{}\\\", Calc::div(number_array)),\\n                        _ => println!(\\\"Only addition, subtraction, multiplication and division are supported.\\\")\\n                    }\\n                }\\n\\n                None => {\\n                    continue;\\n                }\\n            }\\n        }\\n    }\\n}\\n\"\n                },\n                {\n                    \"name\": \"calc.rs\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"f64\",\n                        \"provided\",\n                        \"list\",\n                        \"code\",\n                        \"addition\",\n                        \"uses\",\n                        \"subtraction\",\n                        \"elements\",\n                        \"results\",\n                        \"contains\",\n                        \"vectors\",\n                        \"unit\",\n                        \"assertions\",\n                        \"numbers\",\n                        \"tests\",\n                        \"verify\",\n                        \"written\",\n                        \"element.The\",\n                        \"return\",\n                        \"expected\",\n                        \"Expected\",\n                        \"behave\",\n                        \"defines\",\n                        \"operations\",\n                        \"output\",\n                        \"mul\",\n                        \"subsequent\",\n                        \"element\",\n                        \"sub\",\n                        \"multiplication\",\n                        \"methods\",\n                        \"work\",\n                        \"div\",\n                        \"intended\",\n                        \"test_all_operations\",\n                        \"check\",\n                        \"confirm\",\n                        \">\",\n                        \"add\",\n                        \"Calc\",\n                        \"Subtracts\",\n                        \"struct\",\n                        \"division\",\n                        \"Multiplies\",\n                        \"Divides\",\n                        \"function\",\n                        \"provides\",\n                        \"Rust\",\n                        \"aggregation\"\n                    ],\n                    \"annotation\": \"This code is written in Rust and defines a `Calc` struct that provides methods for basic arithmetic operations: addition, subtraction, multiplication, and division on vectors of floating-point numbers (`Vec<f64>`). The methods `add`, `sub`, `mul`, and `div` perform aggregation operations on the provided list of numbers. \\n\\n- `add`: Sums all elements in the vector.\\n- `sub`: Subtracts each subsequent element from the first element.\\n- `mul`: Multiplies all elements together.\\n- `div`: Divides the first element by each subsequent element.\\n\\nThe `test_all_operations` function contains unit tests to verify that these methods work as expected. It uses assertions to check that the operations return the correct results for various input vectors. Expected output for the tests would confirm all operations behave as intended.\",\n                    \"content\": \"use itertools::Itertools;\\nuse std::ops::{Div, Sub};\\n\\npub struct Calc;\\n\\nimpl Calc {\\n    pub fn add(arr: Vec<f64>) -> f64 {\\n        arr.iter().sum::<f64>()\\n    }\\n\\n    pub fn sub(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Sub::sub).unwrap_or(0.0)\\n    }\\n\\n    pub fn mul(arr: Vec<f64>) -> f64 {\\n        arr.iter().product()\\n    }\\n\\n    pub fn div(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Div::div).unwrap_or(0.0)\\n    }\\n}\\n\\n#[test]\\nfn test_all_operations() {\\n    // addition\\n    assert_eq!(Calc::add([2.0, 4.0, 6.0].to_vec()), 12.0);\\n    assert_eq!(Calc::add([-6.0, 5.0, 10.0].to_vec()), 9.0);\\n\\n    // subtraction\\n    assert_eq!(Calc::sub([10.0, 4.0, 6.0].to_vec()), 0.0);\\n    assert_eq!(Calc::sub([100.0, 10.0, 19.0].to_vec()), 71.0);\\n\\n    // multiplication\\n    assert_eq!(Calc::mul([10.0, 10.0, 2.0].to_vec()), 200.0);\\n    assert_eq!(Calc::mul([-3.0, 2.0].to_vec()), -6.0);\\n\\n    // division\\n    assert_eq!(Calc::div([54.0, 2.0, 3.0].to_vec()), 9.0);\\n    assert_eq!(Calc::div([4.0, 2.0, 5.0].to_vec()), 0.4);\\n}\\n\"\n                }\n            ]\n        }\n    ]\n}"
                        },
                        {
                            "name": "ignore.txt",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": ""
                        },
                        {
                            "name": "integration.py",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "import sys\n\n\nsys.path.insert(0, \"../..\")\nfrom codebase_extract.codebase_extract import CodebaseExtract\nfrom populate_annotations.populate_annotations import PopulateAnnotations\nfrom populate_keywords.populate_keywords import PopulateKeywords\nfrom keyword_extract.keyword_extract import KeywordExtract\nfrom tree_traverse.tree_traverse import TraverseCodebase\nfrom question_answering.question_answer import QueryAnswer\nfrom utilities.utility import obj_to_json\n\n'''\nCreate a class that can run a full integration test of codesense\n- input: Question as a string\n- output: Answer as a string\n'''\n\nclass Integration:\n    def __init__(self, code_base_dir, ignore_paths_file):\n        self.path = code_base_dir\n        self.ignore_paths_file = ignore_paths_file\n        self.code_base_model = {}\n        self.search_result = {}\n    \n    def model_codebase(self):\n        # Extract Codebase\n        codebase_extractor = CodebaseExtract(self.path)\n        self.code_base_model = codebase_extractor.get_model()\n        # Populate Annotations\n        populate_annotations = PopulateAnnotations(self.code_base_model, self.ignore_paths_file)\n        self.code_base_model = populate_annotations.populate_model()\n        # Populate Keywords\n        populate_keywords = PopulateKeywords(self.code_base_model)\n        self.code_base_model = populate_keywords.populate_model()\n        obj_to_json(\"./\", \"codebase\", self.code_base_model)\n\n    def query(self, question):\n        # Extract Keywords\n        extract_keywords = KeywordExtract()\n        query_keywords = extract_keywords.extract(question)\n        # Traverse Tree\n        traverser = TraverseCodebase(self.code_base_model)\n        self.search_result = traverser.get_top_nodes(query_keywords, 5)\n        # Question Answer\n        responder = QueryAnswer(self.search_result)\n        response = responder.get_response(question)\n        return response\n\nclass TestIntegration:\n    def __init__(self):\n        print(\"INTEGRATION TEST\")\n        test_code_base = \"rust_calculator_project\"\n        test_ignore_file = \"ignore.txt\"\n        self.integration = Integration(test_code_base, test_ignore_file)\n    \n    def test_run(self):\n        print(\"modelling codebase...\")\n        self.integration.model_codebase()\n        #Q1\n        question = \"Does this project have a multiplication capability?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = self.integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        #Q2\n        question = \"does it have a square operation functionality?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = self.integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        #Q3\n        question = \"how would we modify the code to add a square function?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = self.integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n    \n    def test_run_loop_prompt(self):\n        print(\"modelling codebase...\")\n        self.integration.model_codebase()\n        while True:\n            question = input(\"QUESTION: \")\n            print(\"querying codebase...\")\n            response = self.integration.query(question)\n            print(f\"RESPONSE: \\n{response}\\n\")\n        \n        \nif __name__ == \"__main__\":\n    testIntegration = TestIntegration()\n    testIntegration.test_run()"
                        },
                        {
                            "name": "rust_calculator_project",
                            "type": "folder",
                            "keywords": [],
                            "children": [
                                {
                                    "name": "Cargo.lock",
                                    "type": "file",
                                    "keywords": [],
                                    "annotation": "",
                                    "content": "# This file is automatically @generated by Cargo.\n# It is not intended for manual editing.\n[[package]]\nname = \"calc\"\nversion = \"0.1.0\"\ndependencies = [\n \"itertools\",\n]\n\n[[package]]\nname = \"either\"\nversion = \"1.6.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\nchecksum = \"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\"\n\n[[package]]\nname = \"itertools\"\nversion = \"0.10.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\nchecksum = \"37d572918e350e82412fe766d24b15e6682fb2ed2bbe018280caa810397cb319\"\ndependencies = [\n \"either\",\n]\n"
                                },
                                {
                                    "name": "Cargo.toml",
                                    "type": "file",
                                    "keywords": [],
                                    "annotation": "",
                                    "content": "[package]\nname = \"calc\"\nversion = \"0.1.0\"\nauthors = [\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\"]\nedition = \"2018\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]\nitertools = \"0.10\"\n"
                                },
                                {
                                    "name": "README.md",
                                    "type": "file",
                                    "keywords": [],
                                    "annotation": "",
                                    "content": "Simple command-line calculator in Rust.\n\n## To Run\n\n1. Clone this repository\n\n2. Make sure you have Rust and cargo installed\n\n3. Cd into the project directory and type `cargo run`\n\n4. To test: run `cargo test`\n"
                                },
                                {
                                    "name": "src",
                                    "type": "folder",
                                    "keywords": [],
                                    "children": [
                                        {
                                            "name": "calc.rs",
                                            "type": "file",
                                            "keywords": [],
                                            "annotation": "",
                                            "content": "use itertools::Itertools;\nuse std::ops::{Div, Sub};\n\npub struct Calc;\n\nimpl Calc {\n    pub fn add(arr: Vec<f64>) -> f64 {\n        arr.iter().sum::<f64>()\n    }\n\n    pub fn sub(arr: Vec<f64>) -> f64 {\n        arr.iter().map(|&x| x as f64).fold1(Sub::sub).unwrap_or(0.0)\n    }\n\n    pub fn mul(arr: Vec<f64>) -> f64 {\n        arr.iter().product()\n    }\n\n    pub fn div(arr: Vec<f64>) -> f64 {\n        arr.iter().map(|&x| x as f64).fold1(Div::div).unwrap_or(0.0)\n    }\n}\n\n#[test]\nfn test_all_operations() {\n    // addition\n    assert_eq!(Calc::add([2.0, 4.0, 6.0].to_vec()), 12.0);\n    assert_eq!(Calc::add([-6.0, 5.0, 10.0].to_vec()), 9.0);\n\n    // subtraction\n    assert_eq!(Calc::sub([10.0, 4.0, 6.0].to_vec()), 0.0);\n    assert_eq!(Calc::sub([100.0, 10.0, 19.0].to_vec()), 71.0);\n\n    // multiplication\n    assert_eq!(Calc::mul([10.0, 10.0, 2.0].to_vec()), 200.0);\n    assert_eq!(Calc::mul([-3.0, 2.0].to_vec()), -6.0);\n\n    // division\n    assert_eq!(Calc::div([54.0, 2.0, 3.0].to_vec()), 9.0);\n    assert_eq!(Calc::div([4.0, 2.0, 5.0].to_vec()), 0.4);\n}\n"
                                        },
                                        {
                                            "name": "main.rs",
                                            "type": "file",
                                            "keywords": [],
                                            "annotation": "",
                                            "content": "mod calc;\nuse calc::Calc;\nuse std::io;\n\nfn main() {\n    println!(\"Welcome to the a basic calculator built with Rust.\");\n\n    loop {\n        println!(\"Please enter an equation or \\\"q\\\" to quit: \");\n\n        let mut input = String::new();\n        io::stdin()\n            .read_line(&mut input)\n            .expect(\"Failed to read input\");\n\n        if input.trim() == \"q\" {\n            println!(\"Thanks for using this program.\");\n            break;\n        }\n\n        let valid_operators = vec![\"+\", \"-\", \"*\", \"/\"];\n\n        for operator in valid_operators {\n            match input.find(operator) {\n                Some(_) => {\n                    let parts: Vec<&str> = input.split(operator).collect();\n\n                    if parts.len() < 2 {\n                        panic!(\"Invalid equation.\");\n                    }\n\n                    let mut number_array = vec![];\n                    let mut counter = 0;\n\n                    while counter != parts.len() {\n                        let val: f64 = parts[counter].trim().parse().ok().expect(\"Enter a number.\");\n                        number_array.push(val);\n                        counter += 1;\n                    }\n\n                    match operator {\n                        \"+\" => println!(\"{}\", Calc::add(number_array)),\n                        \"-\" => println!(\"{}\", Calc::sub(number_array)),\n                        \"*\" => println!(\"{}\", Calc::mul(number_array)),\n                        \"/\" => println!(\"{}\", Calc::div(number_array)),\n                        _ => println!(\"Only addition, subtraction, multiplication and division are supported.\")\n                    }\n                }\n\n                None => {\n                    continue;\n                }\n            }\n        }\n    }\n}\n"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "name": "sample_test_output.txt",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "INTEGRATION TEST\nmodelling codebase...\njson file saved: ./codebase.json\nQ: Does this project have a multiplication capability?\nquerying codebase...\nRESPONSE: \nYes, this project does have multiplication capability. The `calc.rs` file defines a `Calc` struct with a method named `mul` that performs multiplication on vectors of floating-point numbers (`Vec<f64>`). This method multiplies all elements in the provided vector together. Additionally, the `main.rs` file handles user input and uses the `Calc` module to perform various arithmetic operations, including multiplication.\n\nQ: does it have a square operation functionality?\nquerying codebase...\nRESPONSE: \nBased on the provided context, the codebase does not include a square operation functionality. The `Calc` struct in `calc.rs` offers methods for addition, subtraction, multiplication, and division, but it does not mention any support for squaring a number. \n\nTo add such functionality, you would need to implement a new method, such as:\n\n```rust\nimpl Calc {\n    // Other methods...\n\n    pub fn square(&self, x: f64) -> f64 {\n        x * x\n    }\n}\n```\n\nYou would also need to modify the `main.rs` file to handle the square operation input from the user.\n\nQ: how would we modify the code to add a square function?\nquerying codebase...\nRESPONSE: \nTo add a `square` function to the existing `Calc` struct in `calc.rs`, you would define a new method called `square` in the `impl` block. This function will take a `Vec<f64>` and return a new `Vec<f64>` where each element is the square of the corresponding element in the input vector. Additionally, you will need to add a test case for this new function in the `test_all_operations` function.\n\nHere is how you can modify the `calc.rs` file:\n\n```rust\n// Adding new square method to the Calc struct\nimpl Calc {\n    // Existing methods: add, sub, mul, div\n\n    pub fn square(numbers: Vec<f64>) -> Vec<f64> {\n        numbers.into_iter().map(|x| x * x).collect()\n    }\n}\n\n// Adding test case for the new square method\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_all_operations() {\n        // Existing test cases for add, sub, mul, div\n\n        // Test case for square\n        let numbers = vec![1.0, 2.0, 3.0];\n        let squared = Calc::square(numbers.clone());\n        assert_eq!(squared, vec![1.0, 4.0, 9.0]);\n    }\n}\n```\n\nThis modification includes a new method `square` that maps each number in the input vector to its square and collects the results into a new vector. The added test case verifies that the `square` method works as expected."
                        }
                    ]
                }
            ]
        },
        {
            "name": "tree_traverse",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "test_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"English\",\n                        \"Language\",\n                        \"Natural\",\n                        \"Python\",\n                        \"TestKeywordExtract\",\n                        \"Toolkit\",\n                        \"annotated\",\n                        \"based\",\n                        \"class\",\n                        \"code\",\n                        \"contains\",\n                        \"create\",\n                        \"description\",\n                        \"ensuring\",\n                        \"expected\",\n                        \"extracted\",\n                        \"extraction\",\n                        \"extracts\",\n                        \"filtering\",\n                        \"filters\",\n                        \"identifies\",\n                        \"includes\",\n                        \"input\",\n                        \"keyword\",\n                        \"keywords\",\n                        \"list\",\n                        \"lists\",\n                        \"method\",\n                        \"nltk\",\n                        \"nouns\",\n                        \"output\",\n                        \"pieces\",\n                        \"processing\",\n                        \"provided\",\n                        \"query\",\n                        \"running\",\n                        \"script\",\n                        \"selecting\",\n                        \"stopwords\",\n                        \"tagging\",\n                        \"test\",\n                        \"text\",\n                        \"tokenizes\",\n                        \"written\"\n                    ],\n                    \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"Character\",\n                        \"Python\",\n                        \"attributes\",\n                        \"character\",\n                        \"class\",\n                        \"code\",\n                        \"created\",\n                        \"damage\",\n                        \"double_speed\",\n                        \"doubled\",\n                        \"doubles\",\n                        \"game\",\n                        \"health\",\n                        \"includes\",\n                        \"initialized\",\n                        \"instances\",\n                        \"models\",\n                        \"named\",\n                        \"ninja\",\n                        \"output\",\n                        \"parameters\",\n                        \"printed\",\n                        \"showcase\",\n                        \"speed\",\n                        \"speeds\",\n                        \"updated\",\n                        \"using\",\n                        \"warrior\",\n                        \"written\"\n                    ],\n                    \"annotation\": \"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"Expected\",\n                        \"JSON\",\n                        \"Python\",\n                        \"class\",\n                        \"code\",\n                        \"content\",\n                        \"contents\",\n                        \"converts\",\n                        \"create\",\n                        \"defines\",\n                        \"designed\",\n                        \"directories\",\n                        \"directory\",\n                        \"file\",\n                        \"file_to_string\",\n                        \"files\",\n                        \"functionality\",\n                        \"generates\",\n                        \"given\",\n                        \"leaf\",\n                        \"method\",\n                        \"model\",\n                        \"model_to_str\",\n                        \"named\",\n                        \"nodes\",\n                        \"output\",\n                        \"provided\",\n                        \"reads\",\n                        \"representing\",\n                        \"save_model_json\",\n                        \"self.test_path\",\n                        \"stores\",\n                        \"string\",\n                        \"structure\",\n                        \"test_codebase.json\",\n                        \"tests\",\n                        \"traversing\",\n                        \"treating\",\n                        \"tree\",\n                        \"writes\"\n                    ],\n                    \"annotation\": \"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\"test_codebase.json\\\" representing the directory structure of `self.test_path`.\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"Aggregation\",\n                \"Annotation\",\n                \"Answering\",\n                \"Call\",\n                \"CodeBase\",\n                \"Codesense\",\n                \"Compiles\",\n                \"Creates\",\n                \"Extraction\",\n                \"Generates\",\n                \"Generation\",\n                \"Graph\",\n                \"Identifies\",\n                \"Keyword\",\n                \"Produces\",\n                \"Question\",\n                \"Searches\",\n                \"Traversal\",\n                \"Tree\",\n                \"Uses\",\n                \"aggregated\",\n                \"analyze\",\n                \"annotation.7\",\n                \"annotations\",\n                \"answer\",\n                \"based\",\n                \"call\",\n                \"called\",\n                \"code\",\n                \"code.4\",\n                \"codebase\",\n                \"codebase.2\",\n                \"codebases\",\n                \"components\",\n                \"directed\",\n                \"document\",\n                \"file.3\",\n                \"flows\",\n                \"function\",\n                \"functions\",\n                \"graph.5\",\n                \"implementations\",\n                \"include\",\n                \"involves\",\n                \"keywords\",\n                \"matching\",\n                \"nodes\",\n                \"objectives\",\n                \"outlines\",\n                \"project\",\n                \"providing\",\n                \"queries.6\",\n                \"related\",\n                \"report\",\n                \"representing\",\n                \"returns\",\n                \"serves\",\n                \"showing\",\n                \"source\",\n                \"structure\",\n                \"summaries\",\n                \"target\",\n                \"tasks\",\n                \"tree\",\n                \"user\"\n            ],\n            \"annotation\": \"This document outlines a project called \\\"Codesense,\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\n\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\n3. Annotation Generation: Produces text summaries for functions in the code.\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\n\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Automatic\",\n                                \"Extraction\",\n                                \"Gensim\",\n                                \"Keyword\",\n                                \"NLP\",\n                                \"NLTK\",\n                                \"Python\",\n                                \"RAKE\",\n                                \"Rapid\",\n                                \"SSL\",\n                                \"algorithm\",\n                                \"certificate\",\n                                \"changing\",\n                                \"command\",\n                                \"commands\",\n                                \"consists\",\n                                \"downloading\",\n                                \"downloads\",\n                                \"environment\",\n                                \"error\",\n                                \"gensim\",\n                                \"install\",\n                                \"installing\",\n                                \"instructs\",\n                                \"involves\",\n                                \"issue\",\n                                \"language\",\n                                \"model\",\n                                \"occurs\",\n                                \"packages\",\n                                \"processing\",\n                                \"provided\",\n                                \"setting\",\n                                \"shell\",\n                                \"suggests\",\n                                \"text\",\n                                \"tokenization\",\n                                \"use\",\n                                \"version\",\n                                \"words\"\n                            ],\n                            \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"*\",\n                                \"Comparison\",\n                                \"Embeddings\",\n                                \"Extraction\",\n                                \"Gensim\",\n                                \"Keyword\",\n                                \"NLP\",\n                                \"NLTK\",\n                                \"Processing\",\n                                \"Python\",\n                                \"Similarity\",\n                                \"Text\",\n                                \"Word\",\n                                \"Word2Vec\",\n                                \"calculates\",\n                                \"code\",\n                                \"compare_keywords\",\n                                \"compare_words\",\n                                \"comparing\",\n                                \"computes\",\n                                \"console\",\n                                \"context\",\n                                \"create\",\n                                \"embeddings\",\n                                \"employs\",\n                                \"extract_keywords\",\n                                \"extracted\",\n                                \"extraction\",\n                                \"extracts\",\n                                \"file\",\n                                \"focuses\",\n                                \"function\",\n                                \"input\",\n                                \"keyword\",\n                                \"keywords\",\n                                \"keywords.The\",\n                                \"language\",\n                                \"libraries\",\n                                \"library\",\n                                \"list\",\n                                \"lists\",\n                                \"model\",\n                                \"modeling.1\",\n                                \"output\",\n                                \"performs\",\n                                \"processes\",\n                                \"processing\",\n                                \"reads\",\n                                \"removes\",\n                                \"returned\",\n                                \"score\",\n                                \"sentences\",\n                                \"similarity\",\n                                \"tagging\",\n                                \"techniques\",\n                                \"text\",\n                                \"texts\",\n                                \"tokenizes\",\n                                \"uses\",\n                                \"using\",\n                                \"vector\",\n                                \"verbs\",\n                                \"word\",\n                                \"words\",\n                                \"words.3\",\n                                \"written\"\n                            ],\n                            \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"C++\",\n                                \"Solution\",\n                                \"adds\",\n                                \"calculates\",\n                                \"captures\",\n                                \"class\",\n                                \"description\",\n                                \"difference\",\n                                \"element\",\n                                \"end\",\n                                \"function\",\n                                \"increase\",\n                                \"iterates\",\n                                \"list\",\n                                \"maxP\",\n                                \"maxProfit\",\n                                \"method\",\n                                \"opportunities\",\n                                \"price\",\n                                \"prices\",\n                                \"profit\",\n                                \"refers\",\n                                \"representing\",\n                                \"returns\",\n                                \"sets\",\n                                \"stock\"\n                            ],\n                            \"annotation\": \"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Additionally\",\n                                \"Face\",\n                                \"Hugging\",\n                                \"JSON\",\n                                \"NLTK\",\n                                \"Python\",\n                                \"README\",\n                                \"Transformers\",\n                                \"annotation\",\n                                \"annotations\",\n                                \"attributes\",\n                                \"called\",\n                                \"character\",\n                                \"code\",\n                                \"codebase\",\n                                \"codebases\",\n                                \"codesense\",\n                                \"components\",\n                                \"creating\",\n                                \"defined\",\n                                \"designed\",\n                                \"detailing\",\n                                \"directory\",\n                                \"employing\",\n                                \"environment\",\n                                \"expected\",\n                                \"extraction\",\n                                \"functionalities\",\n                                \"game\",\n                                \"generating\",\n                                \"generation\",\n                                \"include\",\n                                \"instructions\",\n                                \"involve\",\n                                \"keyword\",\n                                \"lists\",\n                                \"methods\",\n                                \"model\",\n                                \"modeling\",\n                                \"object\",\n                                \"outputs\",\n                                \"performing\",\n                                \"project\",\n                                \"provide\",\n                                \"provided\",\n                                \"representations\",\n                                \"representing\",\n                                \"scripts\",\n                                \"setting\",\n                                \"structure\",\n                                \"structured\",\n                                \"structures\",\n                                \"summaries\",\n                                \"tasks\",\n                                \"text\",\n                                \"using\",\n                                \"utilities\",\n                                \"working\"\n                            ],\n                            \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Hidden\",\n                                \"JSON\",\n                                \"Python\",\n                                \"called\",\n                                \"calls\",\n                                \"children\",\n                                \"code\",\n                                \"codebase.json\",\n                                \"contents\",\n                                \"converts\",\n                                \"create\",\n                                \"create_folder_structure_json\",\n                                \"creating\",\n                                \"designed\",\n                                \"dictionary\",\n                                \"directories\",\n                                \"directory\",\n                                \"file\",\n                                \"file_path\",\n                                \"file_to_string\",\n                                \"files\",\n                                \"folder\",\n                                \"folders\",\n                                \"found\",\n                                \"function\",\n                                \"given\",\n                                \"ignored\",\n                                \"including\",\n                                \"indentation\",\n                                \"list\",\n                                \"named\",\n                                \"names\",\n                                \"navigates\",\n                                \"nested\",\n                                \"object\",\n                                \"output\",\n                                \"path\",\n                                \"prints\",\n                                \"read\",\n                                \"reads\",\n                                \"representation\",\n                                \"representing\",\n                                \"returns\",\n                                \"saves\",\n                                \"script\",\n                                \"specifies\",\n                                \"starting\",\n                                \"string\",\n                                \"structure\",\n                                \"types\"\n                            ],\n                            \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"CodeLlama\",\n                                \"English\",\n                                \"Python\",\n                                \"accelerate\",\n                                \"code\",\n                                \"command\",\n                                \"environment\",\n                                \"install\",\n                                \"installed\",\n                                \"instruction\",\n                                \"libraries\",\n                                \"model\",\n                                \"packages\",\n                                \"pip\",\n                                \"plain\",\n                                \"provides\",\n                                \"required\",\n                                \"run\",\n                                \"running\",\n                                \"snippet\",\n                                \"transformers\",\n                                \"working\"\n                            ],\n                            \"annotation\": \"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Face\",\n                                \"First\",\n                                \"Hugging\",\n                                \"Llama\",\n                                \"Python\",\n                                \"Transformers\",\n                                \"characters\",\n                                \"code\",\n                                \"continuation\",\n                                \"enabled\",\n                                \"expected\",\n                                \"function\",\n                                \"generate\",\n                                \"generated\",\n                                \"generation\",\n                                \"imports\",\n                                \"language\",\n                                \"length\",\n                                \"library\",\n                                \"load\",\n                                \"model\",\n                                \"output\",\n                                \"pipeline\",\n                                \"prints\",\n                                \"prompt\",\n                                \"provided\",\n                                \"provides\",\n                                \"sampling\",\n                                \"text\",\n                                \"time\",\n                                \"transformers\",\n                                \"use\",\n                                \"uses\",\n                                \"written\"\n                            ],\n                            \"annotation\": \"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\"Once upon a time\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "top_1.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        }\n    ]\n}"
                },
                {
                    "name": "top_3.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        },\n        {\n            \"score\": 0.5,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"testkeywordextract\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"keyword_extract.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"English\",\n                    \"Language\",\n                    \"Natural\",\n                    \"Python\",\n                    \"TestKeywordExtract\",\n                    \"Toolkit\",\n                    \"annotated\",\n                    \"based\",\n                    \"class\",\n                    \"code\",\n                    \"contains\",\n                    \"create\",\n                    \"description\",\n                    \"ensuring\",\n                    \"expected\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"filtering\",\n                    \"filters\",\n                    \"identifies\",\n                    \"includes\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"list\",\n                    \"lists\",\n                    \"method\",\n                    \"nltk\",\n                    \"nouns\",\n                    \"output\",\n                    \"pieces\",\n                    \"processing\",\n                    \"provided\",\n                    \"query\",\n                    \"running\",\n                    \"script\",\n                    \"selecting\",\n                    \"stopwords\",\n                    \"tagging\",\n                    \"test\",\n                    \"text\",\n                    \"tokenizes\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"info.txt\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Automatic\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"RAKE\",\n                    \"Rapid\",\n                    \"SSL\",\n                    \"algorithm\",\n                    \"certificate\",\n                    \"changing\",\n                    \"command\",\n                    \"commands\",\n                    \"consists\",\n                    \"downloading\",\n                    \"downloads\",\n                    \"environment\",\n                    \"error\",\n                    \"gensim\",\n                    \"install\",\n                    \"installing\",\n                    \"instructs\",\n                    \"involves\",\n                    \"issue\",\n                    \"language\",\n                    \"model\",\n                    \"occurs\",\n                    \"packages\",\n                    \"processing\",\n                    \"provided\",\n                    \"setting\",\n                    \"shell\",\n                    \"suggests\",\n                    \"text\",\n                    \"tokenization\",\n                    \"use\",\n                    \"version\",\n                    \"words\"\n                ],\n                \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n            }\n        }\n    ]\n}"
                },
                {
                    "name": "top_5.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        },\n        {\n            \"score\": 0.5,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"testkeywordextract\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"keyword_extract.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"English\",\n                    \"Language\",\n                    \"Natural\",\n                    \"Python\",\n                    \"TestKeywordExtract\",\n                    \"Toolkit\",\n                    \"annotated\",\n                    \"based\",\n                    \"class\",\n                    \"code\",\n                    \"contains\",\n                    \"create\",\n                    \"description\",\n                    \"ensuring\",\n                    \"expected\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"filtering\",\n                    \"filters\",\n                    \"identifies\",\n                    \"includes\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"list\",\n                    \"lists\",\n                    \"method\",\n                    \"nltk\",\n                    \"nouns\",\n                    \"output\",\n                    \"pieces\",\n                    \"processing\",\n                    \"provided\",\n                    \"query\",\n                    \"running\",\n                    \"script\",\n                    \"selecting\",\n                    \"stopwords\",\n                    \"tagging\",\n                    \"test\",\n                    \"text\",\n                    \"tokenizes\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"info.txt\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Automatic\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"RAKE\",\n                    \"Rapid\",\n                    \"SSL\",\n                    \"algorithm\",\n                    \"certificate\",\n                    \"changing\",\n                    \"command\",\n                    \"commands\",\n                    \"consists\",\n                    \"downloading\",\n                    \"downloads\",\n                    \"environment\",\n                    \"error\",\n                    \"gensim\",\n                    \"install\",\n                    \"installing\",\n                    \"instructs\",\n                    \"involves\",\n                    \"issue\",\n                    \"language\",\n                    \"model\",\n                    \"occurs\",\n                    \"packages\",\n                    \"processing\",\n                    \"provided\",\n                    \"setting\",\n                    \"shell\",\n                    \"suggests\",\n                    \"text\",\n                    \"tokenization\",\n                    \"use\",\n                    \"version\",\n                    \"words\"\n                ],\n                \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"codebase.json\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Additionally\",\n                    \"Face\",\n                    \"Hugging\",\n                    \"JSON\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"README\",\n                    \"Transformers\",\n                    \"annotation\",\n                    \"annotations\",\n                    \"attributes\",\n                    \"called\",\n                    \"character\",\n                    \"code\",\n                    \"codebase\",\n                    \"codebases\",\n                    \"codesense\",\n                    \"components\",\n                    \"creating\",\n                    \"defined\",\n                    \"designed\",\n                    \"detailing\",\n                    \"directory\",\n                    \"employing\",\n                    \"environment\",\n                    \"expected\",\n                    \"extraction\",\n                    \"functionalities\",\n                    \"game\",\n                    \"generating\",\n                    \"generation\",\n                    \"include\",\n                    \"instructions\",\n                    \"involve\",\n                    \"keyword\",\n                    \"lists\",\n                    \"methods\",\n                    \"model\",\n                    \"modeling\",\n                    \"object\",\n                    \"outputs\",\n                    \"performing\",\n                    \"project\",\n                    \"provide\",\n                    \"provided\",\n                    \"representations\",\n                    \"representing\",\n                    \"scripts\",\n                    \"setting\",\n                    \"structure\",\n                    \"structured\",\n                    \"structures\",\n                    \"summaries\",\n                    \"tasks\",\n                    \"text\",\n                    \"using\",\n                    \"utilities\",\n                    \"working\"\n                ],\n                \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Hidden\",\n                    \"JSON\",\n                    \"Python\",\n                    \"called\",\n                    \"calls\",\n                    \"children\",\n                    \"code\",\n                    \"codebase.json\",\n                    \"contents\",\n                    \"converts\",\n                    \"create\",\n                    \"create_folder_structure_json\",\n                    \"creating\",\n                    \"designed\",\n                    \"dictionary\",\n                    \"directories\",\n                    \"directory\",\n                    \"file\",\n                    \"file_path\",\n                    \"file_to_string\",\n                    \"files\",\n                    \"folder\",\n                    \"folders\",\n                    \"found\",\n                    \"function\",\n                    \"given\",\n                    \"ignored\",\n                    \"including\",\n                    \"indentation\",\n                    \"list\",\n                    \"named\",\n                    \"names\",\n                    \"navigates\",\n                    \"nested\",\n                    \"object\",\n                    \"output\",\n                    \"path\",\n                    \"prints\",\n                    \"read\",\n                    \"reads\",\n                    \"representation\",\n                    \"representing\",\n                    \"returns\",\n                    \"saves\",\n                    \"script\",\n                    \"specifies\",\n                    \"starting\",\n                    \"string\",\n                    \"structure\",\n                    \"types\"\n                ],\n                \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n            }\n        }\n    ]\n}"
                },
                {
                    "name": "tree_traverse.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import sys\n\nsys.path.insert(0, \"..\")\nfrom utilities.utility import obj_to_json, json_to_obj\n\n'''\nCreate a class to find the most relevant node in the codebase model given some keywords\n- input:\n    - list of keywords\n    - codebase model object\n- output:\n    - object of top nodes containing: file_name, annotation, content, and matching_keywords\n'''\n\n\nclass TraverseCodebase:\n    def __init__(self, model_obj):\n        self.model = model_obj\n        self.top_nodes_with_score = []\n        self.result_file_name = \"result\"\n\n    def compute_score(self, target_keywords, input_keywords):\n        score = 0\n        if not input_keywords or not target_keywords:  # handle empty list\n            return score\n        # input keywords should already be lowered, so only lower target_keywords\n        target_keywords = self.convert_keywords_to_lowercase(target_keywords)\n        for word in input_keywords:\n            if word in target_keywords:\n                score += 1\n        return score/len(input_keywords)\n\n    def get_matched_keywords(self, target_keywords, input_keywords):\n        target_keywords_lowered = self.convert_keywords_to_lowercase(\n            target_keywords)\n        input_set = set(input_keywords)\n        target_set = set(target_keywords_lowered)\n        # find intersection\n        common_elems = input_set.intersection(target_set)\n        return list(common_elems)\n\n    def get_top_nodes(self, input_keywords, n):\n        # we need to reset the top_nodes list to empty so that multiple calls of this method don't append to it\n        self.top_nodes_with_score = []\n        # lower input keywords to compute score properly\n        input_keywords_lowered = self.convert_keywords_to_lowercase(\n            input_keywords)\n        # recursively populate top_nodes list\n        self._get_top_nodes(self.model, input_keywords_lowered)\n        # after traversal sort top_nodes list by score in descending order\n        self.top_nodes_with_score.sort(key=lambda x: x[0], reverse=True)  \n        # return result\n        return self.build_result(n, input_keywords_lowered)\n\n    def _get_top_nodes(self, model, input_keywords):\n        if model[\"type\"] == \"file\":\n            # get matching keyword score\n            score = self.compute_score(model[\"keywords\"], input_keywords)\n            self.top_nodes_with_score.append((score, model))\n            return model\n        else:\n            for child in model[\"children\"]:\n                self._get_top_nodes(child, input_keywords)\n\n    def build_result(self, n, input_keywords):\n        result = {\"input_keywords\": input_keywords, \"results\": []}\n        for entry in self.top_nodes_with_score:\n            score = entry[0]\n            node = entry[1]\n            # add matched keywords attribute\n            matched_keywords = self.get_matched_keywords(\n                node[\"keywords\"], input_keywords)\n            entry = {'score': score, 'matched_keywords': matched_keywords, 'node': node}\n            result[\"results\"].append(entry)\n        result[\"results\"] = result[\"results\"][:n]\n        return result\n\n    def convert_keywords_to_lowercase(self, keywords):\n        return [word.lower() for word in keywords]\n\n\nclass TestTraverseCodebase:\n    def __init__(self):\n        self.test_model = json_to_obj(\"test_codebase.json\")\n        self.traverser = TraverseCodebase(self.test_model)\n\n    def test_save_top_1_nodes(self):\n        print(f\"Testing Traverse Codebase to save top 1 nodes\")\n        input_keywords = [\"Python\", \"function\", \"TestKeywordExtract\",\n                          \"NLTK\", \"Word2Vec\", \"extract_keywords\"]\n        updated_model = self.traverser.get_top_nodes(input_keywords, 1)\n        obj_to_json(\"./\", \"top_1\", updated_model)\n        assert type(updated_model) == dict\n\n    def test_save_top_3_nodes(self):\n        print(f\"Testing Traverse Codebase to save top 3 nodes\")\n        input_keywords = [\"Python\", \"function\", \"TestKeywordExtract\",\n                          \"NLTK\", \"Word2Vec\", \"extract_keywords\"]\n        updated_model = self.traverser.get_top_nodes(input_keywords, 3)\n        obj_to_json(\"./\", \"top_3\", updated_model)\n        assert type(updated_model) == dict\n\n    def test_save_top_5_nodes(self):\n        print(f\"Testing Traverse Codebase to save top 5 nodes\")\n        input_keywords = [\"Python\", \"function\", \"TestKeywordExtract\",\n                          \"NLTK\", \"Word2Vec\", \"extract_keywords\"]\n        updated_model = self.traverser.get_top_nodes(input_keywords, 5)\n        obj_to_json(\"./\", \"top_5\", updated_model)\n        assert type(updated_model) == dict\n\n\nif __name__ == \"__main__\":\n    testTraverseCodebase = TestTraverseCodebase()\n    testTraverseCodebase.test_save_top_1_nodes()\n    testTraverseCodebase.test_save_top_3_nodes()\n    testTraverseCodebase.test_save_top_5_nodes()\n\n"
                }
            ]
        },
        {
            "name": "utilities",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "test.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"data\": \"test\"\n}"
                },
                {
                    "name": "test2.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"data\": \"test2\"\n}"
                },
                {
                    "name": "utility.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import json\nimport os\n\n'''\nCreate a utilty methods that can be used across multiple classes\n'''\n\n\ndef obj_to_json(output_path, filename, obj):\n    # makesure filename doesn't have .json extension\n    # makesure output_path has trailing backslash\n    output_file_path = os.path.join(output_path, f\"{filename}.json\")\n    save_file = open(output_file_path, 'w')\n    json.dump(obj, save_file, indent=4)\n    save_file.close()\n    print(f\"json file saved: {output_file_path}\")\n    # return output file path for debugging purposes\n    return output_file_path\n\n\ndef json_to_obj(json_file_path):\n    d = {}\n    with open(json_file_path) as json_data:\n        d = json.load(json_data)\n    return d\n\n\ndef file_to_string(file_path):  # save file content as string\n    with open(file_path, 'r') as file:\n        file_content = file.read()\n    file.close()\n    return file_content\n\n\nclass TestUtility:\n    def __init__(self):\n        self.test_json_file = \"test.json\"\n\n    def test_json_to_obj(self):\n        test_obj = json_to_obj(self.test_json_file)\n        assert test_obj[\"data\"] == \"test\"\n\n    def test_obj_to_json(self):\n        # load object & modify it\n        test_obj = json_to_obj(self.test_json_file)\n        test_obj[\"data\"] = \"test2\"\n        # write object\n        obj_to_json(\"./\", \"test2\", test_obj)\n        # verify if object was written correctly\n        assert json_to_obj(\"test2.json\")[\"data\"] == \"test2\"\n\n    def test_file_to_string(self):\n        test_str = file_to_string(self.test_json_file)\n        # print(test_str)\n        expected_str = '''{\n    \"data\": \"test\"\n}'''\n        assert test_str == expected_str\n\n\nif __name__ == \"__main__\":\n    testUtility = TestUtility()\n    testUtility.test_json_to_obj()\n    testUtility.test_obj_to_json()\n    testUtility.test_file_to_string()\n"
                }
            ]
        }
    ]
}