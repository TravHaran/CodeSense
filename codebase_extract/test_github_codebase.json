{
    "name": "codesense",
    "path": "codesense/",
    "type": "folder",
    "keywords": [],
    "children": [
        {
            "name": "README.md",
            "path": "codesense/README.md",
            "type": "file",
            "keywords": [],
            "annotation": "",
            "content": "# Project Codesense\n\n## Breakdown\n\n### 1. CodeBase Tree Extraction\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\n### 2. Call Graph Extraction\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\n### 3. Annotation Generation\n    - for a fucntion defined in code generate a text summarization\n### 4. Annotation Aggregation\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\n### 5. Keyword Extraction\n    - from the aggregated annotation report extract a list of keywords\n    - from a usery query extract a list of keywords\n### 6. Tree Traversal\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\n    - once the target node has been found, return it's corresponding aggregated annotation report\n### 7. Question Answering\n    - given the aggregated annoation report as context, provide an answer to the user's query."
        },
        {
            "name": "annotation_aggregate",
            "path": "codesense/annotation_aggregate",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "annotation_aggregate.py",
                    "path": "codesense/annotation_aggregate/annotation_aggregate.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import sys\n\nsys.path.insert(0, \"..\")\nfrom utilities.utility import json_to_obj\n\n'''\nCreate a class to aggregate the annotations of some target nodes\n- input:\n    - search_result object containing most relevant nodes with annotations \n- output:\n    - an aggregate of all the relevant annotations in string format\n    - optionally save output as txt file\n'''\n\nclass AnnotationAggregate:\n    def __init__(self, traverse_obj):\n        self.result_model = traverse_obj\n        self.annotations = []\n    \n    def aggregate_annotations(self):\n        for entry in self.result_model[\"results\"]:\n            node = entry[\"node\"]\n            self.annotations.append((node[\"name\"], node[\"annotation\"]))\n        return self.format_output()\n    \n    def format_output(self):\n        output = \"Relevant Files: \\n\\n\"\n        count = 0\n        for entry in self.annotations:\n            count += 1\n            name = entry[0]\n            annotation = entry[1]\n            output += str(f\"FILENAME: {name}\\nDESCRIPTION: \\\"{annotation}\\\"\\n\\n\")\n        return output\n\n\nclass TestAnnotationAggregate:\n    def test_aggreagate_top_1_results(self):\n        test_traverse_obj = json_to_obj(\"top_1.json\")\n        aggregator = AnnotationAggregate(test_traverse_obj)\n        print(\"\\nTesting Aggregation of top 1 results:\")\n        result = aggregator.aggregate_annotations()\n        print(result)\n    def test_aggreagate_top_3_results(self):\n        test_traverse_obj = json_to_obj(\"top_3.json\")\n        aggregator = AnnotationAggregate(test_traverse_obj)\n        print(\"\\nTesting Aggregation of top 3 results:\")\n        print(aggregator.aggregate_annotations())\n\nif __name__ == \"__main__\":\n    testAnnotationAggregate = TestAnnotationAggregate()\n    testAnnotationAggregate.test_aggreagate_top_1_results()\n    testAnnotationAggregate.test_aggreagate_top_3_results()"
                },
                {
                    "name": "top_1.json",
                    "path": "codesense/annotation_aggregate/top_1.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        }\n    ]\n}"
                },
                {
                    "name": "top_3.json",
                    "path": "codesense/annotation_aggregate/top_3.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        },\n        {\n            \"score\": 0.5,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"testkeywordextract\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"keyword_extract.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"English\",\n                    \"Language\",\n                    \"Natural\",\n                    \"Python\",\n                    \"TestKeywordExtract\",\n                    \"Toolkit\",\n                    \"annotated\",\n                    \"based\",\n                    \"class\",\n                    \"code\",\n                    \"contains\",\n                    \"create\",\n                    \"description\",\n                    \"ensuring\",\n                    \"expected\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"filtering\",\n                    \"filters\",\n                    \"identifies\",\n                    \"includes\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"list\",\n                    \"lists\",\n                    \"method\",\n                    \"nltk\",\n                    \"nouns\",\n                    \"output\",\n                    \"pieces\",\n                    \"processing\",\n                    \"provided\",\n                    \"query\",\n                    \"running\",\n                    \"script\",\n                    \"selecting\",\n                    \"stopwords\",\n                    \"tagging\",\n                    \"test\",\n                    \"text\",\n                    \"tokenizes\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"info.txt\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Automatic\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"RAKE\",\n                    \"Rapid\",\n                    \"SSL\",\n                    \"algorithm\",\n                    \"certificate\",\n                    \"changing\",\n                    \"command\",\n                    \"commands\",\n                    \"consists\",\n                    \"downloading\",\n                    \"downloads\",\n                    \"environment\",\n                    \"error\",\n                    \"gensim\",\n                    \"install\",\n                    \"installing\",\n                    \"instructs\",\n                    \"involves\",\n                    \"issue\",\n                    \"language\",\n                    \"model\",\n                    \"occurs\",\n                    \"packages\",\n                    \"processing\",\n                    \"provided\",\n                    \"setting\",\n                    \"shell\",\n                    \"suggests\",\n                    \"text\",\n                    \"tokenization\",\n                    \"use\",\n                    \"version\",\n                    \"words\"\n                ],\n                \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n            }\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "annotation_generate",
            "path": "codesense/annotation_generate",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "annotation_generate.py",
                    "path": "codesense/annotation_generate/annotation_generate.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "from openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n'''\nCreate a class to annotate a piece of given code\n- input:\n    - Code (Function, Class, etc.)\n- output:\n    - Summary of code in text\n'''\n\nclass AnnotationGeneration:\n    def __init__(self):\n        self.res = \"\"\n\n    def snippet_summary(self, snippet):\n            ## Set the API Key\n            load_dotenv()\n            API_KEY = os.getenv('OPENAI_SECRET_API_KEY')\n            client = OpenAI(api_key=API_KEY)\n\n            #GPT4o REPONSE REQUEST\n            MODEL=\"gpt-4o\"\n\n            completion = client.chat.completions.create(\n            model=MODEL,\n            #Prompt modelling, grounding the model to provide a more concise and clear summary when given a piece of code\n            messages=[\n                {\"role\": \"system\", \"content\": '''You are an AI designed to explain code clearly and concisely. When given a piece of code, your task is to provide a quick summary without giving a detailed breakdown. Your summary should include the programming language, the purpose of the code, a brief explanation of its key components functionality and logic, and the expected output. Respond in a single blurb of text.\n\n            Here is the piece of code for you to explain:\n\n            python\n            Copy code\n            def is_prime(n):\n                if n <= 1:\n                    return False\n                for i in range(2, int(n**0.5) + 1):\n                    if n % i == 0:\n                        return False\n                return True\n\n            number = 7\n            result = is_prime(number)\n            print(f\"Is {number} a prime number? {result}\")\n            Expected Explanation:\n\n            The code is written in Python and checks if a given number is a prime number. The function is_prime(n) returns True if n is prime by testing divisibility from 2 to the square root of n, otherwise it returns False. The variable number is set to 7, and the function is called to check if 7 is prime, with the result printed. The output will be: \"Is 7 a prime number? True\".'''},\n                \n                {\"role\": \"user\", \"content\": f'''With that said. Explain the given code:\n                    {snippet}\n                '''}\n            ]\n            )\n            return completion.choices[0].message.content\n    \n\n### TESTING \nclass TestSnippetSummary:\n     def __init__(self):\n        self.summarizer = AnnotationGeneration()\n        print(\"Testing Snippet Summarizer... \\n\")\n    \n        #High-depth code is code that has many variables, refers to many functions, changes variables, and is overall complex to understand from a quick view\n     def test_snippet_summarizer_from_high_depth_code(self):\n          print(\"Testing code snippet summarizer with a high depth function... \\n\")\n          code_snippet = '''\n          const handleCreateEvent = async(e) => {\n            e.preventDefault()\n            // console.log(user)\n            // const dateNow = new Date()\n            // var date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\"\\\"\", \"\").replace(\"\\\"\", \"\");\n                if (isDateRange == false){\n                    await addDoc(collection(db, 'schools', completeSchoolName, 'events'), {\n                      title: title,\n                      description: description,\n                      dateTime: dateTime,\n                      location: location,\n                      club: club,\n                      dateAdded: Date().toLocaleString(),\n                      // createdBy: {name: user.displayName, email: user.email}\n                  }).then(setTitle(\"\")).then(setDescription(\"\")).then(setDateTime(\"\")).then(setStartDateTime(\"\")).then(setEndDateTime(\"\")).then(setLocation(\"\")).then(setClub(\"\")).then(fetchUser())\n                  \n                  const chosenDate = new Date(dateTime)\n                  var dateNow = new Date(chosenDate)\n                  dateNow.setDate(chosenDate.getDate() + 1)\n                  const date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\"\\\"\", \"\").replace(\"\\\"\", \"\")\n                  await setDoc(doc(db, 'schools', completeSchoolName, 'announcements', date), {\n                    notes: [{\n                      title: title,\n                      description: description,\n                    club: club}],  \n                    createdBy: {name: user.name, email: user.email},\n                      dateAdded: Date().toLocaleString(),\n                      // createdBy: {name: user.displayName, email: user.email}\n                  }).then(setTitle(\"\")).then(setDescription(\"\")).then(setDateTime(\"\")).then(setLocation(\"\")).then(setClub(\"\")).then(fetchUser())\n\n                } else {\n                  await addDoc(collection(db, 'schools', completeSchoolName, 'events'), {\n                    title: title,\n                    description: description,\n                    startDate: startDateTime,\n                    endDate: endDateTime,\n                    location: location,\n                    club: club,\n                    dateAdded: Date().toLocaleString(),\n                    // createdBy: {name: user.displayName, email: user.email}\n                }).then(setTitle(\"\")).then(setDescription(\"\")).then(setDateTime(\"\")).then(setStartDateTime(\"\")).then(setEndDateTime(\"\")).then(setLocation(\"\")).then(setClub(\"\")).then(fetchUser())\n                \n                const chosenDate = new Date(startDateTime)\n                var dateNow = new Date(chosenDate)\n                dateNow.setDate(chosenDate.getDate() + 1)\n                const date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\"\\\"\", \"\").replace(\"\\\"\", \"\")\n                await setDoc(doc(db, 'schools', completeSchoolName, 'announcements', date), {\n                  notes: [{\n                    title: title,\n                    description: description,\n                  club: club}],  \n                  createdBy: {name: user.name, email: user.email},\n                    dateAdded: Date().toLocaleString(),\n                    // createdBy: {name: user.displayName, email: user.email}\n                }).then(setTitle(\"\")).then(setDescription(\"\")).then(setDateTime(\"\")).then(setLocation(\"\")).then(setClub(\"\")).then(fetchUser())\n\n                }\n                \n      }'''\n          output = self.summarizer.snippet_summary(code_snippet)\n          print(f\"CODE SUMMARY: \\n{output} \\n\\n\")\n          assert type(output) == str\n\n          '''\n          EXPECTED OUTPUT:The code is written in JavaScript, specifically using the async/await syntax to handle asynchronous operations with Firestore, a cloud database from Firebase. It defines an event-handling function `handleCreateEvent` meant to create and save event data into the Firestore database. When a form submission event triggers the function, it first prevents the default behavior with `e.preventDefault()`. \n          The function checks if `isDateRange` is false and, based on this, either adds or updates single or range-dated event documents in the Firestore under the 'events' collection. It also conditionally updates the 'announcements' collection based on the existence of date ranges. After database operations, it resets multiple state variables (title, description, dateTime, etc.) and fetches user data. \n          The function ensures newly created or modified data incorporates the current date and time and user metadata. The expected result includes adding appropriate entries in the Firestore under both 'events' and 'announcements' collections and resetting the form's state. \n          '''\n\n\n     #Low-depth code is code that is fairly simple, not many variables, and easy to follow \n     def test_snippet_summarizer_from_low_depth_code(self):\n          print(\"Testing code snippet summarizer with a low depth function... \\n\")\n          code_snippet = '''\n          class Solution:\n            def topK(self, nums: List[int], k: int) -> List[int]:\n                count = {}\n                freq = [[] for i in range (len(nums) + 1)]\n       \n            for n in nums:\n                count[n] = 1 + count.get(n, 0)\n            for n, c in count.items():\n                freq[c].append(n)\n       \n            res = []\n       \n            for i in range(len(freq) - 1, 0, -1):\n                for n in freq[i]:\n                    res.append(n)\n                    if len(res) == k:\n                        return res'''\n          output = self.summarizer.snippet_summary(code_snippet)\n          print(f\"CODE SUMMARY: \\n{output} \\n\\n\")\n          assert type(output) == str\n          '''\n          EXPECTED OUTPUT:\n          The code is written in Python and defines a method `topK` within a class `Solution`. The purpose of this method is to return the top k most frequent integers from a given list `nums`. \n          It first creates a `count` dictionary to tally the frequency of each number in `nums`, then organizes these frequencies into a list of lists `freq`. \n          The method then iterates through `freq` in reverse order to gather the k most frequent numbers into the result list `res`, which is returned once it reaches the desired length k. \n          '''\n    \n\nif __name__ == \"__main__\":\n    TestSnippetSummary = TestSnippetSummary()\n    TestSnippetSummary.test_snippet_summarizer_from_low_depth_code()\n    TestSnippetSummary.test_snippet_summarizer_from_high_depth_code()\n"
                },
                {
                    "name": "info.txt",
                    "path": "codesense/annotation_generate/info.txt",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "pip install:\n    python-dotenv\n    openai\n\nmake sure you have the .env file containing the openai secret key in this directory"
                }
            ]
        },
        {
            "name": "app.py",
            "path": "codesense/app.py",
            "type": "file",
            "keywords": [],
            "annotation": "",
            "content": "from codebase_extract.codebase_extract import CodebaseExtract\nfrom codebase_extract.github_codebase_extract import CodeBaseExtractGithub\nfrom populate_annotations.populate_annotations import PopulateAnnotations\nfrom populate_keywords.populate_keywords import PopulateKeywords\nfrom keyword_extract.keyword_extract import KeywordExtract\nfrom tree_traverse.tree_traverse import TraverseCodebase\nfrom question_answering.question_answer import QueryAnswer\nfrom utilities.utility import obj_to_json\n\nclass App:\n    def model_code_base(self, code_base_path: str, ignore_paths) -> dict:\n        # Extract Codebase\n        if code_base_path.startswith(\"https://github.com\"):\n            codebase_extractor = CodeBaseExtractGithub(code_base_path)\n        else: # a local directory path was passed\n            codebase_extractor = CodebaseExtract(code_base_path)\n        code_base_model = codebase_extractor.get_model()\n        # Populate Annotations\n        populate_annotations = PopulateAnnotations(code_base_model, ignore_paths)\n        code_base_model = populate_annotations.populate_model()\n        # Populate Keywords\n        populate_keywords = PopulateKeywords(code_base_model)\n        code_base_model = populate_keywords.populate_model()\n        # # Save final model\n        # obj_to_json(\"./out\", \"codebase\", code_base_model)\n        return code_base_model\n    \n    def query_code_base(self, code_base_model: dict, question: str, search_result_limit: int) -> str:\n        # Extract Keywords\n        extract_keywords = KeywordExtract()\n        query_keywords = extract_keywords.extract(question)\n        # Traverse Tree\n        traverser = TraverseCodebase(code_base_model)\n        search_result = traverser.get_top_nodes(query_keywords, search_result_limit)\n        # Question Answer\n        responder = QueryAnswer(search_result)\n        response = responder.get_response(question)\n        return response\n\nclass TestApp:\n    def __init__(self):\n        self.test_github_repo = \"https://github.com/TravHaran/rust-calculator\"\n        self.test_ignore = {\"ignore\" : []}\n        self.app = App()\n    \n    def test_run(self):\n        print(f\"modelling codebase from repo: {self.test_github_repo}\")\n        model = self.app.model_code_base(self.test_github_repo, self.test_ignore)\n        #Q1\n        question = \"Does this project have a multiplication capability?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = self.app.query_code_base(model, question, 3)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        #Q2\n        question = \"does it have a square operation functionality?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = self.app.query_code_base(model, question, 3)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        #Q3\n        question = \"how would we modify the code to add a square function?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = self.app.query_code_base(model, question, 3)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        \nif __name__ == \"__main__\":\n    testApp = TestApp()\n    testApp.test_run()     \n        \n"
        },
        {
            "name": "codebase_extract",
            "path": "codesense/codebase_extract",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "codebase_extract.py",
                    "path": "codesense/codebase_extract/codebase_extract.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import os\nimport sys\nsys.path.insert(0, \"..\")\n\nfrom utilities.utility import obj_to_json, file_to_string\n\n'''\nCreate a class to extract a model of a codebase as a tree\n- input: local directory path as a string\n- output: \n    - object containing tree structure of directory\n    - at leaf nodes store content of file as a string (if it's content is readable)\n'''\n\n\nclass CodebaseExtract:\n    def __init__(self, path):\n        # Initialize the output dictionary model with folder contents\n        # name, type, keywords, and empty list for children\n        self.path = path\n        self.model = {}\n    \n    def get_model(self):\n        self.model = self._build_model(self.path)\n        return self.model\n\n    def _build_model(self, path):  # extracts a directory as a json object\n        model = {'name': os.path.basename(path),\n                 'type': 'folder', 'keywords': [], 'children': []}\n        # Check if the path is a directory\n        if not os.path.isdir(path):\n            return model\n        # Iterate over the entries in the directory\n        for entry in os.listdir(path):\n            if not entry.startswith('.'): # ignore hidden folders & files\n                # Create the file path for current entry\n                entry_path = os.path.join(path, entry)\n                # if the entry is a directory, recursively call the function\n                if os.path.isdir(entry_path):\n                    model['children'].append(self._build_model(entry_path))\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\n                else:\n                    content = \"\"\n                    # save file content as string\n                    try:\n                        content = file_to_string(entry_path)\n                    except Exception: # handle unreadable file content\n                        content = \"n/a\"\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\n                    ], 'annotation': \"\", 'content': content})\n        return model\n\n\nclass TestCodebaseExtract:\n    def __init__(self):\n        self.test_path = \"../../codesense\"\n        self.extractor = CodebaseExtract(self.test_path)\n        print(\"Testing Codebase Extractor...\\n\")\n\n    def test_extract_codebase(self):\n        print(\"Testing codebase extraction of current project directory...\\n\")\n        output = self.extractor.get_model()\n        obj_to_json(\"./\", \"test_codebase\", output)\n        assert type(output) == dict\n\n\nif __name__ == \"__main__\":\n    testCodebaseExtract = TestCodebaseExtract()\n    testCodebaseExtract.test_extract_codebase()\n"
                },
                {
                    "name": "dir_content.json",
                    "path": "codesense/codebase_extract/dir_content.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "[\n    {\n        \"name\": \"test_codebase.json\",\n        \"path\": \"tree_traverse/test_codebase.json\",\n        \"sha\": \"5ae0369d5ac1f17547aee2c41acfb08285cdb5b7\",\n        \"size\": 66321,\n        \"url\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/test_codebase.json?ref=main\",\n        \"html_url\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/test_codebase.json\",\n        \"git_url\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/5ae0369d5ac1f17547aee2c41acfb08285cdb5b7\",\n        \"download_url\": \"https://raw.githubusercontent.com/TravHaran/codesense/main/tree_traverse/test_codebase.json\",\n        \"type\": \"file\",\n        \"_links\": {\n            \"self\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/test_codebase.json?ref=main\",\n            \"git\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/5ae0369d5ac1f17547aee2c41acfb08285cdb5b7\",\n            \"html\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/test_codebase.json\"\n        }\n    },\n    {\n        \"name\": \"top_1.json\",\n        \"path\": \"tree_traverse/top_1.json\",\n        \"sha\": \"a4410c886f3d01593f9d4585b189f9fa329534b5\",\n        \"size\": 7242,\n        \"url\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/top_1.json?ref=main\",\n        \"html_url\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/top_1.json\",\n        \"git_url\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/a4410c886f3d01593f9d4585b189f9fa329534b5\",\n        \"download_url\": \"https://raw.githubusercontent.com/TravHaran/codesense/main/tree_traverse/top_1.json\",\n        \"type\": \"file\",\n        \"_links\": {\n            \"self\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/top_1.json?ref=main\",\n            \"git\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/a4410c886f3d01593f9d4585b189f9fa329534b5\",\n            \"html\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/top_1.json\"\n        }\n    },\n    {\n        \"name\": \"top_3.json\",\n        \"path\": \"tree_traverse/top_3.json\",\n        \"sha\": \"ad53c205934a0a1315fb35bc857c3ba352a8c03b\",\n        \"size\": 15275,\n        \"url\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/top_3.json?ref=main\",\n        \"html_url\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/top_3.json\",\n        \"git_url\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/ad53c205934a0a1315fb35bc857c3ba352a8c03b\",\n        \"download_url\": \"https://raw.githubusercontent.com/TravHaran/codesense/main/tree_traverse/top_3.json\",\n        \"type\": \"file\",\n        \"_links\": {\n            \"self\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/top_3.json?ref=main\",\n            \"git\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/ad53c205934a0a1315fb35bc857c3ba352a8c03b\",\n            \"html\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/top_3.json\"\n        }\n    },\n    {\n        \"name\": \"top_5.json\",\n        \"path\": \"tree_traverse/top_5.json\",\n        \"sha\": \"10b4ca71266ebba31790437e8f604476c8fdbe8f\",\n        \"size\": 42949,\n        \"url\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/top_5.json?ref=main\",\n        \"html_url\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/top_5.json\",\n        \"git_url\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/10b4ca71266ebba31790437e8f604476c8fdbe8f\",\n        \"download_url\": \"https://raw.githubusercontent.com/TravHaran/codesense/main/tree_traverse/top_5.json\",\n        \"type\": \"file\",\n        \"_links\": {\n            \"self\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/top_5.json?ref=main\",\n            \"git\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/10b4ca71266ebba31790437e8f604476c8fdbe8f\",\n            \"html\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/top_5.json\"\n        }\n    },\n    {\n        \"name\": \"tree_traverse.py\",\n        \"path\": \"tree_traverse/tree_traverse.py\",\n        \"sha\": \"e3e75f4563352e78a0560efcf214955e83049c89\",\n        \"size\": 4787,\n        \"url\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/tree_traverse.py?ref=main\",\n        \"html_url\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/tree_traverse.py\",\n        \"git_url\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/e3e75f4563352e78a0560efcf214955e83049c89\",\n        \"download_url\": \"https://raw.githubusercontent.com/TravHaran/codesense/main/tree_traverse/tree_traverse.py\",\n        \"type\": \"file\",\n        \"_links\": {\n            \"self\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/tree_traverse.py?ref=main\",\n            \"git\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/e3e75f4563352e78a0560efcf214955e83049c89\",\n            \"html\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/tree_traverse.py\"\n        }\n    }\n]"
                },
                {
                    "name": "file_content.json",
                    "path": "codesense/codebase_extract/file_content.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"tree_traverse.py\",\n    \"path\": \"tree_traverse/tree_traverse.py\",\n    \"sha\": \"e3e75f4563352e78a0560efcf214955e83049c89\",\n    \"size\": 4787,\n    \"url\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/tree_traverse.py?ref=main\",\n    \"html_url\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/tree_traverse.py\",\n    \"git_url\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/e3e75f4563352e78a0560efcf214955e83049c89\",\n    \"download_url\": \"https://raw.githubusercontent.com/TravHaran/codesense/main/tree_traverse/tree_traverse.py\",\n    \"type\": \"file\",\n    \"content\": \"aW1wb3J0IHN5cwoKc3lzLnBhdGguaW5zZXJ0KDAsICIuLiIpCmZyb20gdXRp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\\nZV90b3BfNV9ub2RlcygpCgo=\\n\",\n    \"encoding\": \"base64\",\n    \"_links\": {\n        \"self\": \"https://api.github.com/repos/TravHaran/codesense/contents/tree_traverse/tree_traverse.py?ref=main\",\n        \"git\": \"https://api.github.com/repos/TravHaran/codesense/git/blobs/e3e75f4563352e78a0560efcf214955e83049c89\",\n        \"html\": \"https://github.com/TravHaran/codesense/blob/main/tree_traverse/tree_traverse.py\"\n    }\n}"
                },
                {
                    "name": "github_codebase_extract.py",
                    "path": "codesense/codebase_extract/github_codebase_extract.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import sys\nimport os\nimport base64\nfrom urllib.parse import urlparse\nfrom dotenv import load_dotenv\nimport requests\n\nsys.path.insert(0, \"..\")\n\nfrom utilities.utility import obj_to_json\n\nclass CodeBaseExtractGithub:\n    def __init__(self, github_repo):\n        load_dotenv()\n        self.token = os.getenv('GITHUB_API_KEY') # make sure .env file contains api key\n        # ex: https://github.com/TravHaran/codesense\n        self.owner, self.repo_name = self.extract_owner_and_repo(github_repo)\n        self.model = {}\n    \n    def extract_owner_and_repo(self, url):\n        parsed_url = urlparse(url)\n        path_parts = parsed_url.path.strip('/').split('/')\n        \n        if len(path_parts) >= 2:\n            owner = path_parts[0]\n            repo_name = path_parts[1]\n            return owner, repo_name\n        else:\n            raise ValueError(\"Invalid GitHub URL format\")\n    \n    def get_content(self, path):\n        headers = {\"Authorization\" : \"token {}\".format(self.token)}\n        url = f\"https://api.github.com/repos/{self.owner}/{self.repo_name}/contents/{path}\"\n        content = requests.get(url, headers=headers)\n        return content.json()\n    \n    def content_is_dir(self, content):\n        return type(content) == list\n     \n    def get_model(self):\n        path_content = self.get_content(\"\")\n        self.model = self._build_model(\"\", path_content)\n        return self.model\n    \n    def _build_model(self, path, content):\n        model = {'name': os.path.basename(path),\n                 'type': 'folder', 'keywords': [], 'children': []}\n        # Check if the path is a directory\n        if not self.content_is_dir(content):\n            return model\n        # Iterate over the entries in the directory\n        for entry in content:\n            name = entry[\"name\"]\n            if not name.startswith('.'): # ignore hidden folders and files\n                # Create the file path for current entry\n                entry_path = os.path.join(path, name)\n                # if the entry is a directory, recursively call the function\n                entry_content = self.get_content(entry_path)\n                if self.content_is_dir(entry_content):\n                    model['children'].append(self._build_model(entry_path, entry_content))\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\n                else:\n                    content = \"\"\n                    # the file content is from the api response is encoded in base64\n                    try:\n                        content = base64.b64decode(entry_content[\"content\"]).decode('utf-8')\n                        # content = entry_content[\"content\"]\n                    except Exception: # handle decode errors\n                        content = \"n/a\"\n                    model['children'].append({'name': name, 'type': 'file', 'keywords': [\n                    ], 'annotation': \"\", 'content': content})\n        return model\n                        \n\nclass TestCodebaseExtractGithub:\n    def __init__(self):\n        self.test_github_repo = \"https://github.com/TravHaran/codesense\"      \n        self.extractor = CodeBaseExtractGithub(self.test_github_repo)\n        print(\"Testing Github Codebase Extractor...\\n\")\n    \n    def test_extract_codebase(self):\n        print(f\"Testing codebase extraction of {self.test_github_repo}\\n\")\n        output = self.extractor.get_model()\n        obj_to_json(\"./\", \"test_github_codebase\", output)\n        assert type(output) == dict\n        \nif __name__ == \"__main__\":\n    testCodebaseExtractGithub = TestCodebaseExtractGithub()\n    testCodebaseExtractGithub.test_extract_codebase()     \n\n"
                },
                {
                    "name": "test_codebase.json",
                    "path": "codesense/codebase_extract/test_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "test_github_codebase.json",
                    "path": "codesense/codebase_extract/test_github_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"annotation_aggregate\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"annotation_aggregate.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import sys\\n\\nsys.path.insert(0, \\\"..\\\")\\nfrom utilities.utility import json_to_obj\\n\\n'''\\nCreate a class to aggregate the annotations of some target nodes\\n- input:\\n    - search_result object containing most relevant nodes with annotations \\n- output:\\n    - an aggregate of all the relevant annotations in string format\\n    - optionally save output as txt file\\n'''\\n\\nclass AnnotationAggregate:\\n    def __init__(self, traverse_obj):\\n        self.result_model = traverse_obj\\n        self.annotations = []\\n    \\n    def aggregate_annotations(self):\\n        for entry in self.result_model[\\\"results\\\"]:\\n            node = entry[\\\"node\\\"]\\n            self.annotations.append((node[\\\"name\\\"], node[\\\"annotation\\\"]))\\n        return self.format_output()\\n    \\n    def format_output(self):\\n        output = \\\"Relevant Files: \\\\n\\\\n\\\"\\n        count = 0\\n        for entry in self.annotations:\\n            count += 1\\n            name = entry[0]\\n            annotation = entry[1]\\n            output += str(f\\\"FILENAME: {name}\\\\nDESCRIPTION: \\\\\\\"{annotation}\\\\\\\"\\\\n\\\\n\\\")\\n        return output\\n\\n\\nclass TestAnnotationAggregate:\\n    def test_aggreagate_top_1_results(self):\\n        test_traverse_obj = json_to_obj(\\\"top_1.json\\\")\\n        aggregator = AnnotationAggregate(test_traverse_obj)\\n        print(\\\"\\\\nTesting Aggregation of top 1 results:\\\")\\n        result = aggregator.aggregate_annotations()\\n        print(result)\\n    def test_aggreagate_top_3_results(self):\\n        test_traverse_obj = json_to_obj(\\\"top_3.json\\\")\\n        aggregator = AnnotationAggregate(test_traverse_obj)\\n        print(\\\"\\\\nTesting Aggregation of top 3 results:\\\")\\n        print(aggregator.aggregate_annotations())\\n\\nif __name__ == \\\"__main__\\\":\\n    testAnnotationAggregate = TestAnnotationAggregate()\\n    testAnnotationAggregate.test_aggreagate_top_1_results()\\n    testAnnotationAggregate.test_aggreagate_top_3_results()\"\n                },\n                {\n                    \"name\": \"top_1.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"input_keywords\\\": [\\n        \\\"python\\\",\\n        \\\"function\\\",\\n        \\\"testkeywordextract\\\",\\n        \\\"nltk\\\",\\n        \\\"word2vec\\\",\\n        \\\"extract_keywords\\\"\\n    ],\\n    \\\"results\\\": [\\n        {\\n            \\\"score\\\": 0.8333333333333334,\\n            \\\"matched_keywords\\\": [\\n                \\\"python\\\",\\n                \\\"function\\\",\\n                \\\"extract_keywords\\\",\\n                \\\"word2vec\\\",\\n                \\\"nltk\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"main.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"*\\\",\\n                    \\\"Comparison\\\",\\n                    \\\"Embeddings\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Processing\\\",\\n                    \\\"Python\\\",\\n                    \\\"Similarity\\\",\\n                    \\\"Text\\\",\\n                    \\\"Word\\\",\\n                    \\\"Word2Vec\\\",\\n                    \\\"calculates\\\",\\n                    \\\"code\\\",\\n                    \\\"compare_keywords\\\",\\n                    \\\"compare_words\\\",\\n                    \\\"comparing\\\",\\n                    \\\"computes\\\",\\n                    \\\"console\\\",\\n                    \\\"context\\\",\\n                    \\\"create\\\",\\n                    \\\"embeddings\\\",\\n                    \\\"employs\\\",\\n                    \\\"extract_keywords\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"file\\\",\\n                    \\\"focuses\\\",\\n                    \\\"function\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"keywords.The\\\",\\n                    \\\"language\\\",\\n                    \\\"libraries\\\",\\n                    \\\"library\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"model\\\",\\n                    \\\"modeling.1\\\",\\n                    \\\"output\\\",\\n                    \\\"performs\\\",\\n                    \\\"processes\\\",\\n                    \\\"processing\\\",\\n                    \\\"reads\\\",\\n                    \\\"removes\\\",\\n                    \\\"returned\\\",\\n                    \\\"score\\\",\\n                    \\\"sentences\\\",\\n                    \\\"similarity\\\",\\n                    \\\"tagging\\\",\\n                    \\\"techniques\\\",\\n                    \\\"text\\\",\\n                    \\\"texts\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"uses\\\",\\n                    \\\"using\\\",\\n                    \\\"vector\\\",\\n                    \\\"verbs\\\",\\n                    \\\"word\\\",\\n                    \\\"words\\\",\\n                    \\\"words.3\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n            }\\n        }\\n    ]\\n}\"\n                },\n                {\n                    \"name\": \"top_3.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"input_keywords\\\": [\\n        \\\"python\\\",\\n        \\\"function\\\",\\n        \\\"testkeywordextract\\\",\\n        \\\"nltk\\\",\\n        \\\"word2vec\\\",\\n        \\\"extract_keywords\\\"\\n    ],\\n    \\\"results\\\": [\\n        {\\n            \\\"score\\\": 0.8333333333333334,\\n            \\\"matched_keywords\\\": [\\n                \\\"python\\\",\\n                \\\"function\\\",\\n                \\\"extract_keywords\\\",\\n                \\\"word2vec\\\",\\n                \\\"nltk\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"main.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"*\\\",\\n                    \\\"Comparison\\\",\\n                    \\\"Embeddings\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Processing\\\",\\n                    \\\"Python\\\",\\n                    \\\"Similarity\\\",\\n                    \\\"Text\\\",\\n                    \\\"Word\\\",\\n                    \\\"Word2Vec\\\",\\n                    \\\"calculates\\\",\\n                    \\\"code\\\",\\n                    \\\"compare_keywords\\\",\\n                    \\\"compare_words\\\",\\n                    \\\"comparing\\\",\\n                    \\\"computes\\\",\\n                    \\\"console\\\",\\n                    \\\"context\\\",\\n                    \\\"create\\\",\\n                    \\\"embeddings\\\",\\n                    \\\"employs\\\",\\n                    \\\"extract_keywords\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"file\\\",\\n                    \\\"focuses\\\",\\n                    \\\"function\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"keywords.The\\\",\\n                    \\\"language\\\",\\n                    \\\"libraries\\\",\\n                    \\\"library\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"model\\\",\\n                    \\\"modeling.1\\\",\\n                    \\\"output\\\",\\n                    \\\"performs\\\",\\n                    \\\"processes\\\",\\n                    \\\"processing\\\",\\n                    \\\"reads\\\",\\n                    \\\"removes\\\",\\n                    \\\"returned\\\",\\n                    \\\"score\\\",\\n                    \\\"sentences\\\",\\n                    \\\"similarity\\\",\\n                    \\\"tagging\\\",\\n                    \\\"techniques\\\",\\n                    \\\"text\\\",\\n                    \\\"texts\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"uses\\\",\\n                    \\\"using\\\",\\n                    \\\"vector\\\",\\n                    \\\"verbs\\\",\\n                    \\\"word\\\",\\n                    \\\"words\\\",\\n                    \\\"words.3\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.5,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"testkeywordextract\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"keyword_extract.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"English\\\",\\n                    \\\"Language\\\",\\n                    \\\"Natural\\\",\\n                    \\\"Python\\\",\\n                    \\\"TestKeywordExtract\\\",\\n                    \\\"Toolkit\\\",\\n                    \\\"annotated\\\",\\n                    \\\"based\\\",\\n                    \\\"class\\\",\\n                    \\\"code\\\",\\n                    \\\"contains\\\",\\n                    \\\"create\\\",\\n                    \\\"description\\\",\\n                    \\\"ensuring\\\",\\n                    \\\"expected\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"filtering\\\",\\n                    \\\"filters\\\",\\n                    \\\"identifies\\\",\\n                    \\\"includes\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"method\\\",\\n                    \\\"nltk\\\",\\n                    \\\"nouns\\\",\\n                    \\\"output\\\",\\n                    \\\"pieces\\\",\\n                    \\\"processing\\\",\\n                    \\\"provided\\\",\\n                    \\\"query\\\",\\n                    \\\"running\\\",\\n                    \\\"script\\\",\\n                    \\\"selecting\\\",\\n                    \\\"stopwords\\\",\\n                    \\\"tagging\\\",\\n                    \\\"test\\\",\\n                    \\\"text\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.3333333333333333,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"info.txt\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"Automatic\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Python\\\",\\n                    \\\"RAKE\\\",\\n                    \\\"Rapid\\\",\\n                    \\\"SSL\\\",\\n                    \\\"algorithm\\\",\\n                    \\\"certificate\\\",\\n                    \\\"changing\\\",\\n                    \\\"command\\\",\\n                    \\\"commands\\\",\\n                    \\\"consists\\\",\\n                    \\\"downloading\\\",\\n                    \\\"downloads\\\",\\n                    \\\"environment\\\",\\n                    \\\"error\\\",\\n                    \\\"gensim\\\",\\n                    \\\"install\\\",\\n                    \\\"installing\\\",\\n                    \\\"instructs\\\",\\n                    \\\"involves\\\",\\n                    \\\"issue\\\",\\n                    \\\"language\\\",\\n                    \\\"model\\\",\\n                    \\\"occurs\\\",\\n                    \\\"packages\\\",\\n                    \\\"processing\\\",\\n                    \\\"provided\\\",\\n                    \\\"setting\\\",\\n                    \\\"shell\\\",\\n                    \\\"suggests\\\",\\n                    \\\"text\\\",\\n                    \\\"tokenization\\\",\\n                    \\\"use\\\",\\n                    \\\"version\\\",\\n                    \\\"words\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\\\",\\n                \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n            }\\n        }\\n    ]\\n}\"\n                }\n            ]\n        },\n        {\n            \"name\": \"annotation_generate\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"annotation_generate.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"from openai import OpenAI\\nimport os\\nfrom dotenv import load_dotenv\\n\\n'''\\nCreate a class to annotate a piece of given code\\n- input:\\n    - Code (Function, Class, etc.)\\n- output:\\n    - Summary of code in text\\n'''\\n\\nclass AnnotationGeneration:\\n    def __init__(self):\\n        self.res = \\\"\\\"\\n\\n    def snippet_summary(self, snippet):\\n            ## Set the API Key\\n            load_dotenv()\\n            API_KEY = os.getenv('OPENAI_SECRET_API_KEY')\\n            client = OpenAI(api_key=API_KEY)\\n\\n            #GPT4o REPONSE REQUEST\\n            MODEL=\\\"gpt-4o\\\"\\n\\n            completion = client.chat.completions.create(\\n            model=MODEL,\\n            #Prompt modelling, grounding the model to provide a more concise and clear summary when given a piece of code\\n            messages=[\\n                {\\\"role\\\": \\\"system\\\", \\\"content\\\": '''You are an AI designed to explain code clearly and concisely. When given a piece of code, your task is to provide a quick summary without giving a detailed breakdown. Your summary should include the programming language, the purpose of the code, a brief explanation of its key components functionality and logic, and the expected output. Respond in a single blurb of text.\\n\\n            Here is the piece of code for you to explain:\\n\\n            python\\n            Copy code\\n            def is_prime(n):\\n                if n <= 1:\\n                    return False\\n                for i in range(2, int(n**0.5) + 1):\\n                    if n % i == 0:\\n                        return False\\n                return True\\n\\n            number = 7\\n            result = is_prime(number)\\n            print(f\\\"Is {number} a prime number? {result}\\\")\\n            Expected Explanation:\\n\\n            The code is written in Python and checks if a given number is a prime number. The function is_prime(n) returns True if n is prime by testing divisibility from 2 to the square root of n, otherwise it returns False. The variable number is set to 7, and the function is called to check if 7 is prime, with the result printed. The output will be: \\\"Is 7 a prime number? True\\\".'''},\\n                \\n                {\\\"role\\\": \\\"user\\\", \\\"content\\\": f'''With that said. Explain the given code:\\n                    {snippet}\\n                '''}\\n            ]\\n            )\\n            return completion.choices[0].message.content\\n    \\n\\n### TESTING \\nclass TestSnippetSummary:\\n     def __init__(self):\\n        self.summarizer = AnnotationGeneration()\\n        print(\\\"Testing Snippet Summarizer... \\\\n\\\")\\n    \\n        #High-depth code is code that has many variables, refers to many functions, changes variables, and is overall complex to understand from a quick view\\n     def test_snippet_summarizer_from_high_depth_code(self):\\n          print(\\\"Testing code snippet summarizer with a high depth function... \\\\n\\\")\\n          code_snippet = '''\\n          const handleCreateEvent = async(e) => {\\n            e.preventDefault()\\n            // console.log(user)\\n            // const dateNow = new Date()\\n            // var date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\\\"\\\\\\\"\\\", \\\"\\\").replace(\\\"\\\\\\\"\\\", \\\"\\\");\\n                if (isDateRange == false){\\n                    await addDoc(collection(db, 'schools', completeSchoolName, 'events'), {\\n                      title: title,\\n                      description: description,\\n                      dateTime: dateTime,\\n                      location: location,\\n                      club: club,\\n                      dateAdded: Date().toLocaleString(),\\n                      // createdBy: {name: user.displayName, email: user.email}\\n                  }).then(setTitle(\\\"\\\")).then(setDescription(\\\"\\\")).then(setDateTime(\\\"\\\")).then(setStartDateTime(\\\"\\\")).then(setEndDateTime(\\\"\\\")).then(setLocation(\\\"\\\")).then(setClub(\\\"\\\")).then(fetchUser())\\n                  \\n                  const chosenDate = new Date(dateTime)\\n                  var dateNow = new Date(chosenDate)\\n                  dateNow.setDate(chosenDate.getDate() + 1)\\n                  const date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\\\"\\\\\\\"\\\", \\\"\\\").replace(\\\"\\\\\\\"\\\", \\\"\\\")\\n                  await setDoc(doc(db, 'schools', completeSchoolName, 'announcements', date), {\\n                    notes: [{\\n                      title: title,\\n                      description: description,\\n                    club: club}],  \\n                    createdBy: {name: user.name, email: user.email},\\n                      dateAdded: Date().toLocaleString(),\\n                      // createdBy: {name: user.displayName, email: user.email}\\n                  }).then(setTitle(\\\"\\\")).then(setDescription(\\\"\\\")).then(setDateTime(\\\"\\\")).then(setLocation(\\\"\\\")).then(setClub(\\\"\\\")).then(fetchUser())\\n\\n                } else {\\n                  await addDoc(collection(db, 'schools', completeSchoolName, 'events'), {\\n                    title: title,\\n                    description: description,\\n                    startDate: startDateTime,\\n                    endDate: endDateTime,\\n                    location: location,\\n                    club: club,\\n                    dateAdded: Date().toLocaleString(),\\n                    // createdBy: {name: user.displayName, email: user.email}\\n                }).then(setTitle(\\\"\\\")).then(setDescription(\\\"\\\")).then(setDateTime(\\\"\\\")).then(setStartDateTime(\\\"\\\")).then(setEndDateTime(\\\"\\\")).then(setLocation(\\\"\\\")).then(setClub(\\\"\\\")).then(fetchUser())\\n                \\n                const chosenDate = new Date(startDateTime)\\n                var dateNow = new Date(chosenDate)\\n                dateNow.setDate(chosenDate.getDate() + 1)\\n                const date = JSON.stringify(dateNow.getFullYear()+'.'+(dateNow.getMonth()+1)+'.'+dateNow.getDate()).replace(\\\"\\\\\\\"\\\", \\\"\\\").replace(\\\"\\\\\\\"\\\", \\\"\\\")\\n                await setDoc(doc(db, 'schools', completeSchoolName, 'announcements', date), {\\n                  notes: [{\\n                    title: title,\\n                    description: description,\\n                  club: club}],  \\n                  createdBy: {name: user.name, email: user.email},\\n                    dateAdded: Date().toLocaleString(),\\n                    // createdBy: {name: user.displayName, email: user.email}\\n                }).then(setTitle(\\\"\\\")).then(setDescription(\\\"\\\")).then(setDateTime(\\\"\\\")).then(setLocation(\\\"\\\")).then(setClub(\\\"\\\")).then(fetchUser())\\n\\n                }\\n                \\n      }'''\\n          output = self.summarizer.snippet_summary(code_snippet)\\n          print(f\\\"CODE SUMMARY: \\\\n{output} \\\\n\\\\n\\\")\\n          assert type(output) == str\\n\\n          '''\\n          EXPECTED OUTPUT:The code is written in JavaScript, specifically using the async/await syntax to handle asynchronous operations with Firestore, a cloud database from Firebase. It defines an event-handling function `handleCreateEvent` meant to create and save event data into the Firestore database. When a form submission event triggers the function, it first prevents the default behavior with `e.preventDefault()`. \\n          The function checks if `isDateRange` is false and, based on this, either adds or updates single or range-dated event documents in the Firestore under the 'events' collection. It also conditionally updates the 'announcements' collection based on the existence of date ranges. After database operations, it resets multiple state variables (title, description, dateTime, etc.) and fetches user data. \\n          The function ensures newly created or modified data incorporates the current date and time and user metadata. The expected result includes adding appropriate entries in the Firestore under both 'events' and 'announcements' collections and resetting the form's state. \\n          '''\\n\\n\\n     #Low-depth code is code that is fairly simple, not many variables, and easy to follow \\n     def test_snippet_summarizer_from_low_depth_code(self):\\n          print(\\\"Testing code snippet summarizer with a low depth function... \\\\n\\\")\\n          code_snippet = '''\\n          class Solution:\\n            def topK(self, nums: List[int], k: int) -> List[int]:\\n                count = {}\\n                freq = [[] for i in range (len(nums) + 1)]\\n       \\n            for n in nums:\\n                count[n] = 1 + count.get(n, 0)\\n            for n, c in count.items():\\n                freq[c].append(n)\\n       \\n            res = []\\n       \\n            for i in range(len(freq) - 1, 0, -1):\\n                for n in freq[i]:\\n                    res.append(n)\\n                    if len(res) == k:\\n                        return res'''\\n          output = self.summarizer.snippet_summary(code_snippet)\\n          print(f\\\"CODE SUMMARY: \\\\n{output} \\\\n\\\\n\\\")\\n          assert type(output) == str\\n          '''\\n          EXPECTED OUTPUT:\\n          The code is written in Python and defines a method `topK` within a class `Solution`. The purpose of this method is to return the top k most frequent integers from a given list `nums`. \\n          It first creates a `count` dictionary to tally the frequency of each number in `nums`, then organizes these frequencies into a list of lists `freq`. \\n          The method then iterates through `freq` in reverse order to gather the k most frequent numbers into the result list `res`, which is returned once it reaches the desired length k. \\n          '''\\n    \\n\\nif __name__ == \\\"__main__\\\":\\n    TestSnippetSummary = TestSnippetSummary()\\n    TestSnippetSummary.test_snippet_summarizer_from_low_depth_code()\\n    TestSnippetSummary.test_snippet_summarizer_from_high_depth_code()\\n\"\n                },\n                {\n                    \"name\": \"info.txt\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"pip install:\\n    python-dotenv\\n    openai\\n\\nmake sure you have the .env file containing the openai secret key in this directory\"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import os\\nimport sys\\nsys.path.insert(0, \\\"..\\\")\\n\\nfrom utilities.utility import obj_to_json, file_to_string\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - object containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n    \\n    def get_model(self):\\n        self.model = self._build_model(self.path)\\n        return self.model\\n\\n    def _build_model(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'): # ignore hidden folders & files\\n                # Create the file path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self._build_model(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = file_to_string(entry_path)\\n                    except Exception: # handle unreadable file content\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"../../codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.get_model()\\n        obj_to_json(\\\"./\\\", \\\"test_codebase\\\", output)\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                },\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"keywords\\\": [],\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"test_codebase.json\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\n'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self, path):\\\\n        # Initialize the output dictionary model with folder contents\\\\n        # name, type, keywords, and empty list for children\\\\n        self.path = path\\\\n        self.model = {}\\\\n\\\\n    def file_to_string(self, file_path):  # save file content as string\\\\n        with open(file_path, 'r') as file:\\\\n            file_content = file.read()\\\\n        file.close()\\\\n        return file_content\\\\n\\\\n    def extract(self, path):  # extracts a directory as a json object\\\\n        model = {'name': os.path.basename(path),\\\\n                 'type': 'folder', 'keywords': [], 'children': []}\\\\n        # Check if the path is a directory\\\\n        if not os.path.isdir(path):\\\\n            return model\\\\n\\\\n        # Iterate over the entries in the directory\\\\n        for entry in os.listdir(path):\\\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\\\n                # Create the fill path for current entry\\\\n                entry_path = os.path.join(path, entry)\\\\n                # if the entry is a directory, recursively call the function\\\\n                if os.path.isdir(entry_path):\\\\n                    model['children'].append(self.extract(entry_path))\\\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\\\n                else:\\\\n                    content = \\\\\\\"\\\\\\\"\\\\n                    # save file content as string\\\\n                    try:\\\\n                        content = self.file_to_string(entry_path)\\\\n                    except OSError:\\\\n                        content = \\\\\\\"n/a\\\\\\\"\\\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\\\n                    ], 'annotation': \\\\\\\"\\\\\\\", 'content': content})\\\\n        return model\\\\n\\\\n    def model_to_str(self):  # convert codebase json to string\\\\n        output_str = json.dumps(self.model, indent=4)\\\\n        return output_str\\\\n\\\\n    def save_model_json(self, file_name):  # codebase model json file\\\\n        save_file = open(f\\\\\\\"{file_name}.json\\\\\\\", 'w')\\\\n        self.model = self.extract(self.path)\\\\n        json.dump(self.model, save_file, indent=4)\\\\n        save_file.close()\\\\n        print(f\\\\\\\"Codebase model saved as {file_name}\\\\\\\")\\\\n        return self.model\\\\n\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.test_path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        self.extractor = CodebaseExtract(self.test_path)\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        output = self.extractor.save_model_json(\\\\\\\"test_codebase\\\\\\\")\\\\n        # model_str = self.extractor.model_to_str()\\\\n        # print(f\\\\\\\"Codebase model: {model_str}\\\\\\\")\\\\n        assert type(output) == dict\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testCodebaseExtract = TestCodebaseExtract()\\\\n    testCodebaseExtract.test_extract_codebase()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [],\\n            \\\"annotation\\\": \\\"\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"tree_traverse\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": []\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                }\n            ]\n        },\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"populate_annotations\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"ignore.txt\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"codesense/keyword_extract\\ncodesense/extras/codebase_extraction/codebase.json\\ncodesense/README.md\\n\"\n                },\n                {\n                    \"name\": \"populate_annotations.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import os\\nimport sys\\n\\nsys.path.insert(0, \\\"..\\\")\\nfrom annotation_generate.annotation_generate import AnnotationGeneration\\nfrom utilities.utility import file_to_string, obj_to_json, json_to_obj\\n\\n'''\\nCreate a class to populate the codebase json with annotations\\n- input: \\n    - codebase model object\\n- output:\\n    - codebase model object with updated annotation fields\\n'''\\n\\nclass PopulateAnnotations:\\n    def __init__(self, model_obj, ignore_paths_file):\\n        #ignore_paths is a txt file containing file_paths to ignore\\n        self.annotator = AnnotationGeneration()\\n        self.model = model_obj\\n        self.ignore_list = self.build_ignore_list(ignore_paths_file)\\n    \\n    def build_ignore_list(self, txt_file):\\n        # read txt file as string\\n        ignore_list = file_to_string(txt_file).splitlines()\\n        return ignore_list\\n        \\n    \\n    def annotate(self, content_str):\\n        formated_str = content_str.replace(\\\"\\\\n\\\", \\\"\\\") # remove newline characters\\n        output = self.annotator.snippet_summary(formated_str) # comment this out to stub API call for testing purposes\\n        # output = \\\"test\\\"\\n        return output\\n    \\n    def populate_model(self):\\n        self._populate(self.model, self.model[\\\"name\\\"])\\n        return self.model\\n        \\n    def _populate(self, model, cur_path):\\n        if model[\\\"type\\\"] == \\\"file\\\":\\n            if model[\\\"content\\\"] not in [\\\"n/a\\\", \\\"\\\"]:\\n                annotation = self.annotate(model[\\\"content\\\"])\\n                model[\\\"annotation\\\"] = annotation \\n                return model  \\n        else:\\n            for child in model[\\\"children\\\"]:\\n                #build path string of traversal\\n                new_path = os.path.join(cur_path, child[\\\"name\\\"])\\n                if new_path not in self.ignore_list:\\n                    self._populate(child, new_path)\\n\\nclass TestPopulateAnnotations:\\n    def __init__(self):\\n        self.test_model = json_to_obj(\\\"test_codebase_original.json\\\") \\n        self.test_ignore_file = \\\"ignore.txt\\\"\\n        self.populator = PopulateAnnotations(self.test_model, self.test_ignore_file)\\n        \\n    def test_populate_annotations(self):\\n        print(\\\"Testing annotation population\\\")\\n        updated_model = self.populator.populate_model()\\n        obj_to_json(\\\"./\\\", \\\"test\\\", updated_model)\\n        assert type(updated_model) == dict\\n    \\n        \\n\\nif __name__ == \\\"__main__\\\":\\n    testPopulateAnnotations = TestPopulateAnnotations()\\n    testPopulateAnnotations.test_populate_annotations()\\n\\n                \\n        \\n\"\n                },\n                {\n                    \"name\": \"test.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"keywords\\\": [],\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"test\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"test_codebase.json\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"test\\\",\\n                    \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\n'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self, path):\\\\n        # Initialize the output dictionary model with folder contents\\\\n        # name, type, keywords, and empty list for children\\\\n        self.path = path\\\\n        self.model = {}\\\\n\\\\n    def file_to_string(self, file_path):  # save file content as string\\\\n        with open(file_path, 'r') as file:\\\\n            file_content = file.read()\\\\n        file.close()\\\\n        return file_content\\\\n\\\\n    def extract(self, path):  # extracts a directory as a json object\\\\n        model = {'name': os.path.basename(path),\\\\n                 'type': 'folder', 'keywords': [], 'children': []}\\\\n        # Check if the path is a directory\\\\n        if not os.path.isdir(path):\\\\n            return model\\\\n\\\\n        # Iterate over the entries in the directory\\\\n        for entry in os.listdir(path):\\\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\\\n                # Create the fill path for current entry\\\\n                entry_path = os.path.join(path, entry)\\\\n                # if the entry is a directory, recursively call the function\\\\n                if os.path.isdir(entry_path):\\\\n                    model['children'].append(self.extract(entry_path))\\\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\\\n                else:\\\\n                    content = \\\\\\\"\\\\\\\"\\\\n                    # save file content as string\\\\n                    try:\\\\n                        content = self.file_to_string(entry_path)\\\\n                    except OSError:\\\\n                        content = \\\\\\\"n/a\\\\\\\"\\\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\\\n                    ], 'annotation': \\\\\\\"\\\\\\\", 'content': content})\\\\n        return model\\\\n\\\\n    def model_to_str(self):  # convert codebase json to string\\\\n        output_str = json.dumps(self.model, indent=4)\\\\n        return output_str\\\\n\\\\n    def save_model_json(self, file_name):  # codebase model json file\\\\n        save_file = open(f\\\\\\\"{file_name}.json\\\\\\\", 'w')\\\\n        self.model = self.extract(self.path)\\\\n        json.dump(self.model, save_file, indent=4)\\\\n        save_file.close()\\\\n        print(f\\\\\\\"Codebase model saved as {file_name}\\\\\\\")\\\\n        return self.model\\\\n\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.test_path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        self.extractor = CodebaseExtract(self.test_path)\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        output = self.extractor.save_model_json(\\\\\\\"test_codebase\\\\\\\")\\\\n        # model_str = self.extractor.model_to_str()\\\\n        # print(f\\\\\\\"Codebase model: {model_str}\\\\\\\")\\\\n        assert type(output) == dict\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testCodebaseExtract = TestCodebaseExtract()\\\\n    testCodebaseExtract.test_extract_codebase()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [],\\n            \\\"annotation\\\": \\\"\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"tree_traverse\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": []\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"test\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"test\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"test\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"test\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"test\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"test\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                },\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"keywords\\\": [],\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"test_codebase.json\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\\\\\"test_codebase.json\\\\\\\" representing the directory structure of `self.test_path`.\\\",\\n                    \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\n'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self, path):\\\\n        # Initialize the output dictionary model with folder contents\\\\n        # name, type, keywords, and empty list for children\\\\n        self.path = path\\\\n        self.model = {}\\\\n\\\\n    def file_to_string(self, file_path):  # save file content as string\\\\n        with open(file_path, 'r') as file:\\\\n            file_content = file.read()\\\\n        file.close()\\\\n        return file_content\\\\n\\\\n    def extract(self, path):  # extracts a directory as a json object\\\\n        model = {'name': os.path.basename(path),\\\\n                 'type': 'folder', 'keywords': [], 'children': []}\\\\n        # Check if the path is a directory\\\\n        if not os.path.isdir(path):\\\\n            return model\\\\n\\\\n        # Iterate over the entries in the directory\\\\n        for entry in os.listdir(path):\\\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\\\n                # Create the fill path for current entry\\\\n                entry_path = os.path.join(path, entry)\\\\n                # if the entry is a directory, recursively call the function\\\\n                if os.path.isdir(entry_path):\\\\n                    model['children'].append(self.extract(entry_path))\\\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\\\n                else:\\\\n                    content = \\\\\\\"\\\\\\\"\\\\n                    # save file content as string\\\\n                    try:\\\\n                        content = self.file_to_string(entry_path)\\\\n                    except OSError:\\\\n                        content = \\\\\\\"n/a\\\\\\\"\\\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\\\n                    ], 'annotation': \\\\\\\"\\\\\\\", 'content': content})\\\\n        return model\\\\n\\\\n    def model_to_str(self):  # convert codebase json to string\\\\n        output_str = json.dumps(self.model, indent=4)\\\\n        return output_str\\\\n\\\\n    def save_model_json(self, file_name):  # codebase model json file\\\\n        save_file = open(f\\\\\\\"{file_name}.json\\\\\\\", 'w')\\\\n        self.model = self.extract(self.path)\\\\n        json.dump(self.model, save_file, indent=4)\\\\n        save_file.close()\\\\n        print(f\\\\\\\"Codebase model saved as {file_name}\\\\\\\")\\\\n        return self.model\\\\n\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.test_path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        self.extractor = CodebaseExtract(self.test_path)\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        output = self.extractor.save_model_json(\\\\\\\"test_codebase\\\\\\\")\\\\n        # model_str = self.extractor.model_to_str()\\\\n        # print(f\\\\\\\"Codebase model: {model_str}\\\\\\\")\\\\n        assert type(output) == dict\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testCodebaseExtract = TestCodebaseExtract()\\\\n    testCodebaseExtract.test_extract_codebase()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [],\\n            \\\"annotation\\\": \\\"This document outlines a project called \\\\\\\"Codesense,\\\\\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\\\n\\\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\\\n3. Annotation Generation: Produces text summaries for functions in the code.\\\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\\\n\\\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"tree_traverse\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": []\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"This code is structured as a JSON object representing a project directory called \\\\\\\"codesense,\\\\\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\\\\\"codebase.json\\\\\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\\\\\"Once upon a time\\\\\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                },\n                {\n                    \"name\": \"test_codebase_original.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"keywords\\\": [],\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"test_codebase.json\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\n'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self, path):\\\\n        # Initialize the output dictionary model with folder contents\\\\n        # name, type, keywords, and empty list for children\\\\n        self.path = path\\\\n        self.model = {}\\\\n\\\\n    def file_to_string(self, file_path):  # save file content as string\\\\n        with open(file_path, 'r') as file:\\\\n            file_content = file.read()\\\\n        file.close()\\\\n        return file_content\\\\n\\\\n    def extract(self, path):  # extracts a directory as a json object\\\\n        model = {'name': os.path.basename(path),\\\\n                 'type': 'folder', 'keywords': [], 'children': []}\\\\n        # Check if the path is a directory\\\\n        if not os.path.isdir(path):\\\\n            return model\\\\n\\\\n        # Iterate over the entries in the directory\\\\n        for entry in os.listdir(path):\\\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\\\n                # Create the fill path for current entry\\\\n                entry_path = os.path.join(path, entry)\\\\n                # if the entry is a directory, recursively call the function\\\\n                if os.path.isdir(entry_path):\\\\n                    model['children'].append(self.extract(entry_path))\\\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\\\n                else:\\\\n                    content = \\\\\\\"\\\\\\\"\\\\n                    # save file content as string\\\\n                    try:\\\\n                        content = self.file_to_string(entry_path)\\\\n                    except OSError:\\\\n                        content = \\\\\\\"n/a\\\\\\\"\\\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\\\n                    ], 'annotation': \\\\\\\"\\\\\\\", 'content': content})\\\\n        return model\\\\n\\\\n    def model_to_str(self):  # convert codebase json to string\\\\n        output_str = json.dumps(self.model, indent=4)\\\\n        return output_str\\\\n\\\\n    def save_model_json(self, file_name):  # codebase model json file\\\\n        save_file = open(f\\\\\\\"{file_name}.json\\\\\\\", 'w')\\\\n        self.model = self.extract(self.path)\\\\n        json.dump(self.model, save_file, indent=4)\\\\n        save_file.close()\\\\n        print(f\\\\\\\"Codebase model saved as {file_name}\\\\\\\")\\\\n        return self.model\\\\n\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.test_path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        self.extractor = CodebaseExtract(self.test_path)\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        output = self.extractor.save_model_json(\\\\\\\"test_codebase\\\\\\\")\\\\n        # model_str = self.extractor.model_to_str()\\\\n        # print(f\\\\\\\"Codebase model: {model_str}\\\\\\\")\\\\n        assert type(output) == dict\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testCodebaseExtract = TestCodebaseExtract()\\\\n    testCodebaseExtract.test_extract_codebase()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [],\\n            \\\"annotation\\\": \\\"\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"tree_traverse\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": []\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                }\n            ]\n        },\n        {\n            \"name\": \"populate_keywords\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"populate_keywords.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import sys\\n\\nsys.path.insert(0, \\\"..\\\")\\nfrom keyword_extract.keyword_extract import KeywordExtract\\nfrom utilities.utility import obj_to_json, json_to_obj\\n\\n'''\\nCreate a class to populate the codebase json with keywords\\n- input: \\n    - codebase model object\\n- output:\\n    - codebase model object with updated keywords fields\\n'''\\n\\nclass PopulateKeywords:\\n    def __init__(self, model_obj):\\n        self.model = model_obj\\n    \\n    def extractKeywords(self, content_str):\\n        formated_str = content_str.replace(\\\"\\\\n\\\", \\\"\\\") # remove newline characters\\n        extractor = KeywordExtract()\\n        output = extractor.extract(formated_str)\\n        # output = \\\"test\\\"\\n        return output\\n    \\n    def populate_model(self):\\n        self._populate(self.model)\\n        return self.model\\n        \\n    def _populate(self, model):\\n        if model[\\\"type\\\"] == \\\"file\\\":\\n            annotation = model[\\\"annotation\\\"]\\n            keywords = self.extractKeywords(annotation)\\n            model['keywords'] = keywords\\n            return model  \\n        else:\\n            for child in model[\\\"children\\\"]:\\n                self._populate(child)\\n    \\n    \\nclass TestPopulateKeyWords:\\n    def __init__(self):\\n        self.test_model = json_to_obj(\\\"test_codebase_original.json\\\")\\n        self.populator = PopulateKeywords(self.test_model)\\n\\n    def test_populate_keywords(self):\\n        print(\\\"Testing annotation population\\\")\\n        updated_model = self.populator.populate_model()\\n        obj_to_json(\\\"./\\\", \\\"test\\\", updated_model)\\n        assert type(updated_model) == dict\\n\\nif __name__ == \\\"__main__\\\":\\n    testPopulateKeyWords = TestPopulateKeyWords()\\n    testPopulateKeyWords.test_populate_keywords()\\n    \"\n                },\n                {\n                    \"name\": \"test.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"keywords\\\": [],\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [\\n                        \\\"annotated\\\",\\n                        \\\"pieces\\\",\\n                        \\\"method\\\",\\n                        \\\"Python\\\",\\n                        \\\"keyword\\\",\\n                        \\\"query\\\",\\n                        \\\"stopwords\\\",\\n                        \\\"ensuring\\\",\\n                        \\\"processing\\\",\\n                        \\\"extracted\\\",\\n                        \\\"English\\\",\\n                        \\\"list\\\",\\n                        \\\"tokenizes\\\",\\n                        \\\"output\\\",\\n                        \\\"keywords\\\",\\n                        \\\"class\\\",\\n                        \\\"test\\\",\\n                        \\\"input\\\",\\n                        \\\"text\\\",\\n                        \\\"extraction\\\",\\n                        \\\"selecting\\\",\\n                        \\\"running\\\",\\n                        \\\"Natural\\\",\\n                        \\\"filters\\\",\\n                        \\\"provided\\\",\\n                        \\\"lists\\\",\\n                        \\\"TestKeywordExtract\\\",\\n                        \\\"expected\\\",\\n                        \\\"description\\\",\\n                        \\\"nouns\\\",\\n                        \\\"filtering\\\",\\n                        \\\"Language\\\",\\n                        \\\"includes\\\",\\n                        \\\"written\\\",\\n                        \\\"tagging\\\",\\n                        \\\"identifies\\\",\\n                        \\\"based\\\",\\n                        \\\"contains\\\",\\n                        \\\"nltk\\\",\\n                        \\\"script\\\",\\n                        \\\"Toolkit\\\",\\n                        \\\"extracts\\\",\\n                        \\\"code\\\",\\n                        \\\"create\\\"\\n                    ],\\n                    \\\"annotation\\\": \\\"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [\\n                        \\\"Python\\\",\\n                        \\\"damage\\\",\\n                        \\\"Character\\\",\\n                        \\\"named\\\",\\n                        \\\"initialized\\\",\\n                        \\\"health\\\",\\n                        \\\"character\\\",\\n                        \\\"updated\\\",\\n                        \\\"game\\\",\\n                        \\\"ninja\\\",\\n                        \\\"output\\\",\\n                        \\\"class\\\",\\n                        \\\"speeds\\\",\\n                        \\\"created\\\",\\n                        \\\"doubled\\\",\\n                        \\\"showcase\\\",\\n                        \\\"attributes\\\",\\n                        \\\"doubles\\\",\\n                        \\\"code\\\",\\n                        \\\"printed\\\",\\n                        \\\"written\\\",\\n                        \\\"speed\\\",\\n                        \\\"using\\\",\\n                        \\\"models\\\",\\n                        \\\"double_speed\\\",\\n                        \\\"parameters\\\",\\n                        \\\"instances\\\",\\n                        \\\"includes\\\",\\n                        \\\"warrior\\\"\\n                    ],\\n                    \\\"annotation\\\": \\\"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"test_codebase.json\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [\\n                        \\\"string\\\",\\n                        \\\"directories\\\",\\n                        \\\"converts\\\",\\n                        \\\"method\\\",\\n                        \\\"Expected\\\",\\n                        \\\"Python\\\",\\n                        \\\"named\\\",\\n                        \\\"defines\\\",\\n                        \\\"writes\\\",\\n                        \\\"tree\\\",\\n                        \\\"output\\\",\\n                        \\\"reads\\\",\\n                        \\\"class\\\",\\n                        \\\"test_codebase.json\\\",\\n                        \\\"model\\\",\\n                        \\\"stores\\\",\\n                        \\\"designed\\\",\\n                        \\\"treating\\\",\\n                        \\\"files\\\",\\n                        \\\"traversing\\\",\\n                        \\\"save_model_json\\\",\\n                        \\\"leaf\\\",\\n                        \\\"file_to_string\\\",\\n                        \\\"provided\\\",\\n                        \\\"structure\\\",\\n                        \\\"content\\\",\\n                        \\\"tests\\\",\\n                        \\\"generates\\\",\\n                        \\\"contents\\\",\\n                        \\\"directory\\\",\\n                        \\\"given\\\",\\n                        \\\"JSON\\\",\\n                        \\\"nodes\\\",\\n                        \\\"representing\\\",\\n                        \\\"self.test_path\\\",\\n                        \\\"model_to_str\\\",\\n                        \\\"file\\\",\\n                        \\\"functionality\\\",\\n                        \\\"code\\\",\\n                        \\\"create\\\"\\n                    ],\\n                    \\\"annotation\\\": \\\"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\\\\\"test_codebase.json\\\\\\\" representing the directory structure of `self.test_path`.\\\",\\n                    \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\n'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self, path):\\\\n        # Initialize the output dictionary model with folder contents\\\\n        # name, type, keywords, and empty list for children\\\\n        self.path = path\\\\n        self.model = {}\\\\n\\\\n    def file_to_string(self, file_path):  # save file content as string\\\\n        with open(file_path, 'r') as file:\\\\n            file_content = file.read()\\\\n        file.close()\\\\n        return file_content\\\\n\\\\n    def extract(self, path):  # extracts a directory as a json object\\\\n        model = {'name': os.path.basename(path),\\\\n                 'type': 'folder', 'keywords': [], 'children': []}\\\\n        # Check if the path is a directory\\\\n        if not os.path.isdir(path):\\\\n            return model\\\\n\\\\n        # Iterate over the entries in the directory\\\\n        for entry in os.listdir(path):\\\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\\\n                # Create the fill path for current entry\\\\n                entry_path = os.path.join(path, entry)\\\\n                # if the entry is a directory, recursively call the function\\\\n                if os.path.isdir(entry_path):\\\\n                    model['children'].append(self.extract(entry_path))\\\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\\\n                else:\\\\n                    content = \\\\\\\"\\\\\\\"\\\\n                    # save file content as string\\\\n                    try:\\\\n                        content = self.file_to_string(entry_path)\\\\n                    except OSError:\\\\n                        content = \\\\\\\"n/a\\\\\\\"\\\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\\\n                    ], 'annotation': \\\\\\\"\\\\\\\", 'content': content})\\\\n        return model\\\\n\\\\n    def model_to_str(self):  # convert codebase json to string\\\\n        output_str = json.dumps(self.model, indent=4)\\\\n        return output_str\\\\n\\\\n    def save_model_json(self, file_name):  # codebase model json file\\\\n        save_file = open(f\\\\\\\"{file_name}.json\\\\\\\", 'w')\\\\n        self.model = self.extract(self.path)\\\\n        json.dump(self.model, save_file, indent=4)\\\\n        save_file.close()\\\\n        print(f\\\\\\\"Codebase model saved as {file_name}\\\\\\\")\\\\n        return self.model\\\\n\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.test_path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        self.extractor = CodebaseExtract(self.test_path)\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        output = self.extractor.save_model_json(\\\\\\\"test_codebase\\\\\\\")\\\\n        # model_str = self.extractor.model_to_str()\\\\n        # print(f\\\\\\\"Codebase model: {model_str}\\\\\\\")\\\\n        assert type(output) == dict\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testCodebaseExtract = TestCodebaseExtract()\\\\n    testCodebaseExtract.test_extract_codebase()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [\\n                \\\"project\\\",\\n                \\\"involves\\\",\\n                \\\"showing\\\",\\n                \\\"Codesense\\\",\\n                \\\"document\\\",\\n                \\\"tree\\\",\\n                \\\"Extraction\\\",\\n                \\\"Graph\\\",\\n                \\\"answer\\\",\\n                \\\"keywords\\\",\\n                \\\"summaries\\\",\\n                \\\"Tree\\\",\\n                \\\"CodeBase\\\",\\n                \\\"Searches\\\",\\n                \\\"Produces\\\",\\n                \\\"call\\\",\\n                \\\"implementations\\\",\\n                \\\"report\\\",\\n                \\\"outlines\\\",\\n                \\\"Answering\\\",\\n                \\\"called\\\",\\n                \\\"codebase.2\\\",\\n                \\\"Keyword\\\",\\n                \\\"flows\\\",\\n                \\\"structure\\\",\\n                \\\"Traversal\\\",\\n                \\\"aggregated\\\",\\n                \\\"target\\\",\\n                \\\"Creates\\\",\\n                \\\"Identifies\\\",\\n                \\\"annotation.7\\\",\\n                \\\"serves\\\",\\n                \\\"objectives\\\",\\n                \\\"Generates\\\",\\n                \\\"directed\\\",\\n                \\\"Question\\\",\\n                \\\"providing\\\",\\n                \\\"codebase\\\",\\n                \\\"Compiles\\\",\\n                \\\"tasks\\\",\\n                \\\"components\\\",\\n                \\\"source\\\",\\n                \\\"based\\\",\\n                \\\"include\\\",\\n                \\\"queries.6\\\",\\n                \\\"nodes\\\",\\n                \\\"matching\\\",\\n                \\\"codebases\\\",\\n                \\\"file.3\\\",\\n                \\\"Annotation\\\",\\n                \\\"Generation\\\",\\n                \\\"representing\\\",\\n                \\\"annotations\\\",\\n                \\\"user\\\",\\n                \\\"returns\\\",\\n                \\\"related\\\",\\n                \\\"Uses\\\",\\n                \\\"Aggregation\\\",\\n                \\\"functions\\\",\\n                \\\"code.4\\\",\\n                \\\"analyze\\\",\\n                \\\"graph.5\\\",\\n                \\\"code\\\",\\n                \\\"Call\\\",\\n                \\\"function\\\"\\n            ],\\n            \\\"annotation\\\": \\\"This document outlines a project called \\\\\\\"Codesense,\\\\\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\\\n\\\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\\\n3. Annotation Generation: Produces text summaries for functions in the code.\\\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\\\n\\\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"tree_traverse\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": []\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"Python\\\",\\n                                \\\"involves\\\",\\n                                \\\"installing\\\",\\n                                \\\"packages\\\",\\n                                \\\"certificate\\\",\\n                                \\\"processing\\\",\\n                                \\\"install\\\",\\n                                \\\"language\\\",\\n                                \\\"Extraction\\\",\\n                                \\\"downloads\\\",\\n                                \\\"tokenization\\\",\\n                                \\\"SSL\\\",\\n                                \\\"shell\\\",\\n                                \\\"text\\\",\\n                                \\\"Automatic\\\",\\n                                \\\"instructs\\\",\\n                                \\\"model\\\",\\n                                \\\"use\\\",\\n                                \\\"Gensim\\\",\\n                                \\\"consists\\\",\\n                                \\\"issue\\\",\\n                                \\\"provided\\\",\\n                                \\\"Keyword\\\",\\n                                \\\"error\\\",\\n                                \\\"command\\\",\\n                                \\\"NLTK\\\",\\n                                \\\"algorithm\\\",\\n                                \\\"suggests\\\",\\n                                \\\"downloading\\\",\\n                                \\\"changing\\\",\\n                                \\\"commands\\\",\\n                                \\\"setting\\\",\\n                                \\\"environment\\\",\\n                                \\\"RAKE\\\",\\n                                \\\"Rapid\\\",\\n                                \\\"occurs\\\",\\n                                \\\"version\\\",\\n                                \\\"NLP\\\",\\n                                \\\"words\\\",\\n                                \\\"gensim\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"similarity\\\",\\n                                \\\"embeddings\\\",\\n                                \\\"Python\\\",\\n                                \\\"keyword\\\",\\n                                \\\"*\\\",\\n                                \\\"uses\\\",\\n                                \\\"Text\\\",\\n                                \\\"processes\\\",\\n                                \\\"texts\\\",\\n                                \\\"output\\\",\\n                                \\\"processing\\\",\\n                                \\\"extracted\\\",\\n                                \\\"language\\\",\\n                                \\\"Extraction\\\",\\n                                \\\"comparing\\\",\\n                                \\\"tokenizes\\\",\\n                                \\\"list\\\",\\n                                \\\"extraction\\\",\\n                                \\\"keywords\\\",\\n                                \\\"Word\\\",\\n                                \\\"input\\\",\\n                                \\\"text\\\",\\n                                \\\"reads\\\",\\n                                \\\"library\\\",\\n                                \\\"focuses\\\",\\n                                \\\"model\\\",\\n                                \\\"Gensim\\\",\\n                                \\\"Comparison\\\",\\n                                \\\"modeling.1\\\",\\n                                \\\"Embeddings\\\",\\n                                \\\"sentences\\\",\\n                                \\\"Keyword\\\",\\n                                \\\"lists\\\",\\n                                \\\"performs\\\",\\n                                \\\"verbs\\\",\\n                                \\\"compare_keywords\\\",\\n                                \\\"NLTK\\\",\\n                                \\\"console\\\",\\n                                \\\"Word2Vec\\\",\\n                                \\\"written\\\",\\n                                \\\"tagging\\\",\\n                                \\\"create\\\",\\n                                \\\"using\\\",\\n                                \\\"removes\\\",\\n                                \\\"calculates\\\",\\n                                \\\"score\\\",\\n                                \\\"libraries\\\",\\n                                \\\"word\\\",\\n                                \\\"Processing\\\",\\n                                \\\"compare_words\\\",\\n                                \\\"employs\\\",\\n                                \\\"vector\\\",\\n                                \\\"extract_keywords\\\",\\n                                \\\"returned\\\",\\n                                \\\"Similarity\\\",\\n                                \\\"words.3\\\",\\n                                \\\"context\\\",\\n                                \\\"NLP\\\",\\n                                \\\"computes\\\",\\n                                \\\"extracts\\\",\\n                                \\\"file\\\",\\n                                \\\"code\\\",\\n                                \\\"words\\\",\\n                                \\\"techniques\\\",\\n                                \\\"function\\\",\\n                                \\\"keywords.The\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"method\\\",\\n                                \\\"refers\\\",\\n                                \\\"list\\\",\\n                                \\\"class\\\",\\n                                \\\"price\\\",\\n                                \\\"profit\\\",\\n                                \\\"stock\\\",\\n                                \\\"increase\\\",\\n                                \\\"captures\\\",\\n                                \\\"iterates\\\",\\n                                \\\"end\\\",\\n                                \\\"sets\\\",\\n                                \\\"Solution\\\",\\n                                \\\"description\\\",\\n                                \\\"element\\\",\\n                                \\\"calculates\\\",\\n                                \\\"maxP\\\",\\n                                \\\"C++\\\",\\n                                \\\"returns\\\",\\n                                \\\"prices\\\",\\n                                \\\"representing\\\",\\n                                \\\"adds\\\",\\n                                \\\"maxProfit\\\",\\n                                \\\"difference\\\",\\n                                \\\"opportunities\\\",\\n                                \\\"function\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"project\\\",\\n                                \\\"Transformers\\\",\\n                                \\\"Python\\\",\\n                                \\\"keyword\\\",\\n                                \\\"generating\\\",\\n                                \\\"Face\\\",\\n                                \\\"Additionally\\\",\\n                                \\\"generation\\\",\\n                                \\\"character\\\",\\n                                \\\"game\\\",\\n                                \\\"scripts\\\",\\n                                \\\"extraction\\\",\\n                                \\\"summaries\\\",\\n                                \\\"annotation\\\",\\n                                \\\"text\\\",\\n                                \\\"structured\\\",\\n                                \\\"model\\\",\\n                                \\\"detailing\\\",\\n                                \\\"designed\\\",\\n                                \\\"working\\\",\\n                                \\\"structures\\\",\\n                                \\\"utilities\\\",\\n                                \\\"called\\\",\\n                                \\\"provided\\\",\\n                                \\\"structure\\\",\\n                                \\\"attributes\\\",\\n                                \\\"provide\\\",\\n                                \\\"expected\\\",\\n                                \\\"employing\\\",\\n                                \\\"lists\\\",\\n                                \\\"creating\\\",\\n                                \\\"NLTK\\\",\\n                                \\\"README\\\",\\n                                \\\"methods\\\",\\n                                \\\"codebase\\\",\\n                                \\\"representations\\\",\\n                                \\\"using\\\",\\n                                \\\"codesense\\\",\\n                                \\\"directory\\\",\\n                                \\\"components\\\",\\n                                \\\"tasks\\\",\\n                                \\\"defined\\\",\\n                                \\\"include\\\",\\n                                \\\"JSON\\\",\\n                                \\\"performing\\\",\\n                                \\\"object\\\",\\n                                \\\"codebases\\\",\\n                                \\\"setting\\\",\\n                                \\\"environment\\\",\\n                                \\\"representing\\\",\\n                                \\\"instructions\\\",\\n                                \\\"annotations\\\",\\n                                \\\"modeling\\\",\\n                                \\\"outputs\\\",\\n                                \\\"involve\\\",\\n                                \\\"Hugging\\\",\\n                                \\\"functionalities\\\",\\n                                \\\"code\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"This code is structured as a JSON object representing a project directory called \\\\\\\"codesense,\\\\\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"directories\\\",\\n                                \\\"converts\\\",\\n                                \\\"string\\\",\\n                                \\\"read\\\",\\n                                \\\"Python\\\",\\n                                \\\"codebase.json\\\",\\n                                \\\"navigates\\\",\\n                                \\\"folder\\\",\\n                                \\\"named\\\",\\n                                \\\"indentation\\\",\\n                                \\\"nested\\\",\\n                                \\\"types\\\",\\n                                \\\"create_folder_structure_json\\\",\\n                                \\\"list\\\",\\n                                \\\"ignored\\\",\\n                                \\\"output\\\",\\n                                \\\"reads\\\",\\n                                \\\"folders\\\",\\n                                \\\"specifies\\\",\\n                                \\\"prints\\\",\\n                                \\\"names\\\",\\n                                \\\"children\\\",\\n                                \\\"designed\\\",\\n                                \\\"saves\\\",\\n                                \\\"files\\\",\\n                                \\\"file_path\\\",\\n                                \\\"path\\\",\\n                                \\\"file_to_string\\\",\\n                                \\\"called\\\",\\n                                \\\"structure\\\",\\n                                \\\"Hidden\\\",\\n                                \\\"creating\\\",\\n                                \\\"contents\\\",\\n                                \\\"create\\\",\\n                                \\\"calls\\\",\\n                                \\\"directory\\\",\\n                                \\\"given\\\",\\n                                \\\"JSON\\\",\\n                                \\\"returns\\\",\\n                                \\\"object\\\",\\n                                \\\"representing\\\",\\n                                \\\"starting\\\",\\n                                \\\"including\\\",\\n                                \\\"found\\\",\\n                                \\\"script\\\",\\n                                \\\"representation\\\",\\n                                \\\"file\\\",\\n                                \\\"code\\\",\\n                                \\\"dictionary\\\",\\n                                \\\"function\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\\\\\"codebase.json\\\\\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"Python\\\",\\n                                \\\"packages\\\",\\n                                \\\"install\\\",\\n                                \\\"transformers\\\",\\n                                \\\"CodeLlama\\\",\\n                                \\\"English\\\",\\n                                \\\"accelerate\\\",\\n                                \\\"run\\\",\\n                                \\\"snippet\\\",\\n                                \\\"installed\\\",\\n                                \\\"model\\\",\\n                                \\\"running\\\",\\n                                \\\"working\\\",\\n                                \\\"required\\\",\\n                                \\\"instruction\\\",\\n                                \\\"command\\\",\\n                                \\\"pip\\\",\\n                                \\\"provides\\\",\\n                                \\\"libraries\\\",\\n                                \\\"environment\\\",\\n                                \\\"plain\\\",\\n                                \\\"code\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"Transformers\\\",\\n                                \\\"Python\\\",\\n                                \\\"Face\\\",\\n                                \\\"First\\\",\\n                                \\\"uses\\\",\\n                                \\\"time\\\",\\n                                \\\"generation\\\",\\n                                \\\"enabled\\\",\\n                                \\\"language\\\",\\n                                \\\"transformers\\\",\\n                                \\\"library\\\",\\n                                \\\"output\\\",\\n                                \\\"generated\\\",\\n                                \\\"prints\\\",\\n                                \\\"text\\\",\\n                                \\\"imports\\\",\\n                                \\\"model\\\",\\n                                \\\"use\\\",\\n                                \\\"characters\\\",\\n                                \\\"prompt\\\",\\n                                \\\"load\\\",\\n                                \\\"generate\\\",\\n                                \\\"provided\\\",\\n                                \\\"sampling\\\",\\n                                \\\"expected\\\",\\n                                \\\"continuation\\\",\\n                                \\\"pipeline\\\",\\n                                \\\"provides\\\",\\n                                \\\"Llama\\\",\\n                                \\\"written\\\",\\n                                \\\"length\\\",\\n                                \\\"Hugging\\\",\\n                                \\\"code\\\",\\n                                \\\"function\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\\\\\"Once upon a time\\\\\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                },\n                {\n                    \"name\": \"test_codebase_original.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"keywords\\\": [],\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"test_codebase.json\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\\\\\"test_codebase.json\\\\\\\" representing the directory structure of `self.test_path`.\\\",\\n                    \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\n'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self, path):\\\\n        # Initialize the output dictionary model with folder contents\\\\n        # name, type, keywords, and empty list for children\\\\n        self.path = path\\\\n        self.model = {}\\\\n\\\\n    def file_to_string(self, file_path):  # save file content as string\\\\n        with open(file_path, 'r') as file:\\\\n            file_content = file.read()\\\\n        file.close()\\\\n        return file_content\\\\n\\\\n    def extract(self, path):  # extracts a directory as a json object\\\\n        model = {'name': os.path.basename(path),\\\\n                 'type': 'folder', 'keywords': [], 'children': []}\\\\n        # Check if the path is a directory\\\\n        if not os.path.isdir(path):\\\\n            return model\\\\n\\\\n        # Iterate over the entries in the directory\\\\n        for entry in os.listdir(path):\\\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\\\n                # Create the fill path for current entry\\\\n                entry_path = os.path.join(path, entry)\\\\n                # if the entry is a directory, recursively call the function\\\\n                if os.path.isdir(entry_path):\\\\n                    model['children'].append(self.extract(entry_path))\\\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\\\n                else:\\\\n                    content = \\\\\\\"\\\\\\\"\\\\n                    # save file content as string\\\\n                    try:\\\\n                        content = self.file_to_string(entry_path)\\\\n                    except OSError:\\\\n                        content = \\\\\\\"n/a\\\\\\\"\\\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\\\n                    ], 'annotation': \\\\\\\"\\\\\\\", 'content': content})\\\\n        return model\\\\n\\\\n    def model_to_str(self):  # convert codebase json to string\\\\n        output_str = json.dumps(self.model, indent=4)\\\\n        return output_str\\\\n\\\\n    def save_model_json(self, file_name):  # codebase model json file\\\\n        save_file = open(f\\\\\\\"{file_name}.json\\\\\\\", 'w')\\\\n        self.model = self.extract(self.path)\\\\n        json.dump(self.model, save_file, indent=4)\\\\n        save_file.close()\\\\n        print(f\\\\\\\"Codebase model saved as {file_name}\\\\\\\")\\\\n        return self.model\\\\n\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.test_path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        self.extractor = CodebaseExtract(self.test_path)\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        output = self.extractor.save_model_json(\\\\\\\"test_codebase\\\\\\\")\\\\n        # model_str = self.extractor.model_to_str()\\\\n        # print(f\\\\\\\"Codebase model: {model_str}\\\\\\\")\\\\n        assert type(output) == dict\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testCodebaseExtract = TestCodebaseExtract()\\\\n    testCodebaseExtract.test_extract_codebase()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [],\\n            \\\"annotation\\\": \\\"This document outlines a project called \\\\\\\"Codesense,\\\\\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\\\n\\\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\\\n3. Annotation Generation: Produces text summaries for functions in the code.\\\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\\\n\\\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"tree_traverse\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": []\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"This code is structured as a JSON object representing a project directory called \\\\\\\"codesense,\\\\\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\\\\\"codebase.json\\\\\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [],\\n                            \\\"annotation\\\": \\\"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\\\\\"Once upon a time\\\\\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                }\n            ]\n        },\n        {\n            \"name\": \"question_answering\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"question_answer.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"from openai import OpenAI\\nimport os\\nfrom dotenv import load_dotenv\\nimport sys\\n\\nsys.path.insert(0, \\\"..\\\")\\nfrom annotation_aggregate.annotation_aggregate import AnnotationAggregate\\nfrom utilities.utility import json_to_obj\\n\\n'''\\nCreate a class that responds to a user query given context from the codebase\\n- input:\\n    - traversal result object\\n- output:\\n    - Response to user query as string\\n'''\\n\\n\\nclass QueryAnswer:\\n    def __init__(self, traverse_obj):\\n        self.res = \\\"\\\"\\n        self.traversal = traverse_obj\\n        aggregator = AnnotationAggregate(self.traversal)\\n        self.context = aggregator.aggregate_annotations()\\n\\n    ## Set the API Key\\n    def get_response(self, query):\\n        load_dotenv()\\n        API_KEY = os.getenv('OPENAI_SECRET_API_KEY')\\n        client = OpenAI(api_key=API_KEY)\\n\\n        #GPT4o REPONSE REQUEST\\n        MODEL=\\\"gpt-4o\\\"\\n\\n        completion = client.chat.completions.create(\\n        model=MODEL,\\n        #Prompt modelling, grounding the model to provide a more concise and clear summary when given a piece of code\\n        messages=[\\n            {\\\"role\\\": \\\"system\\\", \\\"content\\\": '''\\n            You are a developer assistant designed to provide detailed answers and assistance based on contextual explanations of code in a codebase. Your input consists of explanations of code files and their respective file directories within the codebase. Users will provide queries related to the codebase, seeking clarification, assistance, or suggestions. Your task is to utilize the provided context to generate clear and structured responses to the user queries. Your responses should be informative, accurate, and tailored to the specific query. Additionally, you may suggest potential actions or direct the user to relevant code files within the codebase for further reference. Your responses should solely rely on the provided context, avoiding external knowledge or assumptions. Remember to maintain clarity and coherence in your responses, ensuring that users can easily understand and follow your guidance. Make sure to keep your responses as short as possible as well so that the developer can quickly view an answer their question.\\n\\n            Example:\\n\\n            Query: How does the event-handling function handle errors during Firestore database operations?\\n\\n            Context:\\n            The code is written in JavaScript, specifically using the async/await syntax to handle asynchronous operations with Firestore, a cloud database from Firebase. It defines an event-handling function `handleCreateEvent` meant to create and save event data into the Firestore database. When a form submission event triggers the function, it first prevents the default behavior with `e.preventDefault()`. The function checks if `isDateRange` is false and, based on this, either adds or updates single or range-dated event documents in the Firestore under the 'events' collection. It also conditionally updates the 'announcements' collection based on the existence of date ranges. After database operations, it resets multiple state variables (title, description, dateTime, etc.) and fetches user data. The function ensures newly created or modified data incorporates the current date and time and user metadata. The expected result includes adding appropriate entries in the Firestore under both 'events' and 'announcements' collections and resetting the form's state. \\n            File Directory: NewsFlash/pages/events.js\\n            \\n\\n            Response:\\n            The event-handling function `handleCreateEvent` in the file events.js employs error handling mechanisms to manage errors during Firestore database operations. Within the async function, try-catch blocks are utilized to capture and handle any potential errors that may occur during asynchronous database transactions. Specifically, when performing Firestore operations such as adding or updating event documents, the try block encapsulates these operations, allowing for graceful error handling. In the event of an error, the catch block is triggered, enabling the function to handle the error appropriately, which may include logging the error, displaying a user-friendly message, or initiating corrective actions. Additionally, the function may utilize Firebase's error handling features, such as error codes or error objects, to provide more detailed information about the nature of the error and facilitate troubleshooting. Overall, the event-handling function is designed to handle errors robustly, ensuring the reliability and stability of database operations.\\n            '''},\\n\\n            {\\\"role\\\": \\\"user\\\", \\\"content\\\": f'''With that said. The query and context is given below:\\n            QUERY: {query}\\n            \\n            CODEBASE CONTEXT: {self.context}\\n            '''}\\n        ]\\n        )\\n        return completion.choices[0].message.content\\n\\n### TESTING \\nclass TestQueryAnswering:\\n     def __init__(self):\\n        self.test_model = json_to_obj(\\\"top_5.json\\\")\\n        self.responder = QueryAnswer(self.test_model)\\n        print(\\\"Testing Query Response... \\\\n\\\")\\n    \\n     def test_keyword_extract_explanation(self):\\n        query=\\\"How does keyword extraction work in this project?\\\"\\n        output = self.responder.get_response(query)\\n        print(output)\\n        assert type(output) == str\\n    \\n\\nif __name__ == \\\"__main__\\\":\\n    TestQueryAnswering = TestQueryAnswering()\\n    TestQueryAnswering.test_keyword_extract_explanation()\"\n                },\n                {\n                    \"name\": \"top_5.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"input_keywords\\\": [\\n        \\\"python\\\",\\n        \\\"function\\\",\\n        \\\"testkeywordextract\\\",\\n        \\\"nltk\\\",\\n        \\\"word2vec\\\",\\n        \\\"extract_keywords\\\"\\n    ],\\n    \\\"results\\\": [\\n        {\\n            \\\"score\\\": 0.8333333333333334,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"python\\\",\\n                \\\"word2vec\\\",\\n                \\\"extract_keywords\\\",\\n                \\\"function\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"main.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"*\\\",\\n                    \\\"Comparison\\\",\\n                    \\\"Embeddings\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Processing\\\",\\n                    \\\"Python\\\",\\n                    \\\"Similarity\\\",\\n                    \\\"Text\\\",\\n                    \\\"Word\\\",\\n                    \\\"Word2Vec\\\",\\n                    \\\"calculates\\\",\\n                    \\\"code\\\",\\n                    \\\"compare_keywords\\\",\\n                    \\\"compare_words\\\",\\n                    \\\"comparing\\\",\\n                    \\\"computes\\\",\\n                    \\\"console\\\",\\n                    \\\"context\\\",\\n                    \\\"create\\\",\\n                    \\\"embeddings\\\",\\n                    \\\"employs\\\",\\n                    \\\"extract_keywords\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"file\\\",\\n                    \\\"focuses\\\",\\n                    \\\"function\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"keywords.The\\\",\\n                    \\\"language\\\",\\n                    \\\"libraries\\\",\\n                    \\\"library\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"model\\\",\\n                    \\\"modeling.1\\\",\\n                    \\\"output\\\",\\n                    \\\"performs\\\",\\n                    \\\"processes\\\",\\n                    \\\"processing\\\",\\n                    \\\"reads\\\",\\n                    \\\"removes\\\",\\n                    \\\"returned\\\",\\n                    \\\"score\\\",\\n                    \\\"sentences\\\",\\n                    \\\"similarity\\\",\\n                    \\\"tagging\\\",\\n                    \\\"techniques\\\",\\n                    \\\"text\\\",\\n                    \\\"texts\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"uses\\\",\\n                    \\\"using\\\",\\n                    \\\"vector\\\",\\n                    \\\"verbs\\\",\\n                    \\\"word\\\",\\n                    \\\"words\\\",\\n                    \\\"words.3\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.5,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"python\\\",\\n                \\\"testkeywordextract\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"keyword_extract.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"English\\\",\\n                    \\\"Language\\\",\\n                    \\\"Natural\\\",\\n                    \\\"Python\\\",\\n                    \\\"TestKeywordExtract\\\",\\n                    \\\"Toolkit\\\",\\n                    \\\"annotated\\\",\\n                    \\\"based\\\",\\n                    \\\"class\\\",\\n                    \\\"code\\\",\\n                    \\\"contains\\\",\\n                    \\\"create\\\",\\n                    \\\"description\\\",\\n                    \\\"ensuring\\\",\\n                    \\\"expected\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"filtering\\\",\\n                    \\\"filters\\\",\\n                    \\\"identifies\\\",\\n                    \\\"includes\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"method\\\",\\n                    \\\"nltk\\\",\\n                    \\\"nouns\\\",\\n                    \\\"output\\\",\\n                    \\\"pieces\\\",\\n                    \\\"processing\\\",\\n                    \\\"provided\\\",\\n                    \\\"query\\\",\\n                    \\\"running\\\",\\n                    \\\"script\\\",\\n                    \\\"selecting\\\",\\n                    \\\"stopwords\\\",\\n                    \\\"tagging\\\",\\n                    \\\"test\\\",\\n                    \\\"text\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.3333333333333333,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"info.txt\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"Automatic\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Python\\\",\\n                    \\\"RAKE\\\",\\n                    \\\"Rapid\\\",\\n                    \\\"SSL\\\",\\n                    \\\"algorithm\\\",\\n                    \\\"certificate\\\",\\n                    \\\"changing\\\",\\n                    \\\"command\\\",\\n                    \\\"commands\\\",\\n                    \\\"consists\\\",\\n                    \\\"downloading\\\",\\n                    \\\"downloads\\\",\\n                    \\\"environment\\\",\\n                    \\\"error\\\",\\n                    \\\"gensim\\\",\\n                    \\\"install\\\",\\n                    \\\"installing\\\",\\n                    \\\"instructs\\\",\\n                    \\\"involves\\\",\\n                    \\\"issue\\\",\\n                    \\\"language\\\",\\n                    \\\"model\\\",\\n                    \\\"occurs\\\",\\n                    \\\"packages\\\",\\n                    \\\"processing\\\",\\n                    \\\"provided\\\",\\n                    \\\"setting\\\",\\n                    \\\"shell\\\",\\n                    \\\"suggests\\\",\\n                    \\\"text\\\",\\n                    \\\"tokenization\\\",\\n                    \\\"use\\\",\\n                    \\\"version\\\",\\n                    \\\"words\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\\\",\\n                \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.3333333333333333,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"codebase.json\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"Additionally\\\",\\n                    \\\"Face\\\",\\n                    \\\"Hugging\\\",\\n                    \\\"JSON\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Python\\\",\\n                    \\\"README\\\",\\n                    \\\"Transformers\\\",\\n                    \\\"annotation\\\",\\n                    \\\"annotations\\\",\\n                    \\\"attributes\\\",\\n                    \\\"called\\\",\\n                    \\\"character\\\",\\n                    \\\"code\\\",\\n                    \\\"codebase\\\",\\n                    \\\"codebases\\\",\\n                    \\\"codesense\\\",\\n                    \\\"components\\\",\\n                    \\\"creating\\\",\\n                    \\\"defined\\\",\\n                    \\\"designed\\\",\\n                    \\\"detailing\\\",\\n                    \\\"directory\\\",\\n                    \\\"employing\\\",\\n                    \\\"environment\\\",\\n                    \\\"expected\\\",\\n                    \\\"extraction\\\",\\n                    \\\"functionalities\\\",\\n                    \\\"game\\\",\\n                    \\\"generating\\\",\\n                    \\\"generation\\\",\\n                    \\\"include\\\",\\n                    \\\"instructions\\\",\\n                    \\\"involve\\\",\\n                    \\\"keyword\\\",\\n                    \\\"lists\\\",\\n                    \\\"methods\\\",\\n                    \\\"model\\\",\\n                    \\\"modeling\\\",\\n                    \\\"object\\\",\\n                    \\\"outputs\\\",\\n                    \\\"performing\\\",\\n                    \\\"project\\\",\\n                    \\\"provide\\\",\\n                    \\\"provided\\\",\\n                    \\\"representations\\\",\\n                    \\\"representing\\\",\\n                    \\\"scripts\\\",\\n                    \\\"setting\\\",\\n                    \\\"structure\\\",\\n                    \\\"structured\\\",\\n                    \\\"structures\\\",\\n                    \\\"summaries\\\",\\n                    \\\"tasks\\\",\\n                    \\\"text\\\",\\n                    \\\"using\\\",\\n                    \\\"utilities\\\",\\n                    \\\"working\\\"\\n                ],\\n                \\\"annotation\\\": \\\"This code is structured as a JSON object representing a project directory called \\\\\\\"codesense,\\\\\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\\\",\\n                \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.3333333333333333,\\n            \\\"matched_keywords\\\": [\\n                \\\"function\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"main.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"Hidden\\\",\\n                    \\\"JSON\\\",\\n                    \\\"Python\\\",\\n                    \\\"called\\\",\\n                    \\\"calls\\\",\\n                    \\\"children\\\",\\n                    \\\"code\\\",\\n                    \\\"codebase.json\\\",\\n                    \\\"contents\\\",\\n                    \\\"converts\\\",\\n                    \\\"create\\\",\\n                    \\\"create_folder_structure_json\\\",\\n                    \\\"creating\\\",\\n                    \\\"designed\\\",\\n                    \\\"dictionary\\\",\\n                    \\\"directories\\\",\\n                    \\\"directory\\\",\\n                    \\\"file\\\",\\n                    \\\"file_path\\\",\\n                    \\\"file_to_string\\\",\\n                    \\\"files\\\",\\n                    \\\"folder\\\",\\n                    \\\"folders\\\",\\n                    \\\"found\\\",\\n                    \\\"function\\\",\\n                    \\\"given\\\",\\n                    \\\"ignored\\\",\\n                    \\\"including\\\",\\n                    \\\"indentation\\\",\\n                    \\\"list\\\",\\n                    \\\"named\\\",\\n                    \\\"names\\\",\\n                    \\\"navigates\\\",\\n                    \\\"nested\\\",\\n                    \\\"object\\\",\\n                    \\\"output\\\",\\n                    \\\"path\\\",\\n                    \\\"prints\\\",\\n                    \\\"read\\\",\\n                    \\\"reads\\\",\\n                    \\\"representation\\\",\\n                    \\\"representing\\\",\\n                    \\\"returns\\\",\\n                    \\\"saves\\\",\\n                    \\\"script\\\",\\n                    \\\"specifies\\\",\\n                    \\\"starting\\\",\\n                    \\\"string\\\",\\n                    \\\"structure\\\",\\n                    \\\"types\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\\\\\"codebase.json\\\\\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\\\",\\n                \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n            }\\n        }\\n    ]\\n}\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"test\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"integration_test\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"rust-calculator\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"keywords\\\": [],\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"Cargo.toml\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [\\n                \\\"crate\\\",\\n                \\\"TOML\\\",\\n                \\\"code\\\",\\n                \\\"version\\\",\\n                \\\"dependency\\\",\\n                \\\"manage\\\",\\n                \\\"dependencies\\\",\\n                \\\"Henry\\\",\\n                \\\"edition\\\",\\n                \\\"file\\\",\\n                \\\"written\\\",\\n                \\\"calc\\\",\\n                \\\"Boisdequin\\\",\\n                \\\"serves\\\",\\n                \\\"Obvious\\\",\\n                \\\"package\\\",\\n                \\\"configuration\\\",\\n                \\\"Cargo\\\",\\n                \\\"information\\\",\\n                \\\"declares\\\",\\n                \\\"project\\\",\\n                \\\"manager\\\",\\n                \\\"used\\\",\\n                \\\"author\\\",\\n                \\\"Minimal\\\",\\n                \\\"specifying\\\",\\n                \\\"Tom\\\",\\n                \\\"Language\\\",\\n                \\\"name\\\",\\n                \\\"Rust\\\"\\n            ],\\n            \\\"annotation\\\": \\\"The code is written in TOML (Tom's Obvious, Minimal Language) and serves as a Cargo manifest file for a Rust project. It defines the package information by specifying the name \\\\\\\"calc\\\\\\\", version \\\\\\\"0.1.0\\\\\\\", author \\\\\\\"Henry Boisdequin\\\\\\\", and the Rust edition \\\\\\\"2018\\\\\\\". Additionally, it declares a dependency on the \\\\\\\"itertools\\\\\\\" crate, version \\\\\\\"0.10\\\\\\\". This configuration will be used by Cargo, Rust's package manager, to manage the project and its dependencies.\\\",\\n            \\\"content\\\": \\\"[package]\\\\nname = \\\\\\\"calc\\\\\\\"\\\\nversion = \\\\\\\"0.1.0\\\\\\\"\\\\nauthors = [\\\\\\\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\\\\\\\"]\\\\nedition = \\\\\\\"2018\\\\\\\"\\\\n\\\\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\\\\n\\\\n[dependencies]\\\\nitertools = \\\\\\\"0.10\\\\\\\"\\\\n\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"Cargo.lock\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [\\n                \\\"registry\\\",\\n                \\\"TOML\\\",\\n                \\\"ensure\\\",\\n                \\\"snippet\\\",\\n                \\\"provided\\\",\\n                \\\"version\\\",\\n                \\\"pulled\\\",\\n                \\\"packages\\\",\\n                \\\"dependencies\\\",\\n                \\\"specifies\\\",\\n                \\\"versions\\\",\\n                \\\"reproducibility\\\",\\n                \\\"file\\\",\\n                \\\"written\\\",\\n                \\\"calc\\\",\\n                \\\"depends\\\",\\n                \\\"Obvious\\\",\\n                \\\"Cargo.toml.lock\\\",\\n                \\\"lists\\\",\\n                \\\"package\\\",\\n                \\\"indicates\\\",\\n                \\\"Cargo\\\",\\n                \\\"checksums\\\",\\n                \\\"includes\\\",\\n                \\\"index\\\",\\n                \\\"project\\\",\\n                \\\"manager\\\",\\n                \\\"used\\\",\\n                \\\"Minimal\\\",\\n                \\\"Tom\\\",\\n                \\\"projects\\\",\\n                \\\"itertools\\\",\\n                \\\"Language\\\",\\n                \\\"Rust\\\"\\n            ],\\n            \\\"annotation\\\": \\\"The provided content is a snippet from a `Cargo.toml.lock` file, written in TOML (Tom's Obvious, Minimal Language), which is used by the Cargo package manager for Rust projects. This file specifies the dependencies for a Rust project and includes exact versions and checksums to ensure reproducibility. It lists three packages: `calc` version 0.1.0, `either` version 1.6.1, and `itertools` version 0.10.0. The `itertools` package depends on the `either` package. The file indicates that these dependencies are pulled from the crates.io index, which is the official package registry for Rust.\\\",\\n            \\\"content\\\": \\\"# This file is automatically @generated by Cargo.\\\\n# It is not intended for manual editing.\\\\n[[package]]\\\\nname = \\\\\\\"calc\\\\\\\"\\\\nversion = \\\\\\\"0.1.0\\\\\\\"\\\\ndependencies = [\\\\n \\\\\\\"itertools\\\\\\\",\\\\n]\\\\n\\\\n[[package]]\\\\nname = \\\\\\\"either\\\\\\\"\\\\nversion = \\\\\\\"1.6.1\\\\\\\"\\\\nsource = \\\\\\\"registry+https://github.com/rust-lang/crates.io-index\\\\\\\"\\\\nchecksum = \\\\\\\"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\\\\\\\"\\\\n\\\\n[[package]]\\\\nname = \\\\\\\"itertools\\\\\\\"\\\\nversion = \\\\\\\"0.10.0\\\\\\\"\\\\nsource = \\\\\\\"registry+https://github.com/rust-lang/crates.io-index\\\\\\\"\\\\nchecksum = \\\\\\\"37d572918e350e82412fe766d24b15e6682fb2ed2bbe018280caa810397cb319\\\\\\\"\\\\ndependencies = [\\\\n \\\\\\\"either\\\\\\\",\\\\n]\\\\n\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [\\n                \\\"ensure\\\",\\n                \\\"command\\\",\\n                \\\"provided\\\",\\n                \\\"code\\\",\\n                \\\"execute\\\",\\n                \\\"cargo\\\",\\n                \\\"outlines\\\",\\n                \\\"enabling\\\",\\n                \\\"installed\\\",\\n                \\\"run\\\",\\n                \\\"line\\\",\\n                \\\"navigate\\\",\\n                \\\"need\\\",\\n                \\\"cases\\\",\\n                \\\"directory\\\",\\n                \\\"test\\\",\\n                \\\"perform\\\",\\n                \\\"use\\\",\\n                \\\"package\\\",\\n                \\\"operations\\\",\\n                \\\"system\\\",\\n                \\\"application\\\",\\n                \\\"compile\\\",\\n                \\\"explanation\\\",\\n                \\\"included\\\",\\n                \\\"project\\\",\\n                \\\"calculator\\\",\\n                \\\"manager\\\",\\n                \\\"repository\\\",\\n                \\\"setup\\\",\\n                \\\"implemented\\\",\\n                \\\"containing\\\",\\n                \\\"Rust\\\"\\n            ],\\n            \\\"annotation\\\": \\\"The provided explanation outlines a simple command-line calculator implemented in Rust. To run this calculator, you need to clone the repository containing the code, ensure that Rust and the cargo package manager are installed on your system, navigate to the project directory, and run the command `cargo run`. To execute any test cases included in the project, use the command `cargo test`. This setup will compile and run the calculator application, enabling you to perform basic arithmetic operations via the command line.\\\",\\n            \\\"content\\\": \\\"Simple command-line calculator in Rust.\\\\n\\\\n## To Run\\\\n\\\\n1. Clone this repository\\\\n\\\\n2. Make sure you have Rust and cargo installed\\\\n\\\\n3. Cd into the project directory and type `cargo run`\\\\n\\\\n4. To test: run `cargo test`\\\\n\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"src\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"main.rs\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [\\n                        \\\"result\\\",\\n                        \\\"splits\\\",\\n                        \\\"ends\\\",\\n                        \\\"list\\\",\\n                        \\\"code\\\",\\n                        \\\"program\\\",\\n                        \\\"addition\\\",\\n                        \\\"equation\\\",\\n                        \\\"predefined\\\",\\n                        \\\"subtraction\\\",\\n                        \\\"applies\\\",\\n                        \\\"q\\\",\\n                        \\\"include\\\",\\n                        \\\"message\\\",\\n                        \\\"numbers\\\",\\n                        \\\"reads\\\",\\n                        \\\"written\\\",\\n                        \\\"serves\\\",\\n                        \\\"prompts\\\",\\n                        \\\"breaks\\\",\\n                        \\\"perform\\\",\\n                        \\\"operators\\\",\\n                        \\\"operations\\\",\\n                        \\\"operator\\\",\\n                        \\\"operation\\\",\\n                        \\\"using\\\",\\n                        \\\"quit\\\",\\n                        \\\"printed\\\",\\n                        \\\"multiplication\\\",\\n                        \\\"substrings\\\",\\n                        \\\"calculator\\\",\\n                        \\\"Calc\\\",\\n                        \\\"converts\\\",\\n                        \\\"used\\\",\\n                        \\\"checks\\\",\\n                        \\\"division\\\",\\n                        \\\"input\\\",\\n                        \\\"valid\\\",\\n                        \\\"corresponding\\\",\\n                        \\\"module\\\",\\n                        \\\"Rust\\\"\\n                    ],\\n                    \\\"annotation\\\": \\\"The code is written in Rust and serves as a basic calculator program. It repeatedly prompts the user to enter a mathematical equation or \\\\\\\"q\\\\\\\" to quit. The `Calc` module is used to perform arithmetic operations, which include addition, subtraction, multiplication, and division. The program reads user input, checks for valid operators from a predefined list, splits the input by the operator, converts the substrings into floating-point numbers, and applies the corresponding arithmetic operation using the `Calc` module. The result is then printed. If the input is \\\\\\\"q\\\\\\\", the loop breaks, and the program ends with a thank-you message.\\\",\\n                    \\\"content\\\": \\\"mod calc;\\\\nuse calc::Calc;\\\\nuse std::io;\\\\n\\\\nfn main() {\\\\n    println!(\\\\\\\"Welcome to the a basic calculator built with Rust.\\\\\\\");\\\\n\\\\n    loop {\\\\n        println!(\\\\\\\"Please enter an equation or \\\\\\\\\\\\\\\"q\\\\\\\\\\\\\\\" to quit: \\\\\\\");\\\\n\\\\n        let mut input = String::new();\\\\n        io::stdin()\\\\n            .read_line(&mut input)\\\\n            .expect(\\\\\\\"Failed to read input\\\\\\\");\\\\n\\\\n        if input.trim() == \\\\\\\"q\\\\\\\" {\\\\n            println!(\\\\\\\"Thanks for using this program.\\\\\\\");\\\\n            break;\\\\n        }\\\\n\\\\n        let valid_operators = vec![\\\\\\\"+\\\\\\\", \\\\\\\"-\\\\\\\", \\\\\\\"*\\\\\\\", \\\\\\\"/\\\\\\\"];\\\\n\\\\n        for operator in valid_operators {\\\\n            match input.find(operator) {\\\\n                Some(_) => {\\\\n                    let parts: Vec<&str> = input.split(operator).collect();\\\\n\\\\n                    if parts.len() < 2 {\\\\n                        panic!(\\\\\\\"Invalid equation.\\\\\\\");\\\\n                    }\\\\n\\\\n                    let mut number_array = vec![];\\\\n                    let mut counter = 0;\\\\n\\\\n                    while counter != parts.len() {\\\\n                        let val: f64 = parts[counter].trim().parse().ok().expect(\\\\\\\"Enter a number.\\\\\\\");\\\\n                        number_array.push(val);\\\\n                        counter += 1;\\\\n                    }\\\\n\\\\n                    match operator {\\\\n                        \\\\\\\"+\\\\\\\" => println!(\\\\\\\"{}\\\\\\\", Calc::add(number_array)),\\\\n                        \\\\\\\"-\\\\\\\" => println!(\\\\\\\"{}\\\\\\\", Calc::sub(number_array)),\\\\n                        \\\\\\\"*\\\\\\\" => println!(\\\\\\\"{}\\\\\\\", Calc::mul(number_array)),\\\\n                        \\\\\\\"/\\\\\\\" => println!(\\\\\\\"{}\\\\\\\", Calc::div(number_array)),\\\\n                        _ => println!(\\\\\\\"Only addition, subtraction, multiplication and division are supported.\\\\\\\")\\\\n                    }\\\\n                }\\\\n\\\\n                None => {\\\\n                    continue;\\\\n                }\\\\n            }\\\\n        }\\\\n    }\\\\n}\\\\n\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"calc.rs\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [\\n                        \\\"f64\\\",\\n                        \\\"provided\\\",\\n                        \\\"list\\\",\\n                        \\\"code\\\",\\n                        \\\"addition\\\",\\n                        \\\"uses\\\",\\n                        \\\"subtraction\\\",\\n                        \\\"elements\\\",\\n                        \\\"results\\\",\\n                        \\\"contains\\\",\\n                        \\\"vectors\\\",\\n                        \\\"unit\\\",\\n                        \\\"assertions\\\",\\n                        \\\"numbers\\\",\\n                        \\\"tests\\\",\\n                        \\\"verify\\\",\\n                        \\\"written\\\",\\n                        \\\"element.The\\\",\\n                        \\\"return\\\",\\n                        \\\"expected\\\",\\n                        \\\"Expected\\\",\\n                        \\\"behave\\\",\\n                        \\\"defines\\\",\\n                        \\\"operations\\\",\\n                        \\\"output\\\",\\n                        \\\"mul\\\",\\n                        \\\"subsequent\\\",\\n                        \\\"element\\\",\\n                        \\\"sub\\\",\\n                        \\\"multiplication\\\",\\n                        \\\"methods\\\",\\n                        \\\"work\\\",\\n                        \\\"div\\\",\\n                        \\\"intended\\\",\\n                        \\\"test_all_operations\\\",\\n                        \\\"check\\\",\\n                        \\\"confirm\\\",\\n                        \\\">\\\",\\n                        \\\"add\\\",\\n                        \\\"Calc\\\",\\n                        \\\"Subtracts\\\",\\n                        \\\"struct\\\",\\n                        \\\"division\\\",\\n                        \\\"Multiplies\\\",\\n                        \\\"Divides\\\",\\n                        \\\"function\\\",\\n                        \\\"provides\\\",\\n                        \\\"Rust\\\",\\n                        \\\"aggregation\\\"\\n                    ],\\n                    \\\"annotation\\\": \\\"This code is written in Rust and defines a `Calc` struct that provides methods for basic arithmetic operations: addition, subtraction, multiplication, and division on vectors of floating-point numbers (`Vec<f64>`). The methods `add`, `sub`, `mul`, and `div` perform aggregation operations on the provided list of numbers. \\\\n\\\\n- `add`: Sums all elements in the vector.\\\\n- `sub`: Subtracts each subsequent element from the first element.\\\\n- `mul`: Multiplies all elements together.\\\\n- `div`: Divides the first element by each subsequent element.\\\\n\\\\nThe `test_all_operations` function contains unit tests to verify that these methods work as expected. It uses assertions to check that the operations return the correct results for various input vectors. Expected output for the tests would confirm all operations behave as intended.\\\",\\n                    \\\"content\\\": \\\"use itertools::Itertools;\\\\nuse std::ops::{Div, Sub};\\\\n\\\\npub struct Calc;\\\\n\\\\nimpl Calc {\\\\n    pub fn add(arr: Vec<f64>) -> f64 {\\\\n        arr.iter().sum::<f64>()\\\\n    }\\\\n\\\\n    pub fn sub(arr: Vec<f64>) -> f64 {\\\\n        arr.iter().map(|&x| x as f64).fold1(Sub::sub).unwrap_or(0.0)\\\\n    }\\\\n\\\\n    pub fn mul(arr: Vec<f64>) -> f64 {\\\\n        arr.iter().product()\\\\n    }\\\\n\\\\n    pub fn div(arr: Vec<f64>) -> f64 {\\\\n        arr.iter().map(|&x| x as f64).fold1(Div::div).unwrap_or(0.0)\\\\n    }\\\\n}\\\\n\\\\n#[test]\\\\nfn test_all_operations() {\\\\n    // addition\\\\n    assert_eq!(Calc::add([2.0, 4.0, 6.0].to_vec()), 12.0);\\\\n    assert_eq!(Calc::add([-6.0, 5.0, 10.0].to_vec()), 9.0);\\\\n\\\\n    // subtraction\\\\n    assert_eq!(Calc::sub([10.0, 4.0, 6.0].to_vec()), 0.0);\\\\n    assert_eq!(Calc::sub([100.0, 10.0, 19.0].to_vec()), 71.0);\\\\n\\\\n    // multiplication\\\\n    assert_eq!(Calc::mul([10.0, 10.0, 2.0].to_vec()), 200.0);\\\\n    assert_eq!(Calc::mul([-3.0, 2.0].to_vec()), -6.0);\\\\n\\\\n    // division\\\\n    assert_eq!(Calc::div([54.0, 2.0, 3.0].to_vec()), 9.0);\\\\n    assert_eq!(Calc::div([4.0, 2.0, 5.0].to_vec()), 0.4);\\\\n}\\\\n\\\"\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"ignore.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"\"\n                        },\n                        {\n                            \"name\": \"integration.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import sys\\n\\n\\nsys.path.insert(0, \\\"../..\\\")\\nfrom codebase_extract.codebase_extract import CodebaseExtract\\nfrom populate_annotations.populate_annotations import PopulateAnnotations\\nfrom populate_keywords.populate_keywords import PopulateKeywords\\nfrom keyword_extract.keyword_extract import KeywordExtract\\nfrom tree_traverse.tree_traverse import TraverseCodebase\\nfrom question_answering.question_answer import QueryAnswer\\nfrom utilities.utility import obj_to_json\\n\\n'''\\nCreate a class that can run a full integration test of codesense\\n- input: Question as a string\\n- output: Answer as a string\\n'''\\n\\nclass Integration:\\n    def __init__(self, code_base_dir, ignore_paths_file):\\n        self.path = code_base_dir\\n        self.ignore_paths_file = ignore_paths_file\\n        self.code_base_model = {}\\n        self.search_result = {}\\n    \\n    def model_codebase(self):\\n        # Extract Codebase\\n        codebase_extractor = CodebaseExtract(self.path)\\n        self.code_base_model = codebase_extractor.get_model()\\n        # Populate Annotations\\n        populate_annotations = PopulateAnnotations(self.code_base_model, self.ignore_paths_file)\\n        self.code_base_model = populate_annotations.populate_model()\\n        # Populate Keywords\\n        populate_keywords = PopulateKeywords(self.code_base_model)\\n        self.code_base_model = populate_keywords.populate_model()\\n        obj_to_json(\\\"./\\\", \\\"codebase\\\", self.code_base_model)\\n\\n    def query(self, question):\\n        # Extract Keywords\\n        extract_keywords = KeywordExtract()\\n        query_keywords = extract_keywords.extract(question)\\n        # Traverse Tree\\n        traverser = TraverseCodebase(self.code_base_model)\\n        self.search_result = traverser.get_top_nodes(query_keywords, 5)\\n        # Question Answer\\n        responder = QueryAnswer(self.search_result)\\n        response = responder.get_response(question)\\n        return response\\n\\nclass TestIntegration:\\n    def __init__(self):\\n        print(\\\"INTEGRATION TEST\\\")\\n        test_code_base = \\\"rust_calculator_project\\\"\\n        test_ignore_file = \\\"ignore.txt\\\"\\n        self.integration = Integration(test_code_base, test_ignore_file)\\n    \\n    def test_run(self):\\n        print(\\\"modelling codebase...\\\")\\n        self.integration.model_codebase()\\n        #Q1\\n        question = \\\"Does this project have a multiplication capability?\\\"\\n        print(f\\\"Q: {question}\\\")\\n        print(\\\"querying codebase...\\\")\\n        response = self.integration.query(question)\\n        print(f\\\"RESPONSE: \\\\n{response}\\\\n\\\")\\n        #Q2\\n        question = \\\"does it have a square operation functionality?\\\"\\n        print(f\\\"Q: {question}\\\")\\n        print(\\\"querying codebase...\\\")\\n        response = self.integration.query(question)\\n        print(f\\\"RESPONSE: \\\\n{response}\\\\n\\\")\\n        #Q3\\n        question = \\\"how would we modify the code to add a square function?\\\"\\n        print(f\\\"Q: {question}\\\")\\n        print(\\\"querying codebase...\\\")\\n        response = self.integration.query(question)\\n        print(f\\\"RESPONSE: \\\\n{response}\\\\n\\\")\\n    \\n    def test_run_loop_prompt(self):\\n        print(\\\"modelling codebase...\\\")\\n        self.integration.model_codebase()\\n        while True:\\n            question = input(\\\"QUESTION: \\\")\\n            print(\\\"querying codebase...\\\")\\n            response = self.integration.query(question)\\n            print(f\\\"RESPONSE: \\\\n{response}\\\\n\\\")\\n        \\n        \\nif __name__ == \\\"__main__\\\":\\n    testIntegration = TestIntegration()\\n    testIntegration.test_run()\"\n                        },\n                        {\n                            \"name\": \"rust_calculator_project\",\n                            \"type\": \"folder\",\n                            \"keywords\": [],\n                            \"children\": [\n                                {\n                                    \"name\": \"Cargo.lock\",\n                                    \"type\": \"file\",\n                                    \"keywords\": [],\n                                    \"annotation\": \"\",\n                                    \"content\": \"# This file is automatically @generated by Cargo.\\n# It is not intended for manual editing.\\n[[package]]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\ndependencies = [\\n \\\"itertools\\\",\\n]\\n\\n[[package]]\\nname = \\\"either\\\"\\nversion = \\\"1.6.1\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\\\"\\n\\n[[package]]\\nname = \\\"itertools\\\"\\nversion = \\\"0.10.0\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"37d572918e350e82412fe766d24b15e6682fb2ed2bbe018280caa810397cb319\\\"\\ndependencies = [\\n \\\"either\\\",\\n]\\n\"\n                                },\n                                {\n                                    \"name\": \"Cargo.toml\",\n                                    \"type\": \"file\",\n                                    \"keywords\": [],\n                                    \"annotation\": \"\",\n                                    \"content\": \"[package]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\nauthors = [\\\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\\\"]\\nedition = \\\"2018\\\"\\n\\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\\n\\n[dependencies]\\nitertools = \\\"0.10\\\"\\n\"\n                                },\n                                {\n                                    \"name\": \"README.md\",\n                                    \"type\": \"file\",\n                                    \"keywords\": [],\n                                    \"annotation\": \"\",\n                                    \"content\": \"Simple command-line calculator in Rust.\\n\\n## To Run\\n\\n1. Clone this repository\\n\\n2. Make sure you have Rust and cargo installed\\n\\n3. Cd into the project directory and type `cargo run`\\n\\n4. To test: run `cargo test`\\n\"\n                                },\n                                {\n                                    \"name\": \"src\",\n                                    \"type\": \"folder\",\n                                    \"keywords\": [],\n                                    \"children\": [\n                                        {\n                                            \"name\": \"calc.rs\",\n                                            \"type\": \"file\",\n                                            \"keywords\": [],\n                                            \"annotation\": \"\",\n                                            \"content\": \"use itertools::Itertools;\\nuse std::ops::{Div, Sub};\\n\\npub struct Calc;\\n\\nimpl Calc {\\n    pub fn add(arr: Vec<f64>) -> f64 {\\n        arr.iter().sum::<f64>()\\n    }\\n\\n    pub fn sub(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Sub::sub).unwrap_or(0.0)\\n    }\\n\\n    pub fn mul(arr: Vec<f64>) -> f64 {\\n        arr.iter().product()\\n    }\\n\\n    pub fn div(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Div::div).unwrap_or(0.0)\\n    }\\n}\\n\\n#[test]\\nfn test_all_operations() {\\n    // addition\\n    assert_eq!(Calc::add([2.0, 4.0, 6.0].to_vec()), 12.0);\\n    assert_eq!(Calc::add([-6.0, 5.0, 10.0].to_vec()), 9.0);\\n\\n    // subtraction\\n    assert_eq!(Calc::sub([10.0, 4.0, 6.0].to_vec()), 0.0);\\n    assert_eq!(Calc::sub([100.0, 10.0, 19.0].to_vec()), 71.0);\\n\\n    // multiplication\\n    assert_eq!(Calc::mul([10.0, 10.0, 2.0].to_vec()), 200.0);\\n    assert_eq!(Calc::mul([-3.0, 2.0].to_vec()), -6.0);\\n\\n    // division\\n    assert_eq!(Calc::div([54.0, 2.0, 3.0].to_vec()), 9.0);\\n    assert_eq!(Calc::div([4.0, 2.0, 5.0].to_vec()), 0.4);\\n}\\n\"\n                                        },\n                                        {\n                                            \"name\": \"main.rs\",\n                                            \"type\": \"file\",\n                                            \"keywords\": [],\n                                            \"annotation\": \"\",\n                                            \"content\": \"mod calc;\\nuse calc::Calc;\\nuse std::io;\\n\\nfn main() {\\n    println!(\\\"Welcome to the a basic calculator built with Rust.\\\");\\n\\n    loop {\\n        println!(\\\"Please enter an equation or \\\\\\\"q\\\\\\\" to quit: \\\");\\n\\n        let mut input = String::new();\\n        io::stdin()\\n            .read_line(&mut input)\\n            .expect(\\\"Failed to read input\\\");\\n\\n        if input.trim() == \\\"q\\\" {\\n            println!(\\\"Thanks for using this program.\\\");\\n            break;\\n        }\\n\\n        let valid_operators = vec![\\\"+\\\", \\\"-\\\", \\\"*\\\", \\\"/\\\"];\\n\\n        for operator in valid_operators {\\n            match input.find(operator) {\\n                Some(_) => {\\n                    let parts: Vec<&str> = input.split(operator).collect();\\n\\n                    if parts.len() < 2 {\\n                        panic!(\\\"Invalid equation.\\\");\\n                    }\\n\\n                    let mut number_array = vec![];\\n                    let mut counter = 0;\\n\\n                    while counter != parts.len() {\\n                        let val: f64 = parts[counter].trim().parse().ok().expect(\\\"Enter a number.\\\");\\n                        number_array.push(val);\\n                        counter += 1;\\n                    }\\n\\n                    match operator {\\n                        \\\"+\\\" => println!(\\\"{}\\\", Calc::add(number_array)),\\n                        \\\"-\\\" => println!(\\\"{}\\\", Calc::sub(number_array)),\\n                        \\\"*\\\" => println!(\\\"{}\\\", Calc::mul(number_array)),\\n                        \\\"/\\\" => println!(\\\"{}\\\", Calc::div(number_array)),\\n                        _ => println!(\\\"Only addition, subtraction, multiplication and division are supported.\\\")\\n                    }\\n                }\\n\\n                None => {\\n                    continue;\\n                }\\n            }\\n        }\\n    }\\n}\\n\"\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                            \"name\": \"sample_test_output.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"INTEGRATION TEST\\nmodelling codebase...\\njson file saved: ./codebase.json\\nQ: Does this project have a multiplication capability?\\nquerying codebase...\\nRESPONSE: \\nYes, this project does have multiplication capability. The `calc.rs` file defines a `Calc` struct with a method named `mul` that performs multiplication on vectors of floating-point numbers (`Vec<f64>`). This method multiplies all elements in the provided vector together. Additionally, the `main.rs` file handles user input and uses the `Calc` module to perform various arithmetic operations, including multiplication.\\n\\nQ: does it have a square operation functionality?\\nquerying codebase...\\nRESPONSE: \\nBased on the provided context, the codebase does not include a square operation functionality. The `Calc` struct in `calc.rs` offers methods for addition, subtraction, multiplication, and division, but it does not mention any support for squaring a number. \\n\\nTo add such functionality, you would need to implement a new method, such as:\\n\\n```rust\\nimpl Calc {\\n    // Other methods...\\n\\n    pub fn square(&self, x: f64) -> f64 {\\n        x * x\\n    }\\n}\\n```\\n\\nYou would also need to modify the `main.rs` file to handle the square operation input from the user.\\n\\nQ: how would we modify the code to add a square function?\\nquerying codebase...\\nRESPONSE: \\nTo add a `square` function to the existing `Calc` struct in `calc.rs`, you would define a new method called `square` in the `impl` block. This function will take a `Vec<f64>` and return a new `Vec<f64>` where each element is the square of the corresponding element in the input vector. Additionally, you will need to add a test case for this new function in the `test_all_operations` function.\\n\\nHere is how you can modify the `calc.rs` file:\\n\\n```rust\\n// Adding new square method to the Calc struct\\nimpl Calc {\\n    // Existing methods: add, sub, mul, div\\n\\n    pub fn square(numbers: Vec<f64>) -> Vec<f64> {\\n        numbers.into_iter().map(|x| x * x).collect()\\n    }\\n}\\n\\n// Adding test case for the new square method\\n#[cfg(test)]\\nmod tests {\\n    use super::*;\\n\\n    #[test]\\n    fn test_all_operations() {\\n        // Existing test cases for add, sub, mul, div\\n\\n        // Test case for square\\n        let numbers = vec![1.0, 2.0, 3.0];\\n        let squared = Calc::square(numbers.clone());\\n        assert_eq!(squared, vec![1.0, 4.0, 9.0]);\\n    }\\n}\\n```\\n\\nThis modification includes a new method `square` that maps each number in the input vector to its square and collects the results into a new vector. The added test case verifies that the `square` method works as expected.\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"keywords\\\": [],\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [\\n                        \\\"English\\\",\\n                        \\\"Language\\\",\\n                        \\\"Natural\\\",\\n                        \\\"Python\\\",\\n                        \\\"TestKeywordExtract\\\",\\n                        \\\"Toolkit\\\",\\n                        \\\"annotated\\\",\\n                        \\\"based\\\",\\n                        \\\"class\\\",\\n                        \\\"code\\\",\\n                        \\\"contains\\\",\\n                        \\\"create\\\",\\n                        \\\"description\\\",\\n                        \\\"ensuring\\\",\\n                        \\\"expected\\\",\\n                        \\\"extracted\\\",\\n                        \\\"extraction\\\",\\n                        \\\"extracts\\\",\\n                        \\\"filtering\\\",\\n                        \\\"filters\\\",\\n                        \\\"identifies\\\",\\n                        \\\"includes\\\",\\n                        \\\"input\\\",\\n                        \\\"keyword\\\",\\n                        \\\"keywords\\\",\\n                        \\\"list\\\",\\n                        \\\"lists\\\",\\n                        \\\"method\\\",\\n                        \\\"nltk\\\",\\n                        \\\"nouns\\\",\\n                        \\\"output\\\",\\n                        \\\"pieces\\\",\\n                        \\\"processing\\\",\\n                        \\\"provided\\\",\\n                        \\\"query\\\",\\n                        \\\"running\\\",\\n                        \\\"script\\\",\\n                        \\\"selecting\\\",\\n                        \\\"stopwords\\\",\\n                        \\\"tagging\\\",\\n                        \\\"test\\\",\\n                        \\\"text\\\",\\n                        \\\"tokenizes\\\",\\n                        \\\"written\\\"\\n                    ],\\n                    \\\"annotation\\\": \\\"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [\\n                        \\\"Character\\\",\\n                        \\\"Python\\\",\\n                        \\\"attributes\\\",\\n                        \\\"character\\\",\\n                        \\\"class\\\",\\n                        \\\"code\\\",\\n                        \\\"created\\\",\\n                        \\\"damage\\\",\\n                        \\\"double_speed\\\",\\n                        \\\"doubled\\\",\\n                        \\\"doubles\\\",\\n                        \\\"game\\\",\\n                        \\\"health\\\",\\n                        \\\"includes\\\",\\n                        \\\"initialized\\\",\\n                        \\\"instances\\\",\\n                        \\\"models\\\",\\n                        \\\"named\\\",\\n                        \\\"ninja\\\",\\n                        \\\"output\\\",\\n                        \\\"parameters\\\",\\n                        \\\"printed\\\",\\n                        \\\"showcase\\\",\\n                        \\\"speed\\\",\\n                        \\\"speeds\\\",\\n                        \\\"updated\\\",\\n                        \\\"using\\\",\\n                        \\\"warrior\\\",\\n                        \\\"written\\\"\\n                    ],\\n                    \\\"annotation\\\": \\\"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"test_codebase.json\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"annotation\\\": \\\"\\\",\\n                    \\\"content\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"keywords\\\": [\\n                        \\\"Expected\\\",\\n                        \\\"JSON\\\",\\n                        \\\"Python\\\",\\n                        \\\"class\\\",\\n                        \\\"code\\\",\\n                        \\\"content\\\",\\n                        \\\"contents\\\",\\n                        \\\"converts\\\",\\n                        \\\"create\\\",\\n                        \\\"defines\\\",\\n                        \\\"designed\\\",\\n                        \\\"directories\\\",\\n                        \\\"directory\\\",\\n                        \\\"file\\\",\\n                        \\\"file_to_string\\\",\\n                        \\\"files\\\",\\n                        \\\"functionality\\\",\\n                        \\\"generates\\\",\\n                        \\\"given\\\",\\n                        \\\"leaf\\\",\\n                        \\\"method\\\",\\n                        \\\"model\\\",\\n                        \\\"model_to_str\\\",\\n                        \\\"named\\\",\\n                        \\\"nodes\\\",\\n                        \\\"output\\\",\\n                        \\\"provided\\\",\\n                        \\\"reads\\\",\\n                        \\\"representing\\\",\\n                        \\\"save_model_json\\\",\\n                        \\\"self.test_path\\\",\\n                        \\\"stores\\\",\\n                        \\\"string\\\",\\n                        \\\"structure\\\",\\n                        \\\"test_codebase.json\\\",\\n                        \\\"tests\\\",\\n                        \\\"traversing\\\",\\n                        \\\"treating\\\",\\n                        \\\"tree\\\",\\n                        \\\"writes\\\"\\n                    ],\\n                    \\\"annotation\\\": \\\"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\\\\\"test_codebase.json\\\\\\\" representing the directory structure of `self.test_path`.\\\",\\n                    \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\n'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self, path):\\\\n        # Initialize the output dictionary model with folder contents\\\\n        # name, type, keywords, and empty list for children\\\\n        self.path = path\\\\n        self.model = {}\\\\n\\\\n    def file_to_string(self, file_path):  # save file content as string\\\\n        with open(file_path, 'r') as file:\\\\n            file_content = file.read()\\\\n        file.close()\\\\n        return file_content\\\\n\\\\n    def extract(self, path):  # extracts a directory as a json object\\\\n        model = {'name': os.path.basename(path),\\\\n                 'type': 'folder', 'keywords': [], 'children': []}\\\\n        # Check if the path is a directory\\\\n        if not os.path.isdir(path):\\\\n            return model\\\\n\\\\n        # Iterate over the entries in the directory\\\\n        for entry in os.listdir(path):\\\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\\\n                # Create the fill path for current entry\\\\n                entry_path = os.path.join(path, entry)\\\\n                # if the entry is a directory, recursively call the function\\\\n                if os.path.isdir(entry_path):\\\\n                    model['children'].append(self.extract(entry_path))\\\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\\\n                else:\\\\n                    content = \\\\\\\"\\\\\\\"\\\\n                    # save file content as string\\\\n                    try:\\\\n                        content = self.file_to_string(entry_path)\\\\n                    except OSError:\\\\n                        content = \\\\\\\"n/a\\\\\\\"\\\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\\\n                    ], 'annotation': \\\\\\\"\\\\\\\", 'content': content})\\\\n        return model\\\\n\\\\n    def model_to_str(self):  # convert codebase json to string\\\\n        output_str = json.dumps(self.model, indent=4)\\\\n        return output_str\\\\n\\\\n    def save_model_json(self, file_name):  # codebase model json file\\\\n        save_file = open(f\\\\\\\"{file_name}.json\\\\\\\", 'w')\\\\n        self.model = self.extract(self.path)\\\\n        json.dump(self.model, save_file, indent=4)\\\\n        save_file.close()\\\\n        print(f\\\\\\\"Codebase model saved as {file_name}\\\\\\\")\\\\n        return self.model\\\\n\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.test_path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        self.extractor = CodebaseExtract(self.test_path)\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        output = self.extractor.save_model_json(\\\\\\\"test_codebase\\\\\\\")\\\\n        # model_str = self.extractor.model_to_str()\\\\n        # print(f\\\\\\\"Codebase model: {model_str}\\\\\\\")\\\\n        assert type(output) == dict\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testCodebaseExtract = TestCodebaseExtract()\\\\n    testCodebaseExtract.test_extract_codebase()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"keywords\\\": [\\n                \\\"Aggregation\\\",\\n                \\\"Annotation\\\",\\n                \\\"Answering\\\",\\n                \\\"Call\\\",\\n                \\\"CodeBase\\\",\\n                \\\"Codesense\\\",\\n                \\\"Compiles\\\",\\n                \\\"Creates\\\",\\n                \\\"Extraction\\\",\\n                \\\"Generates\\\",\\n                \\\"Generation\\\",\\n                \\\"Graph\\\",\\n                \\\"Identifies\\\",\\n                \\\"Keyword\\\",\\n                \\\"Produces\\\",\\n                \\\"Question\\\",\\n                \\\"Searches\\\",\\n                \\\"Traversal\\\",\\n                \\\"Tree\\\",\\n                \\\"Uses\\\",\\n                \\\"aggregated\\\",\\n                \\\"analyze\\\",\\n                \\\"annotation.7\\\",\\n                \\\"annotations\\\",\\n                \\\"answer\\\",\\n                \\\"based\\\",\\n                \\\"call\\\",\\n                \\\"called\\\",\\n                \\\"code\\\",\\n                \\\"code.4\\\",\\n                \\\"codebase\\\",\\n                \\\"codebase.2\\\",\\n                \\\"codebases\\\",\\n                \\\"components\\\",\\n                \\\"directed\\\",\\n                \\\"document\\\",\\n                \\\"file.3\\\",\\n                \\\"flows\\\",\\n                \\\"function\\\",\\n                \\\"functions\\\",\\n                \\\"graph.5\\\",\\n                \\\"implementations\\\",\\n                \\\"include\\\",\\n                \\\"involves\\\",\\n                \\\"keywords\\\",\\n                \\\"matching\\\",\\n                \\\"nodes\\\",\\n                \\\"objectives\\\",\\n                \\\"outlines\\\",\\n                \\\"project\\\",\\n                \\\"providing\\\",\\n                \\\"queries.6\\\",\\n                \\\"related\\\",\\n                \\\"report\\\",\\n                \\\"representing\\\",\\n                \\\"returns\\\",\\n                \\\"serves\\\",\\n                \\\"showing\\\",\\n                \\\"source\\\",\\n                \\\"structure\\\",\\n                \\\"summaries\\\",\\n                \\\"target\\\",\\n                \\\"tasks\\\",\\n                \\\"tree\\\",\\n                \\\"user\\\"\\n            ],\\n            \\\"annotation\\\": \\\"This document outlines a project called \\\\\\\"Codesense,\\\\\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\\\n\\\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\\\n3. Annotation Generation: Produces text summaries for functions in the code.\\\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\\\n\\\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"tree_traverse\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": []\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"keywords\\\": [],\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"Automatic\\\",\\n                                \\\"Extraction\\\",\\n                                \\\"Gensim\\\",\\n                                \\\"Keyword\\\",\\n                                \\\"NLP\\\",\\n                                \\\"NLTK\\\",\\n                                \\\"Python\\\",\\n                                \\\"RAKE\\\",\\n                                \\\"Rapid\\\",\\n                                \\\"SSL\\\",\\n                                \\\"algorithm\\\",\\n                                \\\"certificate\\\",\\n                                \\\"changing\\\",\\n                                \\\"command\\\",\\n                                \\\"commands\\\",\\n                                \\\"consists\\\",\\n                                \\\"downloading\\\",\\n                                \\\"downloads\\\",\\n                                \\\"environment\\\",\\n                                \\\"error\\\",\\n                                \\\"gensim\\\",\\n                                \\\"install\\\",\\n                                \\\"installing\\\",\\n                                \\\"instructs\\\",\\n                                \\\"involves\\\",\\n                                \\\"issue\\\",\\n                                \\\"language\\\",\\n                                \\\"model\\\",\\n                                \\\"occurs\\\",\\n                                \\\"packages\\\",\\n                                \\\"processing\\\",\\n                                \\\"provided\\\",\\n                                \\\"setting\\\",\\n                                \\\"shell\\\",\\n                                \\\"suggests\\\",\\n                                \\\"text\\\",\\n                                \\\"tokenization\\\",\\n                                \\\"use\\\",\\n                                \\\"version\\\",\\n                                \\\"words\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"*\\\",\\n                                \\\"Comparison\\\",\\n                                \\\"Embeddings\\\",\\n                                \\\"Extraction\\\",\\n                                \\\"Gensim\\\",\\n                                \\\"Keyword\\\",\\n                                \\\"NLP\\\",\\n                                \\\"NLTK\\\",\\n                                \\\"Processing\\\",\\n                                \\\"Python\\\",\\n                                \\\"Similarity\\\",\\n                                \\\"Text\\\",\\n                                \\\"Word\\\",\\n                                \\\"Word2Vec\\\",\\n                                \\\"calculates\\\",\\n                                \\\"code\\\",\\n                                \\\"compare_keywords\\\",\\n                                \\\"compare_words\\\",\\n                                \\\"comparing\\\",\\n                                \\\"computes\\\",\\n                                \\\"console\\\",\\n                                \\\"context\\\",\\n                                \\\"create\\\",\\n                                \\\"embeddings\\\",\\n                                \\\"employs\\\",\\n                                \\\"extract_keywords\\\",\\n                                \\\"extracted\\\",\\n                                \\\"extraction\\\",\\n                                \\\"extracts\\\",\\n                                \\\"file\\\",\\n                                \\\"focuses\\\",\\n                                \\\"function\\\",\\n                                \\\"input\\\",\\n                                \\\"keyword\\\",\\n                                \\\"keywords\\\",\\n                                \\\"keywords.The\\\",\\n                                \\\"language\\\",\\n                                \\\"libraries\\\",\\n                                \\\"library\\\",\\n                                \\\"list\\\",\\n                                \\\"lists\\\",\\n                                \\\"model\\\",\\n                                \\\"modeling.1\\\",\\n                                \\\"output\\\",\\n                                \\\"performs\\\",\\n                                \\\"processes\\\",\\n                                \\\"processing\\\",\\n                                \\\"reads\\\",\\n                                \\\"removes\\\",\\n                                \\\"returned\\\",\\n                                \\\"score\\\",\\n                                \\\"sentences\\\",\\n                                \\\"similarity\\\",\\n                                \\\"tagging\\\",\\n                                \\\"techniques\\\",\\n                                \\\"text\\\",\\n                                \\\"texts\\\",\\n                                \\\"tokenizes\\\",\\n                                \\\"uses\\\",\\n                                \\\"using\\\",\\n                                \\\"vector\\\",\\n                                \\\"verbs\\\",\\n                                \\\"word\\\",\\n                                \\\"words\\\",\\n                                \\\"words.3\\\",\\n                                \\\"written\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"C++\\\",\\n                                \\\"Solution\\\",\\n                                \\\"adds\\\",\\n                                \\\"calculates\\\",\\n                                \\\"captures\\\",\\n                                \\\"class\\\",\\n                                \\\"description\\\",\\n                                \\\"difference\\\",\\n                                \\\"element\\\",\\n                                \\\"end\\\",\\n                                \\\"function\\\",\\n                                \\\"increase\\\",\\n                                \\\"iterates\\\",\\n                                \\\"list\\\",\\n                                \\\"maxP\\\",\\n                                \\\"maxProfit\\\",\\n                                \\\"method\\\",\\n                                \\\"opportunities\\\",\\n                                \\\"price\\\",\\n                                \\\"prices\\\",\\n                                \\\"profit\\\",\\n                                \\\"refers\\\",\\n                                \\\"representing\\\",\\n                                \\\"returns\\\",\\n                                \\\"sets\\\",\\n                                \\\"stock\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"Additionally\\\",\\n                                \\\"Face\\\",\\n                                \\\"Hugging\\\",\\n                                \\\"JSON\\\",\\n                                \\\"NLTK\\\",\\n                                \\\"Python\\\",\\n                                \\\"README\\\",\\n                                \\\"Transformers\\\",\\n                                \\\"annotation\\\",\\n                                \\\"annotations\\\",\\n                                \\\"attributes\\\",\\n                                \\\"called\\\",\\n                                \\\"character\\\",\\n                                \\\"code\\\",\\n                                \\\"codebase\\\",\\n                                \\\"codebases\\\",\\n                                \\\"codesense\\\",\\n                                \\\"components\\\",\\n                                \\\"creating\\\",\\n                                \\\"defined\\\",\\n                                \\\"designed\\\",\\n                                \\\"detailing\\\",\\n                                \\\"directory\\\",\\n                                \\\"employing\\\",\\n                                \\\"environment\\\",\\n                                \\\"expected\\\",\\n                                \\\"extraction\\\",\\n                                \\\"functionalities\\\",\\n                                \\\"game\\\",\\n                                \\\"generating\\\",\\n                                \\\"generation\\\",\\n                                \\\"include\\\",\\n                                \\\"instructions\\\",\\n                                \\\"involve\\\",\\n                                \\\"keyword\\\",\\n                                \\\"lists\\\",\\n                                \\\"methods\\\",\\n                                \\\"model\\\",\\n                                \\\"modeling\\\",\\n                                \\\"object\\\",\\n                                \\\"outputs\\\",\\n                                \\\"performing\\\",\\n                                \\\"project\\\",\\n                                \\\"provide\\\",\\n                                \\\"provided\\\",\\n                                \\\"representations\\\",\\n                                \\\"representing\\\",\\n                                \\\"scripts\\\",\\n                                \\\"setting\\\",\\n                                \\\"structure\\\",\\n                                \\\"structured\\\",\\n                                \\\"structures\\\",\\n                                \\\"summaries\\\",\\n                                \\\"tasks\\\",\\n                                \\\"text\\\",\\n                                \\\"using\\\",\\n                                \\\"utilities\\\",\\n                                \\\"working\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"This code is structured as a JSON object representing a project directory called \\\\\\\"codesense,\\\\\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"Hidden\\\",\\n                                \\\"JSON\\\",\\n                                \\\"Python\\\",\\n                                \\\"called\\\",\\n                                \\\"calls\\\",\\n                                \\\"children\\\",\\n                                \\\"code\\\",\\n                                \\\"codebase.json\\\",\\n                                \\\"contents\\\",\\n                                \\\"converts\\\",\\n                                \\\"create\\\",\\n                                \\\"create_folder_structure_json\\\",\\n                                \\\"creating\\\",\\n                                \\\"designed\\\",\\n                                \\\"dictionary\\\",\\n                                \\\"directories\\\",\\n                                \\\"directory\\\",\\n                                \\\"file\\\",\\n                                \\\"file_path\\\",\\n                                \\\"file_to_string\\\",\\n                                \\\"files\\\",\\n                                \\\"folder\\\",\\n                                \\\"folders\\\",\\n                                \\\"found\\\",\\n                                \\\"function\\\",\\n                                \\\"given\\\",\\n                                \\\"ignored\\\",\\n                                \\\"including\\\",\\n                                \\\"indentation\\\",\\n                                \\\"list\\\",\\n                                \\\"named\\\",\\n                                \\\"names\\\",\\n                                \\\"navigates\\\",\\n                                \\\"nested\\\",\\n                                \\\"object\\\",\\n                                \\\"output\\\",\\n                                \\\"path\\\",\\n                                \\\"prints\\\",\\n                                \\\"read\\\",\\n                                \\\"reads\\\",\\n                                \\\"representation\\\",\\n                                \\\"representing\\\",\\n                                \\\"returns\\\",\\n                                \\\"saves\\\",\\n                                \\\"script\\\",\\n                                \\\"specifies\\\",\\n                                \\\"starting\\\",\\n                                \\\"string\\\",\\n                                \\\"structure\\\",\\n                                \\\"types\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\\\\\"codebase.json\\\\\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"keywords\\\": [],\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"CodeLlama\\\",\\n                                \\\"English\\\",\\n                                \\\"Python\\\",\\n                                \\\"accelerate\\\",\\n                                \\\"code\\\",\\n                                \\\"command\\\",\\n                                \\\"environment\\\",\\n                                \\\"install\\\",\\n                                \\\"installed\\\",\\n                                \\\"instruction\\\",\\n                                \\\"libraries\\\",\\n                                \\\"model\\\",\\n                                \\\"packages\\\",\\n                                \\\"pip\\\",\\n                                \\\"plain\\\",\\n                                \\\"provides\\\",\\n                                \\\"required\\\",\\n                                \\\"run\\\",\\n                                \\\"running\\\",\\n                                \\\"snippet\\\",\\n                                \\\"transformers\\\",\\n                                \\\"working\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"keywords\\\": [\\n                                \\\"Face\\\",\\n                                \\\"First\\\",\\n                                \\\"Hugging\\\",\\n                                \\\"Llama\\\",\\n                                \\\"Python\\\",\\n                                \\\"Transformers\\\",\\n                                \\\"characters\\\",\\n                                \\\"code\\\",\\n                                \\\"continuation\\\",\\n                                \\\"enabled\\\",\\n                                \\\"expected\\\",\\n                                \\\"function\\\",\\n                                \\\"generate\\\",\\n                                \\\"generated\\\",\\n                                \\\"generation\\\",\\n                                \\\"imports\\\",\\n                                \\\"language\\\",\\n                                \\\"length\\\",\\n                                \\\"library\\\",\\n                                \\\"load\\\",\\n                                \\\"model\\\",\\n                                \\\"output\\\",\\n                                \\\"pipeline\\\",\\n                                \\\"prints\\\",\\n                                \\\"prompt\\\",\\n                                \\\"provided\\\",\\n                                \\\"provides\\\",\\n                                \\\"sampling\\\",\\n                                \\\"text\\\",\\n                                \\\"time\\\",\\n                                \\\"transformers\\\",\\n                                \\\"use\\\",\\n                                \\\"uses\\\",\\n                                \\\"written\\\"\\n                            ],\\n                            \\\"annotation\\\": \\\"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\\\\\"Once upon a time\\\\\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                },\n                {\n                    \"name\": \"top_1.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"input_keywords\\\": [\\n        \\\"python\\\",\\n        \\\"function\\\",\\n        \\\"testkeywordextract\\\",\\n        \\\"nltk\\\",\\n        \\\"word2vec\\\",\\n        \\\"extract_keywords\\\"\\n    ],\\n    \\\"results\\\": [\\n        {\\n            \\\"score\\\": 0.8333333333333334,\\n            \\\"matched_keywords\\\": [\\n                \\\"python\\\",\\n                \\\"function\\\",\\n                \\\"extract_keywords\\\",\\n                \\\"word2vec\\\",\\n                \\\"nltk\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"main.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"*\\\",\\n                    \\\"Comparison\\\",\\n                    \\\"Embeddings\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Processing\\\",\\n                    \\\"Python\\\",\\n                    \\\"Similarity\\\",\\n                    \\\"Text\\\",\\n                    \\\"Word\\\",\\n                    \\\"Word2Vec\\\",\\n                    \\\"calculates\\\",\\n                    \\\"code\\\",\\n                    \\\"compare_keywords\\\",\\n                    \\\"compare_words\\\",\\n                    \\\"comparing\\\",\\n                    \\\"computes\\\",\\n                    \\\"console\\\",\\n                    \\\"context\\\",\\n                    \\\"create\\\",\\n                    \\\"embeddings\\\",\\n                    \\\"employs\\\",\\n                    \\\"extract_keywords\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"file\\\",\\n                    \\\"focuses\\\",\\n                    \\\"function\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"keywords.The\\\",\\n                    \\\"language\\\",\\n                    \\\"libraries\\\",\\n                    \\\"library\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"model\\\",\\n                    \\\"modeling.1\\\",\\n                    \\\"output\\\",\\n                    \\\"performs\\\",\\n                    \\\"processes\\\",\\n                    \\\"processing\\\",\\n                    \\\"reads\\\",\\n                    \\\"removes\\\",\\n                    \\\"returned\\\",\\n                    \\\"score\\\",\\n                    \\\"sentences\\\",\\n                    \\\"similarity\\\",\\n                    \\\"tagging\\\",\\n                    \\\"techniques\\\",\\n                    \\\"text\\\",\\n                    \\\"texts\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"uses\\\",\\n                    \\\"using\\\",\\n                    \\\"vector\\\",\\n                    \\\"verbs\\\",\\n                    \\\"word\\\",\\n                    \\\"words\\\",\\n                    \\\"words.3\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n            }\\n        }\\n    ]\\n}\"\n                },\n                {\n                    \"name\": \"top_3.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"input_keywords\\\": [\\n        \\\"python\\\",\\n        \\\"function\\\",\\n        \\\"testkeywordextract\\\",\\n        \\\"nltk\\\",\\n        \\\"word2vec\\\",\\n        \\\"extract_keywords\\\"\\n    ],\\n    \\\"results\\\": [\\n        {\\n            \\\"score\\\": 0.8333333333333334,\\n            \\\"matched_keywords\\\": [\\n                \\\"python\\\",\\n                \\\"function\\\",\\n                \\\"extract_keywords\\\",\\n                \\\"word2vec\\\",\\n                \\\"nltk\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"main.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"*\\\",\\n                    \\\"Comparison\\\",\\n                    \\\"Embeddings\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Processing\\\",\\n                    \\\"Python\\\",\\n                    \\\"Similarity\\\",\\n                    \\\"Text\\\",\\n                    \\\"Word\\\",\\n                    \\\"Word2Vec\\\",\\n                    \\\"calculates\\\",\\n                    \\\"code\\\",\\n                    \\\"compare_keywords\\\",\\n                    \\\"compare_words\\\",\\n                    \\\"comparing\\\",\\n                    \\\"computes\\\",\\n                    \\\"console\\\",\\n                    \\\"context\\\",\\n                    \\\"create\\\",\\n                    \\\"embeddings\\\",\\n                    \\\"employs\\\",\\n                    \\\"extract_keywords\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"file\\\",\\n                    \\\"focuses\\\",\\n                    \\\"function\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"keywords.The\\\",\\n                    \\\"language\\\",\\n                    \\\"libraries\\\",\\n                    \\\"library\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"model\\\",\\n                    \\\"modeling.1\\\",\\n                    \\\"output\\\",\\n                    \\\"performs\\\",\\n                    \\\"processes\\\",\\n                    \\\"processing\\\",\\n                    \\\"reads\\\",\\n                    \\\"removes\\\",\\n                    \\\"returned\\\",\\n                    \\\"score\\\",\\n                    \\\"sentences\\\",\\n                    \\\"similarity\\\",\\n                    \\\"tagging\\\",\\n                    \\\"techniques\\\",\\n                    \\\"text\\\",\\n                    \\\"texts\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"uses\\\",\\n                    \\\"using\\\",\\n                    \\\"vector\\\",\\n                    \\\"verbs\\\",\\n                    \\\"word\\\",\\n                    \\\"words\\\",\\n                    \\\"words.3\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.5,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"testkeywordextract\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"keyword_extract.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"English\\\",\\n                    \\\"Language\\\",\\n                    \\\"Natural\\\",\\n                    \\\"Python\\\",\\n                    \\\"TestKeywordExtract\\\",\\n                    \\\"Toolkit\\\",\\n                    \\\"annotated\\\",\\n                    \\\"based\\\",\\n                    \\\"class\\\",\\n                    \\\"code\\\",\\n                    \\\"contains\\\",\\n                    \\\"create\\\",\\n                    \\\"description\\\",\\n                    \\\"ensuring\\\",\\n                    \\\"expected\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"filtering\\\",\\n                    \\\"filters\\\",\\n                    \\\"identifies\\\",\\n                    \\\"includes\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"method\\\",\\n                    \\\"nltk\\\",\\n                    \\\"nouns\\\",\\n                    \\\"output\\\",\\n                    \\\"pieces\\\",\\n                    \\\"processing\\\",\\n                    \\\"provided\\\",\\n                    \\\"query\\\",\\n                    \\\"running\\\",\\n                    \\\"script\\\",\\n                    \\\"selecting\\\",\\n                    \\\"stopwords\\\",\\n                    \\\"tagging\\\",\\n                    \\\"test\\\",\\n                    \\\"text\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.3333333333333333,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"info.txt\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"Automatic\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Python\\\",\\n                    \\\"RAKE\\\",\\n                    \\\"Rapid\\\",\\n                    \\\"SSL\\\",\\n                    \\\"algorithm\\\",\\n                    \\\"certificate\\\",\\n                    \\\"changing\\\",\\n                    \\\"command\\\",\\n                    \\\"commands\\\",\\n                    \\\"consists\\\",\\n                    \\\"downloading\\\",\\n                    \\\"downloads\\\",\\n                    \\\"environment\\\",\\n                    \\\"error\\\",\\n                    \\\"gensim\\\",\\n                    \\\"install\\\",\\n                    \\\"installing\\\",\\n                    \\\"instructs\\\",\\n                    \\\"involves\\\",\\n                    \\\"issue\\\",\\n                    \\\"language\\\",\\n                    \\\"model\\\",\\n                    \\\"occurs\\\",\\n                    \\\"packages\\\",\\n                    \\\"processing\\\",\\n                    \\\"provided\\\",\\n                    \\\"setting\\\",\\n                    \\\"shell\\\",\\n                    \\\"suggests\\\",\\n                    \\\"text\\\",\\n                    \\\"tokenization\\\",\\n                    \\\"use\\\",\\n                    \\\"version\\\",\\n                    \\\"words\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\\\",\\n                \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n            }\\n        }\\n    ]\\n}\"\n                },\n                {\n                    \"name\": \"top_5.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"input_keywords\\\": [\\n        \\\"python\\\",\\n        \\\"function\\\",\\n        \\\"testkeywordextract\\\",\\n        \\\"nltk\\\",\\n        \\\"word2vec\\\",\\n        \\\"extract_keywords\\\"\\n    ],\\n    \\\"results\\\": [\\n        {\\n            \\\"score\\\": 0.8333333333333334,\\n            \\\"matched_keywords\\\": [\\n                \\\"python\\\",\\n                \\\"function\\\",\\n                \\\"extract_keywords\\\",\\n                \\\"word2vec\\\",\\n                \\\"nltk\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"main.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"*\\\",\\n                    \\\"Comparison\\\",\\n                    \\\"Embeddings\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Processing\\\",\\n                    \\\"Python\\\",\\n                    \\\"Similarity\\\",\\n                    \\\"Text\\\",\\n                    \\\"Word\\\",\\n                    \\\"Word2Vec\\\",\\n                    \\\"calculates\\\",\\n                    \\\"code\\\",\\n                    \\\"compare_keywords\\\",\\n                    \\\"compare_words\\\",\\n                    \\\"comparing\\\",\\n                    \\\"computes\\\",\\n                    \\\"console\\\",\\n                    \\\"context\\\",\\n                    \\\"create\\\",\\n                    \\\"embeddings\\\",\\n                    \\\"employs\\\",\\n                    \\\"extract_keywords\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"file\\\",\\n                    \\\"focuses\\\",\\n                    \\\"function\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"keywords.The\\\",\\n                    \\\"language\\\",\\n                    \\\"libraries\\\",\\n                    \\\"library\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"model\\\",\\n                    \\\"modeling.1\\\",\\n                    \\\"output\\\",\\n                    \\\"performs\\\",\\n                    \\\"processes\\\",\\n                    \\\"processing\\\",\\n                    \\\"reads\\\",\\n                    \\\"removes\\\",\\n                    \\\"returned\\\",\\n                    \\\"score\\\",\\n                    \\\"sentences\\\",\\n                    \\\"similarity\\\",\\n                    \\\"tagging\\\",\\n                    \\\"techniques\\\",\\n                    \\\"text\\\",\\n                    \\\"texts\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"uses\\\",\\n                    \\\"using\\\",\\n                    \\\"vector\\\",\\n                    \\\"verbs\\\",\\n                    \\\"word\\\",\\n                    \\\"words\\\",\\n                    \\\"words.3\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\\\n\\\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\\\n   \\\\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\\\n\\\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\\\n\\\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.5,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"testkeywordextract\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"keyword_extract.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"English\\\",\\n                    \\\"Language\\\",\\n                    \\\"Natural\\\",\\n                    \\\"Python\\\",\\n                    \\\"TestKeywordExtract\\\",\\n                    \\\"Toolkit\\\",\\n                    \\\"annotated\\\",\\n                    \\\"based\\\",\\n                    \\\"class\\\",\\n                    \\\"code\\\",\\n                    \\\"contains\\\",\\n                    \\\"create\\\",\\n                    \\\"description\\\",\\n                    \\\"ensuring\\\",\\n                    \\\"expected\\\",\\n                    \\\"extracted\\\",\\n                    \\\"extraction\\\",\\n                    \\\"extracts\\\",\\n                    \\\"filtering\\\",\\n                    \\\"filters\\\",\\n                    \\\"identifies\\\",\\n                    \\\"includes\\\",\\n                    \\\"input\\\",\\n                    \\\"keyword\\\",\\n                    \\\"keywords\\\",\\n                    \\\"list\\\",\\n                    \\\"lists\\\",\\n                    \\\"method\\\",\\n                    \\\"nltk\\\",\\n                    \\\"nouns\\\",\\n                    \\\"output\\\",\\n                    \\\"pieces\\\",\\n                    \\\"processing\\\",\\n                    \\\"provided\\\",\\n                    \\\"query\\\",\\n                    \\\"running\\\",\\n                    \\\"script\\\",\\n                    \\\"selecting\\\",\\n                    \\\"stopwords\\\",\\n                    \\\"tagging\\\",\\n                    \\\"test\\\",\\n                    \\\"text\\\",\\n                    \\\"tokenizes\\\",\\n                    \\\"written\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\\\",\\n                \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.3333333333333333,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"info.txt\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"Automatic\\\",\\n                    \\\"Extraction\\\",\\n                    \\\"Gensim\\\",\\n                    \\\"Keyword\\\",\\n                    \\\"NLP\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Python\\\",\\n                    \\\"RAKE\\\",\\n                    \\\"Rapid\\\",\\n                    \\\"SSL\\\",\\n                    \\\"algorithm\\\",\\n                    \\\"certificate\\\",\\n                    \\\"changing\\\",\\n                    \\\"command\\\",\\n                    \\\"commands\\\",\\n                    \\\"consists\\\",\\n                    \\\"downloading\\\",\\n                    \\\"downloads\\\",\\n                    \\\"environment\\\",\\n                    \\\"error\\\",\\n                    \\\"gensim\\\",\\n                    \\\"install\\\",\\n                    \\\"installing\\\",\\n                    \\\"instructs\\\",\\n                    \\\"involves\\\",\\n                    \\\"issue\\\",\\n                    \\\"language\\\",\\n                    \\\"model\\\",\\n                    \\\"occurs\\\",\\n                    \\\"packages\\\",\\n                    \\\"processing\\\",\\n                    \\\"provided\\\",\\n                    \\\"setting\\\",\\n                    \\\"shell\\\",\\n                    \\\"suggests\\\",\\n                    \\\"text\\\",\\n                    \\\"tokenization\\\",\\n                    \\\"use\\\",\\n                    \\\"version\\\",\\n                    \\\"words\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\\\",\\n                \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.3333333333333333,\\n            \\\"matched_keywords\\\": [\\n                \\\"nltk\\\",\\n                \\\"python\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"codebase.json\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"Additionally\\\",\\n                    \\\"Face\\\",\\n                    \\\"Hugging\\\",\\n                    \\\"JSON\\\",\\n                    \\\"NLTK\\\",\\n                    \\\"Python\\\",\\n                    \\\"README\\\",\\n                    \\\"Transformers\\\",\\n                    \\\"annotation\\\",\\n                    \\\"annotations\\\",\\n                    \\\"attributes\\\",\\n                    \\\"called\\\",\\n                    \\\"character\\\",\\n                    \\\"code\\\",\\n                    \\\"codebase\\\",\\n                    \\\"codebases\\\",\\n                    \\\"codesense\\\",\\n                    \\\"components\\\",\\n                    \\\"creating\\\",\\n                    \\\"defined\\\",\\n                    \\\"designed\\\",\\n                    \\\"detailing\\\",\\n                    \\\"directory\\\",\\n                    \\\"employing\\\",\\n                    \\\"environment\\\",\\n                    \\\"expected\\\",\\n                    \\\"extraction\\\",\\n                    \\\"functionalities\\\",\\n                    \\\"game\\\",\\n                    \\\"generating\\\",\\n                    \\\"generation\\\",\\n                    \\\"include\\\",\\n                    \\\"instructions\\\",\\n                    \\\"involve\\\",\\n                    \\\"keyword\\\",\\n                    \\\"lists\\\",\\n                    \\\"methods\\\",\\n                    \\\"model\\\",\\n                    \\\"modeling\\\",\\n                    \\\"object\\\",\\n                    \\\"outputs\\\",\\n                    \\\"performing\\\",\\n                    \\\"project\\\",\\n                    \\\"provide\\\",\\n                    \\\"provided\\\",\\n                    \\\"representations\\\",\\n                    \\\"representing\\\",\\n                    \\\"scripts\\\",\\n                    \\\"setting\\\",\\n                    \\\"structure\\\",\\n                    \\\"structured\\\",\\n                    \\\"structures\\\",\\n                    \\\"summaries\\\",\\n                    \\\"tasks\\\",\\n                    \\\"text\\\",\\n                    \\\"using\\\",\\n                    \\\"utilities\\\",\\n                    \\\"working\\\"\\n                ],\\n                \\\"annotation\\\": \\\"This code is structured as a JSON object representing a project directory called \\\\\\\"codesense,\\\\\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\\\",\\n                \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nfrom nltk.tokenize import word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\n\\\\\\\\n\\\\\\\\n'''\\\\\\\\nCreate a class to extract keywords from text\\\\\\\\n- input:\\\\\\\\n    - sample text as a string\\\\\\\\n-output: \\\\\\\\n    - list of keywords\\\\\\\\n'''\\\\\\\\n\\\\\\\\n\\\\\\\\nclass KeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.keywords = []\\\\\\\\n        # common english stopwords\\\\\\\\n        self.stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n\\\\\\\\n    def extract(self, text):\\\\\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\\\\\n        # identify keywords with part of speech tagging\\\\\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n        # keep only nouns, verbs\\\\\\\\n        for word, pos in pos_tags:\\\\\\\\n            if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n                self.keywords.append(word)\\\\\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\\\\\n        return self.keywords\\\\\\\\n\\\\\\\\n\\\\\\\\nclass TestKeywordExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = KeywordExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Keyword Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_query(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of user query...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from query: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n    def test_extract_keywords_from_annotation(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n            \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(text)\\\\\\\\n        print(f\\\\\\\\\\\\\\\"Keywords from annotation: {output}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        assert type(output) == list\\\\\\\\n\\\\\\\\n\\\\\\\\nif __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\\n    testKeywordExtract = TestKeywordExtract()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\\\\\n\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to model a character in a video game\\\\\\\\n- initialize the class with three parameters\\\\\\\\n    - Health\\\\\\\\n    - Damage\\\\\\\\n    - Speed\\\\\\\\n\\\\\\\\n- define a mathod to double the speed of the character\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass Character:\\\\\\\\n    def __init__(self, health, damage, speed):\\\\\\\\n        self.health = health\\\\\\\\n        self.damage = damage\\\\\\\\n        self.speed = speed\\\\\\\\n    \\\\\\\\n    def double_speed(self):\\\\\\\\n            self.speed *= 2\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nwarrior = Character(100, 50, 10)\\\\\\\\nninja = Character(80, 40, 40)\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\nwarrior.double_speed()\\\\\\\\n\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\\\\\\\\\")\\\\\\\\n  \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                    \\\\\\\"content\\\\\\\": \\\\\\\"'''\\\\\\\\nCreate a class to extract a model of a codebase as a tree\\\\\\\\n- input: local directory path as a string\\\\\\\\n- output: \\\\\\\\n    - json file containing tree structure of directory\\\\\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\\\\\n'''\\\\\\\\n\\\\\\\\nclass CodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.model\\\\\\\\n    \\\\\\\\n    def extract(self, path):\\\\\\\\n        return self.model\\\\\\\\n\\\\\\\\nclass TestCodebaseExtract:\\\\\\\\n    def __init__(self):\\\\\\\\n        self.extractor = CodebaseExtract()\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing Codebase Extractor...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n    \\\\\\\\n    def test_extract_codebase(self):\\\\\\\\n        print(\\\\\\\\\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n        path = \\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\\\\\\\\\"\\\\\\\\n        output = self.extractor.extract(path)\\\\\\\\n        assert type(output) == json\\\\\\\\n        \\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n            \\\\\\\"content\\\\\\\": \\\\\\\"# Project Codesense\\\\\\\\n\\\\\\\\n## Breakdown\\\\\\\\n\\\\\\\\n### 1. CodeBase Tree Extraction\\\\\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\\\\\n### 2. Call Graph Extraction\\\\\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\\\\\n### 3. Annotation Generation\\\\\\\\n    - for a fucntion defined in code generate a text summarization\\\\\\\\n### 4. Annotation Aggregation\\\\\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\\\\\n### 5. Keyword Extraction\\\\\\\\n    - from the aggregated annotation report extract a list of keywords\\\\\\\\n    - from a usery query extract a list of keywords\\\\\\\\n### 6. Tree Traversal\\\\\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\\\\\n### 7. Question Answering\\\\\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"install RAKE\\\\\\\\n`pip3 install --user rake-nltk`\\\\\\\\n\\\\\\\\ninstall supporting nltk packages\\\\\\\\n`python3 -c \\\\\\\\\\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\\\\\\\\\"`\\\\\\\\n\\\\\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\\\\\n\\\\\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\\\\\n\\\\\\\\nto use word2vec install gensim library\\\\\\\\n`pip3 install gensim`\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import nltk\\\\\\\\nimport gensim.downloader\\\\\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\\\\\nfrom nltk.corpus import stopwords\\\\\\\\nimport warnings\\\\\\\\n\\\\\\\\ninput_text1 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\ninput_text2 = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\n\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n#######################extract keywords#######################\\\\\\\\n\\\\\\\\n#download necessary resources\\\\\\\\n# nltk.download('averaged_perceptron_tagger')\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"punkt\\\\\\\\\\\\\\\")\\\\\\\\n# nltk.download(\\\\\\\\\\\\\\\"stopwords\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\ndef extract_keywords(text):\\\\\\\\n    #tokenize the text into words\\\\\\\\n    tokens = word_tokenize(text)\\\\\\\\n    #define a set of common English stopwords\\\\\\\\n    stop_words = set(stopwords.words(\\\\\\\\\\\\\\\"english\\\\\\\\\\\\\\\"))\\\\\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\\\\\n    keywords = []\\\\\\\\n    #identify keywords using part-of-speech tagging\\\\\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\\\\\n    #keep only nouns, proper nouns, and verbs\\\\\\\\n    for word, pos in pos_tags:\\\\\\\\n        if pos.startswith(\\\\\\\\\\\\\\\"NN\\\\\\\\\\\\\\\") or pos.startswith(\\\\\\\\\\\\\\\"VB\\\\\\\\\\\\\\\"):\\\\\\\\n            keywords.append(word)\\\\\\\\n    unique_keywords = list(set(keywords))\\\\\\\\n    return unique_keywords\\\\\\\\n\\\\\\\\n# print(extract_keywords(input_text1))\\\\\\\\n\\\\\\\\n#######################compute the similarity between keywords#######################\\\\\\\\n\\\\\\\\nwarnings.filterwarnings(action='ignore')\\\\\\\\n#  Reads \\\\\\\\u2018context.txt\\\\\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\\\\\nsample = open(\\\\\\\\\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\\\\\\\\\")\\\\\\\\ns = sample.read()\\\\\\\\n# Replaces escape character with space\\\\\\\\nf = s.replace(\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\")\\\\\\\\ndata = []\\\\\\\\n# iterate through each sentence in the file\\\\\\\\nfor i in sent_tokenize(f):\\\\\\\\n    temp = []\\\\\\\\n    # tokenize the sentence into words\\\\\\\\n    for j in word_tokenize(i):\\\\\\\\n        temp.append(j.lower())\\\\\\\\n    data.append(temp)\\\\\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\\\\\n                                vector_size=100, window=5, sg=1)\\\\\\\\n\\\\\\\\ndef compare_words(w1, w2):\\\\\\\\n    if w1 == w2:\\\\\\\\n        return 1\\\\\\\\n    if w1 in model.wv and w2 in model.wv:\\\\\\\\n        return model.wv.similarity(w1, w2)\\\\\\\\n    else:\\\\\\\\n        return 0\\\\\\\\n\\\\\\\\ndef compare_keywords(l1, l2):\\\\\\\\n    output = 0\\\\\\\\n    for word1 in l1:\\\\\\\\n        word1 = word1.lower()\\\\\\\\n        for word2 in l2:\\\\\\\\n            output += compare_words(word1, word2.lower())\\\\\\\\n    return output\\\\\\\\n\\\\\\\\nlist1 = extract_keywords(input_text1)\\\\\\\\nlist2 = extract_keywords(input_text2)\\\\\\\\nprint(compare_keywords(list1, list2))\\\\\\\\n\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"{\\\\\\\\n    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codesense\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"template.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extract.py\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"README.md\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n        },\\\\\\\\n        {\\\\\\\\n            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"extras\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n            \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"keyword_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"context.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase_extraction\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                },\\\\\\\\n                {\\\\\\\\n                    \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"annotation_generation\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"folder\\\\\\\\\\\\\\\",\\\\\\\\n                    \\\\\\\\\\\\\\\"children\\\\\\\\\\\\\\\": [\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"info.txt\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        },\\\\\\\\n                        {\\\\\\\\n                            \\\\\\\\\\\\\\\"name\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"main.py\\\\\\\\\\\\\\\",\\\\\\\\n                            \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"file\\\\\\\\\\\\\\\"\\\\\\\\n                        }\\\\\\\\n                    ]\\\\\\\\n                }\\\\\\\\n            ]\\\\\\\\n        }\\\\\\\\n    ]\\\\\\\\n}\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"import os\\\\\\\\nimport json\\\\\\\\n\\\\\\\\ndef create_folder_structure_json(path):\\\\\\\\n    # Initialize the result dictionary with folder\\\\\\\\n    # name, type, and an empty list for children\\\\\\\\n    result = {'name': os.path.basename(path),\\\\\\\\n              'type': 'folder', 'children': []}\\\\\\\\n    \\\\\\\\n    # Check if the path is a directory\\\\\\\\n    if not os.path.isdir(path):\\\\\\\\n        return result\\\\\\\\n    \\\\\\\\n    # Iterate over the entries in the directory\\\\\\\\n    for entry in os.listdir(path):\\\\\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\\\\\n            # Create the full path for current entry\\\\\\\\n            entry_path = os.path.join(path, entry)\\\\\\\\n            \\\\\\\\n            #if the entry is a directory, recursively call the function\\\\\\\\n            if os.path.isdir(entry_path):\\\\\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\\\\\n            # if the entry is a file, create a dictionary with name and type\\\\\\\\n            else:\\\\\\\\n                try:\\\\\\\\n                    content = file_to_string(entry_path)\\\\\\\\n                except OSError:\\\\\\\\n                    content = \\\\\\\\\\\\\\\"n/a\\\\\\\\\\\\\\\"\\\\\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\\\\\n    return result\\\\\\\\n\\\\\\\\ndef file_to_string(file_path):\\\\\\\\n    with open(file_path, 'r') as file:\\\\\\\\n        file_content = file.read()\\\\\\\\n    file.close()\\\\\\\\n    return file_content\\\\\\\\n# Specify the path to the folder you want to create the JSON for\\\\\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\\\\\n\\\\\\\\n# Call the function to create the JSON representation\\\\\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\\\\\n\\\\\\\\n# Convert the dictionary to a JSON string with indentation\\\\\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\\\\\n\\\\\\\\n# Print the JSON representation of the folder structure\\\\\\\\nprint(folder_json_str)\\\\\\\\n\\\\\\\\n# Save as a JSON file\\\\\\\\nsave_file = open(\\\\\\\\\\\\\\\"codebase.json\\\\\\\\\\\\\\\", 'w')\\\\\\\\njson.dump(folder_json, save_file, indent=4)\\\\\\\\nsave_file.close()\\\\\\\\n\\\\\\\\n\\\\\\\\n    \\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"to run codellama model install transformers\\\\\\\\n`pip install transformers accelerate`\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\",\\\\n                            \\\\\\\"content\\\\\\\": \\\\\\\"from transformers import pipeline\\\\\\\\n\\\\\\\\n# Load Llama 3 model from Hugging Face\\\\\\\\nllama3_model = pipeline(\\\\\\\\\\\\\\\"text-generation\\\\\\\\\\\\\\\", model=\\\\\\\\\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n# Generate text using the Llama 3 model\\\\\\\\nprompt = \\\\\\\\\\\\\\\"Once upon a time\\\\\\\\\\\\\\\"\\\\\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\\\\\n\\\\\\\\n# Print the generated text\\\\\\\\nprint(generated_text[0]['generated_text'])\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n            }\\n        },\\n        {\\n            \\\"score\\\": 0.3333333333333333,\\n            \\\"matched_keywords\\\": [\\n                \\\"python\\\",\\n                \\\"function\\\"\\n            ],\\n            \\\"node\\\": {\\n                \\\"name\\\": \\\"main.py\\\",\\n                \\\"type\\\": \\\"file\\\",\\n                \\\"keywords\\\": [\\n                    \\\"Hidden\\\",\\n                    \\\"JSON\\\",\\n                    \\\"Python\\\",\\n                    \\\"called\\\",\\n                    \\\"calls\\\",\\n                    \\\"children\\\",\\n                    \\\"code\\\",\\n                    \\\"codebase.json\\\",\\n                    \\\"contents\\\",\\n                    \\\"converts\\\",\\n                    \\\"create\\\",\\n                    \\\"create_folder_structure_json\\\",\\n                    \\\"creating\\\",\\n                    \\\"designed\\\",\\n                    \\\"dictionary\\\",\\n                    \\\"directories\\\",\\n                    \\\"directory\\\",\\n                    \\\"file\\\",\\n                    \\\"file_path\\\",\\n                    \\\"file_to_string\\\",\\n                    \\\"files\\\",\\n                    \\\"folder\\\",\\n                    \\\"folders\\\",\\n                    \\\"found\\\",\\n                    \\\"function\\\",\\n                    \\\"given\\\",\\n                    \\\"ignored\\\",\\n                    \\\"including\\\",\\n                    \\\"indentation\\\",\\n                    \\\"list\\\",\\n                    \\\"named\\\",\\n                    \\\"names\\\",\\n                    \\\"navigates\\\",\\n                    \\\"nested\\\",\\n                    \\\"object\\\",\\n                    \\\"output\\\",\\n                    \\\"path\\\",\\n                    \\\"prints\\\",\\n                    \\\"read\\\",\\n                    \\\"reads\\\",\\n                    \\\"representation\\\",\\n                    \\\"representing\\\",\\n                    \\\"returns\\\",\\n                    \\\"saves\\\",\\n                    \\\"script\\\",\\n                    \\\"specifies\\\",\\n                    \\\"starting\\\",\\n                    \\\"string\\\",\\n                    \\\"structure\\\",\\n                    \\\"types\\\"\\n                ],\\n                \\\"annotation\\\": \\\"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\\\\\"codebase.json\\\\\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\\\",\\n                \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                # save file content as string\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n            }\\n        }\\n    ]\\n}\"\n                },\n                {\n                    \"name\": \"tree_traverse.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import sys\\n\\nsys.path.insert(0, \\\"..\\\")\\nfrom utilities.utility import obj_to_json, json_to_obj\\n\\n'''\\nCreate a class to find the most relevant node in the codebase model given some keywords\\n- input:\\n    - list of keywords\\n    - codebase model object\\n- output:\\n    - object of top nodes containing: file_name, annotation, content, and matching_keywords\\n'''\\n\\n\\nclass TraverseCodebase:\\n    def __init__(self, model_obj):\\n        self.model = model_obj\\n        self.top_nodes_with_score = []\\n        self.result_file_name = \\\"result\\\"\\n\\n    def compute_score(self, target_keywords, input_keywords):\\n        score = 0\\n        if not input_keywords or not target_keywords:  # handle empty list\\n            return score\\n        # input keywords should already be lowered, so only lower target_keywords\\n        target_keywords = self.convert_keywords_to_lowercase(target_keywords)\\n        for word in input_keywords:\\n            if word in target_keywords:\\n                score += 1\\n        return score/len(input_keywords)\\n\\n    def get_matched_keywords(self, target_keywords, input_keywords):\\n        target_keywords_lowered = self.convert_keywords_to_lowercase(\\n            target_keywords)\\n        input_set = set(input_keywords)\\n        target_set = set(target_keywords_lowered)\\n        # find intersection\\n        common_elems = input_set.intersection(target_set)\\n        return list(common_elems)\\n\\n    def get_top_nodes(self, input_keywords, n):\\n        # we need to reset the top_nodes list to empty so that multiple calls of this method don't append to it\\n        self.top_nodes_with_score = []\\n        # lower input keywords to compute score properly\\n        input_keywords_lowered = self.convert_keywords_to_lowercase(\\n            input_keywords)\\n        # recursively populate top_nodes list\\n        self._get_top_nodes(self.model, input_keywords_lowered)\\n        # after traversal sort top_nodes list by score in descending order\\n        self.top_nodes_with_score.sort(key=lambda x: x[0], reverse=True)  \\n        # return result\\n        return self.build_result(n, input_keywords_lowered)\\n\\n    def _get_top_nodes(self, model, input_keywords):\\n        if model[\\\"type\\\"] == \\\"file\\\":\\n            # get matching keyword score\\n            score = self.compute_score(model[\\\"keywords\\\"], input_keywords)\\n            self.top_nodes_with_score.append((score, model))\\n            return model\\n        else:\\n            for child in model[\\\"children\\\"]:\\n                self._get_top_nodes(child, input_keywords)\\n\\n    def build_result(self, n, input_keywords):\\n        result = {\\\"input_keywords\\\": input_keywords, \\\"results\\\": []}\\n        for entry in self.top_nodes_with_score:\\n            score = entry[0]\\n            node = entry[1]\\n            # add matched keywords attribute\\n            matched_keywords = self.get_matched_keywords(\\n                node[\\\"keywords\\\"], input_keywords)\\n            entry = {'score': score, 'matched_keywords': matched_keywords, 'node': node}\\n            result[\\\"results\\\"].append(entry)\\n        result[\\\"results\\\"] = result[\\\"results\\\"][:n]\\n        return result\\n\\n    def convert_keywords_to_lowercase(self, keywords):\\n        return [word.lower() for word in keywords]\\n\\n\\nclass TestTraverseCodebase:\\n    def __init__(self):\\n        self.test_model = json_to_obj(\\\"test_codebase.json\\\")\\n        self.traverser = TraverseCodebase(self.test_model)\\n\\n    def test_save_top_1_nodes(self):\\n        print(f\\\"Testing Traverse Codebase to save top 1 nodes\\\")\\n        input_keywords = [\\\"Python\\\", \\\"function\\\", \\\"TestKeywordExtract\\\",\\n                          \\\"NLTK\\\", \\\"Word2Vec\\\", \\\"extract_keywords\\\"]\\n        updated_model = self.traverser.get_top_nodes(input_keywords, 1)\\n        obj_to_json(\\\"./\\\", \\\"top_1\\\", updated_model)\\n        assert type(updated_model) == dict\\n\\n    def test_save_top_3_nodes(self):\\n        print(f\\\"Testing Traverse Codebase to save top 3 nodes\\\")\\n        input_keywords = [\\\"Python\\\", \\\"function\\\", \\\"TestKeywordExtract\\\",\\n                          \\\"NLTK\\\", \\\"Word2Vec\\\", \\\"extract_keywords\\\"]\\n        updated_model = self.traverser.get_top_nodes(input_keywords, 3)\\n        obj_to_json(\\\"./\\\", \\\"top_3\\\", updated_model)\\n        assert type(updated_model) == dict\\n\\n    def test_save_top_5_nodes(self):\\n        print(f\\\"Testing Traverse Codebase to save top 5 nodes\\\")\\n        input_keywords = [\\\"Python\\\", \\\"function\\\", \\\"TestKeywordExtract\\\",\\n                          \\\"NLTK\\\", \\\"Word2Vec\\\", \\\"extract_keywords\\\"]\\n        updated_model = self.traverser.get_top_nodes(input_keywords, 5)\\n        obj_to_json(\\\"./\\\", \\\"top_5\\\", updated_model)\\n        assert type(updated_model) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testTraverseCodebase = TestTraverseCodebase()\\n    testTraverseCodebase.test_save_top_1_nodes()\\n    testTraverseCodebase.test_save_top_3_nodes()\\n    testTraverseCodebase.test_save_top_5_nodes()\\n\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"utilities\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"data\\\": \\\"test\\\"\\n}\"\n                },\n                {\n                    \"name\": \"test2.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"{\\n    \\\"data\\\": \\\"test2\\\"\\n}\"\n                },\n                {\n                    \"name\": \"utility.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import json\\nimport os\\n\\n'''\\nCreate a utilty methods that can be used across multiple classes\\n'''\\n\\n\\ndef obj_to_json(output_path, filename, obj):\\n    # makesure filename doesn't have .json extension\\n    # makesure output_path has trailing backslash\\n    output_file_path = os.path.join(output_path, f\\\"{filename}.json\\\")\\n    save_file = open(output_file_path, 'w')\\n    json.dump(obj, save_file, indent=4)\\n    save_file.close()\\n    print(f\\\"json file saved: {output_file_path}\\\")\\n    # return output file path for debugging purposes\\n    return output_file_path\\n\\n\\ndef json_to_obj(json_file_path):\\n    d = {}\\n    with open(json_file_path) as json_data:\\n        d = json.load(json_data)\\n    return d\\n\\n\\ndef file_to_string(file_path):  # save file content as string\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n\\n\\nclass TestUtility:\\n    def __init__(self):\\n        self.test_json_file = \\\"test.json\\\"\\n\\n    def test_json_to_obj(self):\\n        test_obj = json_to_obj(self.test_json_file)\\n        assert test_obj[\\\"data\\\"] == \\\"test\\\"\\n\\n    def test_obj_to_json(self):\\n        # load object & modify it\\n        test_obj = json_to_obj(self.test_json_file)\\n        test_obj[\\\"data\\\"] = \\\"test2\\\"\\n        # write object\\n        obj_to_json(\\\"./\\\", \\\"test2\\\", test_obj)\\n        # verify if object was written correctly\\n        assert json_to_obj(\\\"test2.json\\\")[\\\"data\\\"] == \\\"test2\\\"\\n\\n    def test_file_to_string(self):\\n        test_str = file_to_string(self.test_json_file)\\n        # print(test_str)\\n        expected_str = '''{\\n    \\\"data\\\": \\\"test\\\"\\n}'''\\n        assert test_str == expected_str\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testUtility = TestUtility()\\n    testUtility.test_json_to_obj()\\n    testUtility.test_obj_to_json()\\n    testUtility.test_file_to_string()\\n\"\n                }\n            ]\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "keyword_extract",
            "path": "codesense/keyword_extract",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "keyword_extract.py",
                    "path": "codesense/keyword_extract/keyword_extract.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n\n'''\nCreate a class to extract keywords from text\n- input:\n    - sample text as a string\n-output: \n    - list of keywords\n'''\n\n\nclass KeywordExtract:\n    def __init__(self):\n        self.keywords = []\n        # common english stopwords\n        self.stop_words = set(stopwords.words(\"english\"))\n\n    def extract(self, text):\n        tokens = word_tokenize(text)  # tokenize text\n        filtered_tokens = [word for word in tokens if word.lower(\n        ) not in self.stop_words]  # filter out stopwords\n        # identify keywords with part of speech tagging\n        pos_tags = nltk.pos_tag(filtered_tokens)\n        # keep only nouns, verbs\n        for word, pos in pos_tags:\n            if pos.startswith(\"NN\") or pos.startswith(\"VB\"):\n                self.keywords.append(word)\n        self.keywords = list(set(self.keywords))  # remove duplicates\n        return self.keywords\n\n\nclass TestKeywordExtract:\n    def __init__(self):\n        self.extractor = KeywordExtract()\n        print(\"Testing Keyword Extractor...\\n\")\n\n    def test_extract_keywords_from_query(self):\n        print(\"Testing keywword extraction of user query...\\n\")\n        text = \"I want to modify the maxProfit function to have an initial maxP value of 10\"\n        output = self.extractor.extract(text)\n        print(f\"Keywords from query: {output}\\n\")\n        assert type(output) == list\n\n    def test_extract_keywords_from_annotation(self):\n        print(\"Testing keywword extraction of code annotation...\\n\")\n        text = \"\"\"\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\n\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\n            \"\"\"\n        output = self.extractor.extract(text)\n        print(f\"Keywords from annotation: {output}\\n\")\n        assert type(output) == list\n\n\nif __name__ == \"__main__\":\n    testKeywordExtract = TestKeywordExtract()\n    testKeywordExtract.test_extract_keywords_from_query()\n    testKeywordExtract.test_extract_keywords_from_annotation()\n"
                }
            ]
        },
        {
            "name": "multithreading",
            "path": "codesense/multithreading",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "batch_model.py",
                    "path": "codesense/multithreading/batch_model.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import threading\nfrom queue import Queue\nimport sys\n\nsys.path.insert(0, \"..\")\nfrom app import App\nfrom utilities.utility import obj_to_json, json_to_obj\n'''\ndefine a class that can accept a list of codebases and model them\n- use multithreading\n- input: list of strings\n- output: list of modeled codebase objects\n'''\n\nclass BatchModel:\n    def __init__(self, codebases: list[(str, dict)]):\n        self.num_threads = len(codebases)\n        self.q = Queue(maxsize=0)\n        self.input_codebases = codebases\n        self.result = []\n    \n    def model_codebase(self, q, output_list)-> dict:\n        while True:\n            entry = q.get()\n            codebase = entry[0]\n            ignores = entry[1]\n            model = App().model_code_base(codebase, ignores)\n            output_list.append(model)\n            q.task_done()\n    \n    def run(self):\n        for i in range(self.num_threads):\n            worker = threading.Thread(target=self.model_codebase, daemon=True, args=(self.q, self.result))\n            worker.start()\n        \n        for entry in self.input_codebases:\n            self.q.put(entry)\n        self.q.join()\n        return self.result\n\nclass TestBatchModel:\n    def __init__(self):\n        self.test_codebases_list = [\n            (\"https://github.com/TravHaran/rust-calculator\", {\"ignore\" : []}),\n            (\"https://github.com/sachinl0har/Basic-Calc\", {\"ignore\" : [\"README.md\"]})\n        ]\n        \n    def test_batch(self):\n        batchModel =BatchModel(self.test_codebases_list)\n        output = {\"results\": []}\n        output[\"results\"] = batchModel.run()\n        obj_to_json(\"../out\", \"batch_model_codebase\", output)\n            \n\nif __name__ == \"__main__\":\n    testBatchModel = TestBatchModel()\n    testBatchModel.test_batch()\n        \n            \n            \n            "
                }
            ]
        },
        {
            "name": "out",
            "path": "codesense/out",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "batch_model_codebase.json",
                    "path": "codesense/out/batch_model_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"results\": [\n        {\n            \"name\": \"\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"README.md\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"# Basic-Calc-in-multiple-languages \\n\\n### Features:\\n1. Addition + - Addition of N Numbers.\\n2. Subtraction - - Subtraction of N Numbers.\\n3. Multipication * - Multipication of Two Numbers.\\n4. Division / - Division of two Numbers.\\n5. Modulus % - Modulus of Two Numbers.\\n6. Hypotenuse - Find Any Triangle's Hypotenuse.\\n7. Power of a Number - Find any Number Power using Base and Exponenent.\\n8. Random Number - Generate any Random Number.\\n9. Converter - Celcius to Fahrenheit and Fahrenheit to Celcius Converter.\\n10. Armstrong Number - You can Find any number is the sum of cubes of each digit is equal to the number itself or Not i.e., Armstrong Number.\\n11. Reverse a Number - Reverse any Number.\\n12. Circle - To Find Radius, Diameter, Circumference and Area.\\n13. Rectangle - To Find Area, Diagonal, Perimeter, Length and Width.\\n14. Tables - You can Get any Table of any Number.\\n15. Matrices - Addition, Subtraction, Multipication of two Matrices and Transpose of a Matrix.\\n\\nMore Features will be uploaded soon.\\n\\n\\nMore Language Programs will be uploaded soon\\n\"\n                },\n                {\n                    \"name\": \"calc.java\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"generation\",\n                        \"calculation\",\n                        \"Based\",\n                        \"sum\",\n                        \"division\",\n                        \"structure\",\n                        \"temperature\",\n                        \"addition\",\n                        \"reversal\",\n                        \"message\",\n                        \"output\",\n                        \"conversion\",\n                        \"selected\",\n                        \"Visiting\",\n                        \"exits\",\n                        \"user\",\n                        \"Thank\",\n                        \"numbers\",\n                        \"multiplication\",\n                        \"implements\",\n                        \"uses\",\n                        \"calculator\",\n                        \"computation\",\n                        \"option\",\n                        \"program\",\n                        \"input\",\n                        \"displays\",\n                        \"Armstrong\",\n                        \"check\",\n                        \"operations\",\n                        \"number\",\n                        \"subtraction\",\n                        \"performs\",\n                        \"entering\",\n                        \"chosen\",\n                        \"matrix\",\n                        \"offer\",\n                        \"operation\",\n                        \"result\",\n                        \"Java\",\n                        \"selecting\",\n                        \"hypotenuse\",\n                        \"example\",\n                        \"exponentiation\",\n                        \"code\",\n                        \"calculations\"\n                    ],\n                    \"annotation\": \"This Java code implements a multi-functional calculator program. The program uses a while loop and switch-case structure to repeatedly offer a menu of 16 operations, such as addition, subtraction, multiplication, division, modulus calculation, computation of hypotenuse, exponentiation, random number generation, temperature conversion, Armstrong number check, number reversal, and various geometric and matrix calculations. Based on user input, it performs the selected operation and displays the result. For example, selecting \\\"1\\\" and entering numbers will output their sum. The program exits gracefully with a \\\"Thank You For Visiting\\\" message when option \\\"16\\\" is chosen.\",\n                    \"content\": \"import java.util.*;\\n\\npublic class calc {\\n    public static void main(String[] args){\\n        double a, b, c = 0, e = 0, x, y, z;\\n        int d, numS;\\n        Scanner calc = new Scanner(System.in);\\n        System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n        while(e == 0){\\n            System.out.println(\\\"\\\\n1. Addition + \\\\n2. Subtraction - \\\\n3. Multipication * \\\\n4. Division / \\\\n5. Modulus % \\\\n6. Hypotenuse \\\\n7. Power of a Number \\\\n8. Random Number \\\\n9. Converter \\\\n10. Armstrong Number \\\\n11. Reverse a Number \\\\n12. Circle \\\\n13. Rectangle \\\\n14. Tables \\\\n15. Matrices \\\\n16. Exit \\\\nEnter Your Choice: \\\\n\\\");\\n        d = calc.nextInt();\\n        switch(d){\\n            case 1:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");  \\n                System.out.println(\\\"\\\\nAddition\\\\n\\\");\\n                System.out.println(\\\"Enter How many Numbers you want to SUM: \\\\n\\\");\\n                numS = calc.nextInt();\\n                System.out.println(\\\"Enter the \\\" + numS + \\\" Numbers: \\\\n\\\");\\n                for(int i = 1; i <= numS; i++){\\n                    System.out.println(\\\"Enter Number \\\" + (i) + \\\" : \\\\n\\\");\\n                    a = calc.nextDouble();\\n                    c += a;\\n                }\\n                System.out.println(\\\"\\\\nThe Sum of \\\" + numS + \\\" Numbers is \\\" + c );\\n                break;\\n            case 2:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");  \\n                System.out.println(\\\"\\\\nSubtraction\\\\n\\\");\\n                System.out.println(\\\"Enter How many Numbers you want to Substract: \\\\n\\\");\\n                numS = calc.nextInt();\\n                System.out.println(\\\"Enter Number 1: \\\\n\\\");\\n                c = calc.nextDouble();\\n                System.out.println(\\\"Enter the \\\" + numS + \\\" Numbers: \\\\n\\\");\\n                for(int i = 2; i <= numS; i++){\\n                    System.out.println(\\\"Enter Number \\\" + (i) + \\\" : \\\\n\\\");\\n                    a = calc.nextDouble();\\n                    c -= a;\\n                }\\n                System.out.println(\\\"\\\\nThe Difference of \\\" + numS + \\\" Numbers is \\\" + c );\\n                break;\\n            case 3:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");  \\n                System.out.println(\\\"\\\\nMultipication\\\\n\\\");\\n                System.out.println(\\\"Enter First Number: \\\\n\\\");\\n                a = calc.nextDouble();\\n                System.out.println(\\\"Enter Second Number: \\\\n\\\");\\n                b = calc.nextDouble();\\n                c = a * b;\\n                System.out.println(\\\"\\\\nThe Combination of \\\" + a + \\\" and \\\" + b + \\\" is \\\" + c );\\n                break;\\n            case 4:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");  \\n                System.out.println(\\\"\\\\nDivision\\\\n\\\");\\n                System.out.println(\\\"Enter First Number: \\\\n\\\");\\n                a = calc.nextDouble();\\n                System.out.println(\\\"Enter Second Number: \\\\n\\\");\\n                b = calc.nextDouble();\\n                c = a / b;\\n                System.out.println(\\\"\\\\nThe Divison of \\\" + a + \\\" and \\\" + b + \\\" is \\\" + c );\\n                break;\\n            case 5:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");  \\n                System.out.println(\\\"\\\\nModulus\\\\n\\\");\\n                System.out.println(\\\"Enter First Number: \\\\n\\\");\\n                a = calc.nextDouble();\\n                System.out.println(\\\"Enter Second Number: \\\\n\\\");\\n                b = calc.nextDouble();\\n                c = a % b;\\n                System.out.println(\\\"\\\\nThe Modulus of \\\" + a + \\\" and \\\" + b + \\\" is \\\" + c );\\n                break;\\n            case 6:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                System.out.println(\\\"Enter side x: \\\");\\n                x  = calc.nextDouble();\\n                System.out.println(\\\"\\\\nEnter side y: \\\");\\n                y = calc.nextDouble();\\n                z = Math.sqrt((x * x) + (y * y));\\n                System.out.println(\\\"The Hypotenuse is \\\" + z);\\n                break;\\n            case 7:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                Scanner pow = new Scanner(System.in);\\n                int base, exp, oexp, result = 1;\\n                System.out.println(\\\"Enter Base Number: \\\");\\n                base = pow.nextInt();\\n                System.out.println(\\\"Enter an Exponent: \\\");\\n                exp = pow.nextInt();\\n                oexp = exp;\\n                while(exp != 0){\\n                    result *= base;\\n                    --exp;\\n                }\\n                System.out.println(\\\"The Power of \\\" + base + \\\" Raised to \\\" + oexp + \\\" is \\\" + result);\\n                break;\\n            case 8:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                int min, max;\\n                Scanner randN = new Scanner(System.in);\\n                System.out.println(\\\"Enter the min number: \\\");\\n                min  = randN.nextInt();\\n                System.out.println(\\\"Enter the max number: \\\");\\n                max = randN.nextInt();\\n                int randNumber = (int)(Math.random() * (max - min + 1) + min);\\n                System.out.println(\\\"Random Number Between \\\" + min + \\\" to \\\" + max + \\\" is \\\" + randNumber);\\n                break;\\n            case 9:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                double f, cel; \\n                int option;\\n                Scanner con = new Scanner(System.in);\\n                System.out.println(\\\"1. Celsius to Fahrenheit \\\\n2. Fahrenheit to Celsius \\\\nEnter Your Choice: \\\");\\n                option = con.nextInt();\\n                switch(option){\\n                    case 1:\\n                        System.out.println(\\\"Enter Temperature in Celsius: \\\");\\n                        cel = con.nextDouble();\\n                        f = (cel * 9/5) + 32;\\n                        System.out.println(\\\"The Temperature of \\\" + cel + \\\" Celsius in Fahrenheit is: \\\" + f);\\n                        break;\\n                    case 2:\\n                        System.out.println(\\\"Enter Temperature in Fahrenheit: \\\");\\n                        f = con.nextDouble();\\n                        cel = (f - 32) * 5/9;\\n                        System.out.println(\\\"The Temperature of \\\" + f + \\\" Fahrenheit in Celsius is: \\\" + cel);\\n                        break;\\n                }\\n                break;\\n            case 10:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                int ce = 0, ae, temp, n;\\n                Scanner num = new Scanner(System.in);\\n                System.out.print(\\\"Enter number to be checked: \\\");\\n                n = num.nextInt();\\n                temp = n;\\n                while(n > 0){\\n                    ae = n % 10;\\n                    n = n/10;\\n                    ce = ce+(ae*ae*ae);\\n                }\\n                if(temp == ce){\\n                    System.out.println(temp+\\\" is an Armstrong Number.\\\");\\n                }else{\\n                    System.out.println(temp+\\\" is not an Armstrong Number.\\\");\\n                }\\n                break;\\n            case 11:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                Scanner revNum = new Scanner(System.in);\\n                int rnum, rev = 0, remainder;\\n                System.out.println(\\\"Enter an Integer: \\\");\\n                rnum = revNum.nextInt();\\n                while(rnum != 0){\\n                    remainder = rnum % 10;\\n                    rev = rev * 10 + remainder;\\n                    rnum /= 10;\\n                }\\n                System.out.println(\\\"Reversed Number of \\\" + rnum + \\\" is \\\" + rev);\\n                break;\\n            case 12:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                Scanner circle = new Scanner(System.in);\\n                double radius, circumference, area, diameter;\\n                int choice;\\n                System.out.println(\\\"Circle\\\\n1. Radius \\\\n2. Diameter \\\\n3. Circumference \\\\n4. Area \\\\nEnter Your Choice: \\\");\\n                choice = circle.nextInt();\\n                switch(choice){\\n                    case 1:\\n                        System.out.println(\\\"Enter Circumference of Circle: \\\");\\n                        circumference = circle.nextDouble();\\n                        radius = circumference/(2 * 3.14);\\n                        System.out.println(\\\"Radius of Circumference \\\" + circumference + \\\" = \\\" + radius);\\n                        break;\\n                    case 2:\\n                        System.out.println(\\\"Enter Radius of Circle: \\\");\\n                        radius = circle.nextDouble();\\n                        diameter = 2 * radius;\\n                        System.out.println(\\\"The Diameter of radius \\\" + radius + \\\" Circle = \\\" + diameter);\\n                        break;\\n                    case 3:\\n                        System.out.println(\\\"Enter Radius of Circle: \\\");\\n                        radius = circle.nextDouble();\\n                        circumference = 2 * 3.14 * radius;\\n                        System.out.println(\\\"The Circumference of Radius \\\" + radius + \\\" Circle = \\\" + circumference);\\n                        break;\\n                    case 4:\\n                        System.out.println(\\\"Enter Radius of Circle: \\\");\\n                        radius = circle.nextDouble();\\n                        area = 3.14 * radius * radius;\\n                        System.out.println(\\\"The Area of Radius \\\" + radius + \\\" Circle = \\\" + area);\\n                        break;\\n                }\\n                break;\\n            case 13:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                double length, width, rectangleArea, perimeter, diagonal;\\n                int option1;\\n                Scanner rectangle = new Scanner(System.in);\\n                System.out.println(\\\"Rectangle\\\\n1. Area \\\\n2. Diagonal \\\\n3. Perimeter \\\\n4. Length \\\\n5. Width \\\\nEnter Your Choice: \\\");\\n                option1 = rectangle.nextInt();\\n                switch(option1){\\n                    case 1:\\n                        System.out.println(\\\"Enter Length of the Rectangle: \\\");\\n                        length = rectangle.nextDouble();\\n                        System.out.println(\\\"Enter Width of the Rectangle: \\\");\\n                        width = rectangle.nextDouble();\\n                        rectangleArea = length * width;\\n                        System.out.println(\\\"The Area of the Rectangle \\\" + length + \\\" Length \\\" + width + \\\" Width is \\\" + rectangleArea);\\n                        break;\\n                    case 2:\\n                        System.out.println(\\\"Enter Length of the Rectangle: \\\");\\n                        length = rectangle.nextDouble();\\n                        System.out.println(\\\"Enter Width of the Rectangle: \\\");\\n                        width = rectangle.nextDouble();\\n                        diagonal = Math.sqrt((length * length) + (width * width));\\n                        System.out.println(\\\"The Diagonal of the Rectangle \\\" + length + \\\" Length \\\" + width + \\\" width is \\\" + diagonal);\\n                        break;\\n                    case 3:\\n                        System.out.println(\\\"Enter Length of the Rectangle: \\\");\\n                        length = rectangle.nextDouble();\\n                        System.out.println(\\\"Enter Width of the Rectangle: \\\");\\n                        width = rectangle.nextDouble();\\n                        perimeter = 2 * (length + width);\\n                        System.out.println(\\\"The Perimeter of the Rectangle \\\" + length + \\\" Length \\\" + width + \\\" width is \\\" + perimeter);\\n                        break;\\n                    case 4:\\n                        System.out.println(\\\"Enter Width of the Rectangle: \\\");\\n                        width = rectangle.nextDouble();\\n                        System.out.println(\\\"Enter Perimeter of the Rectangle: \\\");\\n                        perimeter = rectangle.nextDouble();\\n                        length = (perimeter/2) - width;\\n                        System.out.println(\\\"The Length of Rectangle whose Perimeter = \\\" + perimeter + \\\" and Width = \\\" + width + \\\" is \\\" + length);\\n                        break;\\n                    case 5:\\n                        System.out.println(\\\"Enter Length of the Rectangle: \\\");\\n                        length = rectangle.nextDouble();\\n                        System.out.println(\\\"Enter Perimeter of the Rectangle: \\\");\\n                        perimeter = rectangle.nextDouble();\\n                        width = (perimeter/2) - length;\\n                        System.out.println(\\\"The Width of Rectangle whose Perimeter = \\\" + perimeter + \\\" and Length = \\\" + length + \\\" is \\\" + width);\\n                        break;\\n                }\\n                break;\\n            case 14:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                Scanner tables = new Scanner(System.in);\\n                int tNum;\\n                System.out.println(\\\"Enter a Number Whose Table you want to see: \\\");\\n                tNum = tables.nextInt();\\n                System.out.println(\\\"\\\\n\\\" + tNum + \\\" x 1 = \\\" + tNum * 1 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 2 = \\\" + tNum * 2 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 3 = \\\" + tNum * 3 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 4 = \\\" + tNum * 4 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 5 = \\\" + tNum * 5 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 6 = \\\" + tNum * 6 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 7 = \\\" + tNum * 7 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 8 = \\\" + tNum * 8 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 9 = \\\" + tNum * 9 + \\\" \\\\n\\\");\\n                System.out.println(tNum + \\\" x 10 = \\\" + tNum * 10 + \\\" \\\\n\\\");\\n                break;\\n            case 15:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                int rows, rows1, columns, columns1, option2, i, j, k, rSum = 0;\\n                int[][] matrix1 = new int[100][100], matrix2 = new int[100][100], mSum = new int[100][100];\\n                Scanner matrix = new Scanner(System.in);\\n                System.out.println(\\\"1. Addition \\\\n2. Subtraction \\\\n3. Multipication \\\\n4. Transpose \\\\nEnter Your Choice: \\\");\\n                option2 = matrix.nextInt();\\n                switch(option2){\\n                    case 1:\\n                        System.out.println(\\\"Enter Number of Rows: \\\");\\n                        rows = matrix.nextInt();\\n                        System.out.println(\\\"Enter Number of Columns: \\\");\\n                        columns = matrix.nextInt();\\n                        System.out.println(\\\"\\\\nEnter elements of 1st matrix: \\\\n\\\");\\n                        for(i = 0; i < rows; ++i){\\n                            for(j = 0; j < columns; ++j){\\n                            System.out.format(\\\"Enter Element A%d%d: \\\", i + 1, j + 1);\\n                            matrix1[i][j] = matrix.nextInt();\\n                            }\\n                        }\\n                        System.out.println(\\\"Enter element of 2nd matrix: \\\");\\n                        for(i = 0; i < rows; ++i){\\n                            for(j = 0; j < columns; ++j){\\n                                System.out.format(\\\"Enter Element B%d%d: \\\", i + 1, j + 1);\\n                                matrix2[i][j] = matrix.nextInt();\\n                                }\\n                            }\\n                        for(i = 0; i < rows; ++i){\\n                            for(j = 0; j < columns; ++j){\\n                                mSum[i][j] = matrix1[i][j] + matrix2[i][j];\\n                            }\\n                        }\\n                        System.out.println(\\\"\\\\nSum of Two Matrices: \\\\n\\\");\\n                        for(i = 0; i < rows; ++i){\\n                            for(j = 0; j < columns; ++j){\\n                                System.out.format(\\\"%d \\\", mSum[i][j]);\\n                                if(j == columns - 1){\\n                                    System.out.println(\\\"\\\\n\\\");\\n                                }\\n                            }\\n                        }\\n                break;\\n                case 2:\\n                    System.out.println(\\\"Enter Number of Rows: \\\");\\n                    rows = matrix.nextInt();\\n                    System.out.println(\\\"Enter Number of Columns: \\\");\\n                    columns = matrix.nextInt();\\n                    System.out.println(\\\"\\\\nEnter elements of 1st matrix: \\\\n\\\");\\n                    for(i = 0; i < rows; ++i){\\n                        for(j = 0; j < columns; ++j){\\n                            System.out.format(\\\"Enter Element A%d%d: \\\", i + 1, j + 1);\\n                                matrix1[i][j] = matrix.nextInt();\\n                        }\\n                    }\\n                    System.out.println(\\\"Enter element of 2nd matrix: \\\");\\n                    for(i = 0; i < rows; ++i){\\n                        for(j = 0; j < columns; ++j){\\n                            System.out.format(\\\"Enter Element B%d%d: \\\", i + 1, j + 1);\\n                            matrix2[i][j] = matrix.nextInt();\\n                        }\\n                    }\\n                    for(i = 0; i < rows; ++i){\\n                        for(j = 0; j < columns; ++j){\\n                            mSum[i][j] = matrix1[i][j] - matrix2[i][j];\\n                        }\\n                    }\\n                    System.out.println(\\\"\\\\nDifference of Two Matrices: \\\\n\\\");\\n                    for(i = 0; i < rows; ++i){\\n                        for(j = 0; j < columns; ++j){\\n                            System.out.format(\\\"%d \\\", mSum[i][j]);\\n                            if(j == columns - 1){\\n                                System.out.println(\\\"\\\\n\\\");\\n                            }\\n                        }\\n                    }\\n                    break;\\n                case 3:\\n                    System.out.println(\\\"Enter the number of rows of Matrix 1: \\\");\\n                    rows = matrix.nextInt();\\n                    System.out.println(\\\"Enter the number of columns of Matrix 1: \\\");\\n                    columns = matrix.nextInt();\\n                    matrix1 = new int[rows][columns];\\n                    System.out.println(\\\"Enter elements of Matrix 1: \\\");\\n                    for(i = 0; i < rows; ++i){\\n                        for(j = 0; j < columns; ++j){\\n                            System.out.format(\\\"Enter Element A%d%d: \\\", i + 1, j + 1);\\n                                matrix1[i][j] = matrix.nextInt();\\n                        }\\n                    }\\n                    System.out.println(\\\"Enter the number of rows of Matrix 2: \\\");\\n                    rows1 = matrix.nextInt();\\n                    System.out.println(\\\"Enter the number of columns of Matrix 2: \\\");\\n                    columns1 = matrix.nextInt();\\n                    if(columns != rows1){\\n                        System.out.println(\\\"The Matrices cant be multiplied with each other as Number of columns of Matrix 1 id not equal to Number of rows of Matrix 2.\\\");\\n                    }else{\\n                        int[][] second = new int[rows1][columns1];\\n                        int[][] multiply = new int[rows][columns1];\\n                    System.out.println(\\\"Enter Elements of Matrix 2: \\\");\\n                    for(i = 0; i < rows1; ++i){\\n                        for(j = 0; j < columns1; ++j){\\n                            System.out.format(\\\"Enter Element A%d%d: \\\", i + 1, j + 1);\\n                                second[i][j] = matrix.nextInt();\\n                        }\\n                    }\\n                    for(i = 0; i < rows; i++){\\n                        for(j = 0; j < columns; j++){\\n                            for(k = 0; k < rows1; k++){\\n                                rSum = rSum + matrix1[i][k] * second[k][j];\\n                            }\\n                            multiply[i][j] = rSum;\\n                            rSum = 0;\\n                        }\\n                    }\\n                    System.out.println(\\\"Product of the Matrices: \\\");\\n                    for(i = 0; i < rows; i++){\\n                        for(j = 0;j < columns1; j++){\\n                            System.out.format(\\\"%d\\\\t\\\", multiply[i][j]);\\n                            if(j == columns1 - 1){\\n                            System.out.print(\\\"\\\\n\\\");\\n                            }\\n                        }\\n                    }\\n                    }\\n                    break;\\n                case 4:\\n                    System.out.println(\\\"Enter Number of Rows: \\\");\\n                    rows = matrix.nextInt();\\n                    System.out.println(\\\"Enter Number of Columns: \\\");\\n                    columns = matrix.nextInt();\\n                    System.out.println(\\\"\\\\nEnter elements of the matrix: \\\\n\\\");\\n                    for(i = 0; i < rows; ++i){\\n                        for(j = 0; j < columns; ++j){\\n                            System.out.format(\\\"Enter Element A%d%d: \\\", i + 1, j + 1);\\n                                matrix1[i][j] = matrix.nextInt();\\n                        }\\n                    }\\n                    System.out.println(\\\"The Matrix is: \\\");\\n                    for(i = 0; i < rows; i++){\\n                        for(j = 0; j < columns; j++){\\n                            System.out.print(matrix1[i][j] + \\\" \\\");\\n                        }\\n                        System.out.println();\\n                    }\\n                    int[][] transpose = new int[columns][rows];\\n                    for(i = 0; i < rows; i++){\\n                        for(j = 0; j < columns; j++){\\n                            transpose[i][j] = matrix1[j][i];\\n                        }\\n                    }\\n                    System.out.println(\\\"Printing Matrix After Transpose: \\\");\\n                    for(i = 0; i < rows; i++){\\n                        for(j = 0; j < columns; j++){\\n                            System.out.print(transpose[i][j] + \\\" \\\");\\n                        }\\n                        System.out.println();\\n                    }\\n                    break;\\n                }\\n                break;\\n            case 16:\\n                System.out.print(\\\"\\\\033[H\\\\033[2J\\\");\\n                e = 1;\\n                System.out.println(\\\"\\\\nThank You For Visiting\\\\n\\\");\\n                System.exit(0);\\n                break;\\n        }\\n        } calc.close();\\n    } \\n}\\n\"\n                },\n                {\n                    \"name\": \"calc.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"calculation\",\n                        \"unit\",\n                        \"system\",\n                        \"division\",\n                        \"printing\",\n                        \"clears\",\n                        \"function\",\n                        \"Python\",\n                        \"area\",\n                        \"actions\",\n                        \"addition\",\n                        \"equations\",\n                        \"components\",\n                        \"converting\",\n                        \"executes\",\n                        \"temperatures\",\n                        \"multiplication\",\n                        \"implements\",\n                        \"uses\",\n                        \"takes\",\n                        \"program\",\n                        \"let\",\n                        \"input\",\n                        \"conversions\",\n                        \"operations\",\n                        \"calculating\",\n                        \"choice\",\n                        \"prompts\",\n                        \"include\",\n                        \"subtraction\",\n                        \"print\",\n                        \"prints\",\n                        \"task\",\n                        \"results\",\n                        \"functions\",\n                        \"based\",\n                        \"triangle\",\n                        \"utility\",\n                        \"menu\",\n                        \"string\",\n                        \"corresponding\",\n                        \"messages\",\n                        \"code\",\n                        \"calculations\",\n                        \"solving\"\n                    ],\n                    \"annotation\": \"This Python code implements a command-line calculator and utility program with multiple functions such as arithmetic operations, area calculation of a triangle, solving quadratic equations, unit conversions, and a simple string print. It uses a menu system to let the user choose a task, then executes the corresponding function based on their input. Key components include a main menu which clears the screen and prompts the user for their choice, and various functions for different operations like addition, subtraction, multiplication, division, calculating the area of a triangle, solving quadratic equations, converting temperatures, and printing messages. Each function clears the screen, takes user input, performs calculations or actions, and prints results.\",\n                    \"content\": \"from os import system\\nfrom math import sqrt\\n\\ndef main():\\n    system('cls');\\n    print(\\\"1. Addition\\\\n2. Subtraction\\\\n3. Multipication\\\\n4. Division\\\\n5. Area of Triangle\\\\n6. Quadratic Solutions\\\\n7. Converter\\\\n8. Print\\\\n\\\")\\n    option = int(input(\\\"Enter Your Choice: \\\"))\\n    switchers = {\\n        1: addition,\\n        2: substraction,\\n        3: multipication,\\n        4: division,\\n        5: triangle,\\n        6: quadratic,\\n        7: converter,\\n        8: printing\\n    }\\n    switchers.get(option)()\\n\\ndef addition():\\n    system('cls')\\n    a = float(input(\\\"Enter 1st Number: \\\"))\\n    b = float(input(\\\"Enter 2nd Number: \\\"))\\n    c = a + b\\n    print(\\\"\\\\nSum of %0.2f and %0.2f is : %0.2f\\\" % (a, b, c))\\n\\ndef substraction():\\n    system('cls')\\n    a = float(input(\\\"Enter 1st Number: \\\"))\\n    b = float(input(\\\"Enter 2nd Number: \\\"))\\n    c = a - b\\n    print(\\\"\\\\nDifference of %0.2f and %0.2f is : %0.2f\\\" % (a, b, c))\\n\\ndef multipication():\\n    system('cls')\\n    a = float(input(\\\"Enter 1st Number: \\\"))\\n    b = float(input(\\\"Enter 2nd Number: \\\"))\\n    c = a * b\\n    print(\\\"\\\\nMultipication of %0.2f and %0.2f is : %0.2f\\\" % (a, b, c))\\n\\ndef division():\\n    system('cls')\\n    a = float(input(\\\"Enter 1st Number: \\\"))\\n    b = float(input(\\\"Enter 2nd Number: \\\"))\\n    c = a / b\\n    print(\\\"\\\\nDivision of %0.2f and %0.2f is : %0.2f\\\" % (a, b, c))\\n\\ndef triangle():\\n    system('cls')\\n    a = float(input(\\\"Enter 1st Side: \\\"))\\n    b = float(input(\\\"Enter 2st Side: \\\"))\\n    c = float(input(\\\"Enter 3st Side: \\\"))\\n    s = (a + b + c)/2\\n    area = (s*(s-a)*(s-b)*(s-c)) ** 0.5  \\n    print('The area of the triangle is %0.2f' %area)\\n\\ndef quadratic():\\n    system('cls')\\n    print(\\\"Quadratic Function : (a * x^2) + b*x + c\\\")\\n    a = float(input(\\\"Enter Value of a: \\\"))\\n    b = float(input(\\\"Enter Value of b: \\\"))\\n    c = float(input(\\\"Enter Value of c: \\\"))\\n\\n    r = b**2 - 4*a*c;\\n    if r > 0:\\n        roots = 2\\n        x = (((-b) + sqrt(r))/(2*a))\\n        y = (((-b) + sqrt(r))/(2*a))\\n        print(\\\"There are two roots: %f an d %f\\\" % (x, y))\\n    elif r == 0:\\n        roots = 1\\n        x = (-b) / 2*a\\n        print(\\\"There is 1 root: \\\", x)\\n    else:\\n        roots = 0\\n        print(\\\"No Roots, Disctiminant is less than 0.\\\")\\n\\ndef converter():\\n    system('cls')\\n    print(\\\"1. Celsius to Fahrenheit\\\\n2. Fahrenheit to Celsius\\\\n\\\")\\n    option1 = int(input(\\\"Enter Your Choice: \\\"))\\n    switchers = {\\n        1: celsius,\\n        2: fahrenheit\\n    }\\n    switchers.get(option1)()\\n\\ndef celsius():\\n    system('cls')\\n    cel = float(input(\\\"Enter Temperature in Celsius: \\\"))\\n    f = (cel * 9/5) + 32;\\n    print(\\\"The Temperature of %f Celsius in Fahrenheit is : %f\\\" % (cel, f))\\ndef fahrenheit():\\n    system('cls')\\n    f = float(input(\\\"Enter Temperature in Fahrenheit: \\\"))\\n    cel = (f - 32) * 5/9\\n    print(\\\"The Temperature of %f Fahrenheit in Celsius is : %f\\\" % (f, cel))\\n\\ndef printing():\\n    system('cls')\\n    print(\\\"\\\\\\\"Hello World\\\\\\\"\\\")\\n    print('\\\\'Hello World\\\\'')\\n    message = \\\"Hello World (Printed using a variable)\\\"\\n    print(message)\\n\\nmain()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"Cargo.lock\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"used\",\n                        \"dependencies\",\n                        \"ensures\",\n                        \"sources\",\n                        \"Rust\",\n                        \"project\",\n                        \"builds\",\n                        \"including\",\n                        \"built\",\n                        \"generated\",\n                        \"managed\",\n                        \"version\",\n                        \"depending\",\n                        \"Cargo.lock\",\n                        \"checksum\",\n                        \"URL\",\n                        \"consistency\",\n                        \"Cargo\",\n                        \"itertools\",\n                        \"lock\",\n                        \"package\",\n                        \"lists\",\n                        \"packages\",\n                        \"calc\",\n                        \"versions\",\n                        \"manages\",\n                        \"file\",\n                        \"depends\",\n                        \"projects\",\n                        \"manager\",\n                        \"source\",\n                        \"code\",\n                        \"ensure\",\n                        \"snippet\"\n                    ],\n                    \"annotation\": \"The code is a snippet from a `Cargo.lock` file, which is used in Rust projects managed by Cargo, Rust's package manager. This file is automatically generated and manages the dependencies of the project to ensure consistency across different builds. The snippet lists three packages: \\\"calc\\\" (version 0.1.0, depending on \\\"itertools\\\"), \\\"either\\\" (version 1.6.1, including a checksum and source URL), and \\\"itertools\\\" (version 0.10.0, which depends on \\\"either\\\"). This lock file ensures that the exact versions and sources of these dependencies are used whenever the project is built.\",\n                    \"content\": \"# This file is automatically @generated by Cargo.\\n# It is not intended for manual editing.\\n[[package]]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\ndependencies = [\\n \\\"itertools\\\",\\n]\\n\\n[[package]]\\nname = \\\"either\\\"\\nversion = \\\"1.6.1\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\\\"\\n\\n[[package]]\\nname = \\\"itertools\\\"\\nversion = \\\"0.10.0\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"37d572918e350e82412fe766d24b15e6682fb2ed2bbe018280caa810397cb319\\\"\\ndependencies = [\\n \\\"either\\\",\\n]\\n\"\n                },\n                {\n                    \"name\": \"Cargo.toml\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"crate\",\n                        \"used\",\n                        \"dependencies\",\n                        \">\",\n                        \"section\",\n                        \"Rust\",\n                        \"specifies\",\n                        \"project\",\n                        \"<\",\n                        \"Minimal\",\n                        \"including\",\n                        \"written\",\n                        \"Obvious\",\n                        \"version\",\n                        \"manage\",\n                        \"Boisdequin\",\n                        \"TOML\",\n                        \"Cargo\",\n                        \"serves\",\n                        \"configuration\",\n                        \"Tom\",\n                        \"edition\",\n                        \"@\",\n                        \"package\",\n                        \"author\",\n                        \"metadata\",\n                        \"Cargo.toml\",\n                        \"calc\",\n                        \"Henry\",\n                        \"indicates\",\n                        \"Language\",\n                        \"name\",\n                        \"file\",\n                        \"information\",\n                        \"depends\",\n                        \"projects\",\n                        \"]\",\n                        \"code\",\n                        \"helps\",\n                        \"snippet\"\n                    ],\n                    \"annotation\": \"The code snippet is written in TOML (Tom's Obvious, Minimal Language) and serves as a Rust project's Cargo manifest file (`Cargo.toml`). This manifest specifies the project's metadata, including the package name \\\"calc,\\\" version \\\"0.1.0,\\\" and author information (\\\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\\\"). The Rust edition used is \\\"2018.\\\" The `[dependencies]` section indicates that the project depends on the \\\"itertools\\\" crate with version \\\"0.10\\\". This file helps manage dependencies and project configuration for Rust projects.\",\n                    \"content\": \"[package]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\nauthors = [\\\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\\\"]\\nedition = \\\"2018\\\"\\n\\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\\n\\n[dependencies]\\nitertools = \\\"0.10\\\"\\n\"\n                },\n                {\n                    \"name\": \"README.md\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"text\",\n                        \"Rust\",\n                        \"project\",\n                        \"users\",\n                        \"ensuring\",\n                        \"written\",\n                        \"implementation\",\n                        \"Cargo\",\n                        \"cloning\",\n                        \"executing\",\n                        \"calculator\",\n                        \"guides\",\n                        \"navigating\",\n                        \"testing\",\n                        \"run\",\n                        \"repository\",\n                        \"instructed\",\n                        \"provided\",\n                        \"installed\",\n                        \"execution\",\n                        \"setup\",\n                        \"cargo\",\n                        \"provides\",\n                        \"directory\",\n                        \"test\",\n                        \"purposes\"\n                    ],\n                    \"annotation\": \"The provided text describes a simple command-line calculator written in Rust. To run the calculator, it guides users through cloning the repository, ensuring Rust and Cargo are installed, navigating to the project directory, and executing `cargo run` to start the calculator. For testing purposes, users are instructed to run `cargo test`. The text does not specify the internal implementation of the calculator but provides a basic outline for setup and execution.\",\n                    \"content\": \"Simple command-line calculator in Rust.\\n\\n## To Run\\n\\n1. Clone this repository\\n\\n2. Make sure you have Rust and cargo installed\\n\\n3. Cd into the project directory and type `cargo run`\\n\\n4. To test: run `cargo test`\\n\"\n                },\n                {\n                    \"name\": \"src\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"calc.rs\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"differences\",\n                                \"vector\",\n                                \"used\",\n                                \"correctness\",\n                                \"methods\",\n                                \"Rust\",\n                                \"function\",\n                                \"ensuring\",\n                                \":Itertools\",\n                                \"included\",\n                                \"multiplies\",\n                                \"written\",\n                                \"Calc\",\n                                \"test_all_operations\",\n                                \"itertools\",\n                                \"numbers\",\n                                \"div\",\n                                \"reduces\",\n                                \"arrays\",\n                                \"struct\",\n                                \"sequence\",\n                                \"sub\",\n                                \"operations\",\n                                \"assertions\",\n                                \"include\",\n                                \"elements\",\n                                \"Expected\",\n                                \"subtracts\",\n                                \"validate\",\n                                \"mul\",\n                                \"quotients\",\n                                \"provided\",\n                                \"products\",\n                                \"method\",\n                                \"defines\",\n                                \"sums\",\n                                \"code\",\n                                \"test\",\n                                \"library\"\n                            ],\n                            \"annotation\": \"The code is written in Rust and defines a struct `Calc` with methods for basic arithmetic operations on a vector of floating-point numbers. The `add` method sums all elements, `sub` subtracts elements sequentially, `mul` multiplies all elements, and `div` divides elements sequentially. The `itertools::Itertools` library is used for the `fold1` function, which reduces the sequence. A test function `test_all_operations` is also included to validate each method with assertions. Expected outputs include sums, differences, products, and quotients of provided arrays, ensuring the correctness of the arithmetic operations.\",\n                            \"content\": \"use itertools::Itertools;\\nuse std::ops::{Div, Sub};\\n\\npub struct Calc;\\n\\nimpl Calc {\\n    pub fn add(arr: Vec<f64>) -> f64 {\\n        arr.iter().sum::<f64>()\\n    }\\n\\n    pub fn sub(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Sub::sub).unwrap_or(0.0)\\n    }\\n\\n    pub fn mul(arr: Vec<f64>) -> f64 {\\n        arr.iter().product()\\n    }\\n\\n    pub fn div(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Div::div).unwrap_or(0.0)\\n    }\\n}\\n\\n#[test]\\nfn test_all_operations() {\\n    // addition\\n    assert_eq!(Calc::add([2.0, 4.0, 6.0].to_vec()), 12.0);\\n    assert_eq!(Calc::add([-6.0, 5.0, 10.0].to_vec()), 9.0);\\n\\n    // subtraction\\n    assert_eq!(Calc::sub([10.0, 4.0, 6.0].to_vec()), 0.0);\\n    assert_eq!(Calc::sub([100.0, 10.0, 19.0].to_vec()), 71.0);\\n\\n    // multiplication\\n    assert_eq!(Calc::mul([10.0, 10.0, 2.0].to_vec()), 200.0);\\n    assert_eq!(Calc::mul([-3.0, 2.0].to_vec()), -6.0);\\n\\n    // division\\n    assert_eq!(Calc::div([54.0, 2.0, 3.0].to_vec()), 9.0);\\n    assert_eq!(Calc::div([4.0, 2.0, 5.0].to_vec()), 0.4);\\n}\\n\"\n                        },\n                        {\n                            \"name\": \"main.rs\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"methods\",\n                                \"Rust\",\n                                \"calculated\",\n                                \"named\",\n                                \"occurs\",\n                                \"quit\",\n                                \"equations\",\n                                \"written\",\n                                \"output\",\n                                \"develops\",\n                                \"expected\",\n                                \"exits\",\n                                \"operators\",\n                                \"splits\",\n                                \"numbers\",\n                                \"equation\",\n                                \"involves\",\n                                \"parses\",\n                                \"using\",\n                                \"read\",\n                                \"command\",\n                                \"calculator\",\n                                \"q\",\n                                \"program\",\n                                \"input\",\n                                \"matches\",\n                                \"prompts\",\n                                \"printed\",\n                                \"checks\",\n                                \"operands\",\n                                \"provided\",\n                                \"results\",\n                                \"operator\",\n                                \"operation\",\n                                \"based\",\n                                \"result\",\n                                \"applies\",\n                                \"/\",\n                                \"detected\",\n                                \"varies\",\n                                \"corresponding\",\n                                \"module\",\n                                \"code\",\n                                \"*\"\n                            ],\n                            \"annotation\": \"The code is written in Rust and develops a basic command-line calculator. It continuously prompts the user to enter a mathematical equation or \\\"q\\\" to quit. The input is read, and if it matches the quit command, the program exits. Otherwise, the code checks for valid arithmetic operators (`+`, `-`, `*`, `/`). If an operator is detected, it splits the input into operands, parses them to floating-point numbers, and then applies the corresponding arithmetic operation using methods from a module named `Calc`. The result is printed. If the input format is invalid, a panic occurs. The expected output varies based on user input but involves calculated results of provided equations.\",\n                            \"content\": \"mod calc;\\nuse calc::Calc;\\nuse std::io;\\n\\nfn main() {\\n    println!(\\\"Welcome to the a basic calculator built with Rust.\\\");\\n\\n    loop {\\n        println!(\\\"Please enter an equation or \\\\\\\"q\\\\\\\" to quit: \\\");\\n\\n        let mut input = String::new();\\n        io::stdin()\\n            .read_line(&mut input)\\n            .expect(\\\"Failed to read input\\\");\\n\\n        if input.trim() == \\\"q\\\" {\\n            println!(\\\"Thanks for using this program.\\\");\\n            break;\\n        }\\n\\n        let valid_operators = vec![\\\"+\\\", \\\"-\\\", \\\"*\\\", \\\"/\\\"];\\n\\n        for operator in valid_operators {\\n            match input.find(operator) {\\n                Some(_) => {\\n                    let parts: Vec<&str> = input.split(operator).collect();\\n\\n                    if parts.len() < 2 {\\n                        panic!(\\\"Invalid equation.\\\");\\n                    }\\n\\n                    let mut number_array = vec![];\\n                    let mut counter = 0;\\n\\n                    while counter != parts.len() {\\n                        let val: f64 = parts[counter].trim().parse().ok().expect(\\\"Enter a number.\\\");\\n                        number_array.push(val);\\n                        counter += 1;\\n                    }\\n\\n                    match operator {\\n                        \\\"+\\\" => println!(\\\"{}\\\", Calc::add(number_array)),\\n                        \\\"-\\\" => println!(\\\"{}\\\", Calc::sub(number_array)),\\n                        \\\"*\\\" => println!(\\\"{}\\\", Calc::mul(number_array)),\\n                        \\\"/\\\" => println!(\\\"{}\\\", Calc::div(number_array)),\\n                        _ => println!(\\\"Only addition, subtraction, multiplication and division are supported.\\\")\\n                    }\\n                }\\n\\n                None => {\\n                    continue;\\n                }\\n            }\\n        }\\n    }\\n}\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "codebase.json",
                    "path": "codesense/out/codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"Cargo.lock\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"calc\",\n                \"used\",\n                \"written\",\n                \"versions\",\n                \"meant\",\n                \"Rust\",\n                \"itertools\",\n                \"packages\",\n                \"lists\",\n                \"Tom\",\n                \"sources\",\n                \"named\",\n                \"Minimal\",\n                \"lock\",\n                \"included\",\n                \"dependencies\",\n                \"ensure\",\n                \"depends\",\n                \"builds\",\n                \"project\",\n                \"editing\",\n                \"TOML\",\n                \"generated\",\n                \"Obvious\",\n                \"integrity\",\n                \"Language\",\n                \"portion\",\n                \"code\",\n                \"file\",\n                \"Checksums\",\n                \"format\",\n                \"managing\",\n                \"package\",\n                \"Cargo.lock\",\n                \"manager\",\n                \"Cargo\"\n            ],\n            \"annotation\": \"This code is a portion of a `Cargo.lock` file written in TOML (Tom's Obvious, Minimal Language) format, which is used for managing Rust project dependencies. It lists the packages named `calc`, `either`, and `itertools`, along with their respective versions and sources. The `calc` package depends on `itertools`, while the `itertools` package itself depends on `either`. Checksums are included to ensure package integrity. This file is automatically generated by Cargo, Rust's package manager, and isn't meant for manual editing. The purpose is to lock dependencies to specific versions to ensure reproducible builds.\",\n            \"content\": \"# This file is automatically @generated by Cargo.\\n# It is not intended for manual editing.\\n[[package]]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\ndependencies = [\\n \\\"itertools\\\",\\n]\\n\\n[[package]]\\nname = \\\"either\\\"\\nversion = \\\"1.6.1\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\\\"\\n\\n[[package]]\\nname = \\\"itertools\\\"\\nversion = \\\"0.10.0\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"37d572918e350e82412fe766d24b15e6682fb2ed2bbe018280caa810397cb319\\\"\\ndependencies = [\\n \\\"either\\\",\\n]\\n\"\n        },\n        {\n            \"name\": \"Cargo.toml\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"calc\",\n                \"written\",\n                \"Rust\",\n                \"using\",\n                \"itertools\",\n                \"metadata\",\n                \"Tom\",\n                \"named\",\n                \"configuration\",\n                \"specifies\",\n                \"author\",\n                \"Minimal\",\n                \"dependencies\",\n                \"Boisdequin\",\n                \"manage\",\n                \"Henry\",\n                \"section\",\n                \"project\",\n                \"version\",\n                \"TOML\",\n                \"Obvious\",\n                \"crate\",\n                \"indicates\",\n                \"Language\",\n                \"relies\",\n                \"details\",\n                \"helps\",\n                \"code\",\n                \"file\",\n                \"package\",\n                \"edition\"\n            ],\n            \"annotation\": \"This code is a configuration file written in TOML (Tom's Obvious, Minimal Language) for a Rust project. It specifies the package details for a project named \\\"calc\\\" with version \\\"0.1.0\\\". The author is Henry Boisdequin, and the project is using the 2018 edition of Rust. The dependencies section indicates that the project relies on the \\\"itertools\\\" crate, version \\\"0.10\\\". This configuration helps manage the project's metadata and dependencies.\",\n            \"content\": \"[package]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\nauthors = [\\\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\\\"]\\nedition = \\\"2018\\\"\\n\\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\\n\\n[dependencies]\\nitertools = \\\"0.10\\\"\\n\"\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"directory\",\n                \"test\",\n                \"Rust\",\n                \"command\",\n                \"testing\",\n                \"cargo\",\n                \"calculator\",\n                \"clone\",\n                \"describes\",\n                \"operations\",\n                \"ensure\",\n                \"run\",\n                \"project\",\n                \"line\",\n                \"execute\",\n                \"allows\",\n                \"repository\",\n                \"installed\",\n                \"explanation\",\n                \"code\",\n                \"use\",\n                \"implemented\",\n                \"package\",\n                \"program\",\n                \"manager\",\n                \"Cargo\"\n            ],\n            \"annotation\": \"The explanation describes a simple command-line calculator implemented in Rust. To run the program, first, clone the repository and ensure that Rust and its package manager, Cargo, are installed. Then, navigate to the project's directory and use the `cargo run` command to execute the calculator. For testing, you can run `cargo test`. The code likely allows for basic arithmetic operations via the command line.\",\n            \"content\": \"Simple command-line calculator in Rust.\\n\\n## To Run\\n\\n1. Clone this repository\\n\\n2. Make sure you have Rust and cargo installed\\n\\n3. Cd into the project directory and type `cargo run`\\n\\n4. To test: run `cargo test`\\n\"\n        },\n        {\n            \"name\": \"src\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"calc.rs\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"division\",\n                        \"perform\",\n                        \"struct\",\n                        \"written\",\n                        \"fold1\",\n                        \"test\",\n                        \"Rust\",\n                        \"using\",\n                        \"div\",\n                        \"methods\",\n                        \"e.g.\",\n                        \"elements\",\n                        \"results\",\n                        \"mul\",\n                        \"operations\",\n                        \"sub\",\n                        \"assertions\",\n                        \"summing\",\n                        \"vectors\",\n                        \"numbers\",\n                        \"confirm\",\n                        \"expected\",\n                        \"iter\",\n                        \"multiplies\",\n                        \"method\",\n                        \"function\",\n                        \"test_all_operations\",\n                        \"vector\",\n                        \"operation\",\n                        \"code\",\n                        \"]\",\n                        \"functionality\",\n                        \"defines\",\n                        \"runs\",\n                        \"work\",\n                        \"Calc\",\n                        \"verify\",\n                        \"performs\",\n                        \"subtraction\"\n                    ],\n                    \"annotation\": \"The code is written in Rust and defines a `Calc` struct with methods to perform basic arithmetic operations on vectors of floating-point numbers. The `add` method sums all elements, `sub` method performs sequential subtraction, `mul` method multiplies all elements, and `div` method performs sequential division using Rust's `iter` and `fold1` methods. The `#[test]` function `test_all_operations` runs several assertions to verify that these methods work correctly, e.g., summing the vector `[2.0, 4.0, 6.0]` results in `12.0`. The expected outputs of these assertions confirm the correct functionality of each arithmetic operation within the `Calc` struct.\",\n                    \"content\": \"use itertools::Itertools;\\nuse std::ops::{Div, Sub};\\n\\npub struct Calc;\\n\\nimpl Calc {\\n    pub fn add(arr: Vec<f64>) -> f64 {\\n        arr.iter().sum::<f64>()\\n    }\\n\\n    pub fn sub(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Sub::sub).unwrap_or(0.0)\\n    }\\n\\n    pub fn mul(arr: Vec<f64>) -> f64 {\\n        arr.iter().product()\\n    }\\n\\n    pub fn div(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Div::div).unwrap_or(0.0)\\n    }\\n}\\n\\n#[test]\\nfn test_all_operations() {\\n    // addition\\n    assert_eq!(Calc::add([2.0, 4.0, 6.0].to_vec()), 12.0);\\n    assert_eq!(Calc::add([-6.0, 5.0, 10.0].to_vec()), 9.0);\\n\\n    // subtraction\\n    assert_eq!(Calc::sub([10.0, 4.0, 6.0].to_vec()), 0.0);\\n    assert_eq!(Calc::sub([100.0, 10.0, 19.0].to_vec()), 71.0);\\n\\n    // multiplication\\n    assert_eq!(Calc::mul([10.0, 10.0, 2.0].to_vec()), 200.0);\\n    assert_eq!(Calc::mul([-3.0, 2.0].to_vec()), -6.0);\\n\\n    // division\\n    assert_eq!(Calc::div([54.0, 2.0, 3.0].to_vec()), 9.0);\\n    assert_eq!(Calc::div([4.0, 2.0, 5.0].to_vec()), 0.4);\\n}\\n\"\n                },\n                {\n                    \"name\": \"main.rs\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"division\",\n                        \"written\",\n                        \"output\",\n                        \"module\",\n                        \"Rust\",\n                        \"using\",\n                        \"exits\",\n                        \"methods\",\n                        \"q\",\n                        \"calculator\",\n                        \"checking\",\n                        \"performing\",\n                        \"printed\",\n                        \"equation\",\n                        \"addition\",\n                        \"parsing\",\n                        \"entered\",\n                        \"errors\",\n                        \"operations\",\n                        \"called\",\n                        \"parts\",\n                        \"numbers\",\n                        \"ensures\",\n                        \"input\",\n                        \"processes\",\n                        \"expected\",\n                        \"multiplication\",\n                        \"validity\",\n                        \"quit\",\n                        \"message\",\n                        \"prompts\",\n                        \"termination\",\n                        \"result\",\n                        \"splitting\",\n                        \"operation\",\n                        \"enter\",\n                        \"code\",\n                        \"converting\",\n                        \"format\",\n                        \"string\",\n                        \"operator\",\n                        \"implements\",\n                        \"program\",\n                        \"subtraction\"\n                    ],\n                    \"annotation\": \"The code is written in Rust and implements a basic command-line calculator. It continuously prompts the user to enter an equation or \\\"q\\\" to quit. If \\\"q\\\" is entered, the program exits. Otherwise, it processes the input to detect basic arithmetic operations (addition, subtraction, multiplication, division) by splitting the input string around the operator, converting the parts to numbers, and then performing the operation using methods from a module called `Calc`. The result is then printed. The program ensures input validity by checking for correct format and parsing errors. The expected output will be the result of the arithmetic operation entered by the user or a termination message if \\\"q\\\" is entered.\",\n                    \"content\": \"mod calc;\\nuse calc::Calc;\\nuse std::io;\\n\\nfn main() {\\n    println!(\\\"Welcome to the a basic calculator built with Rust.\\\");\\n\\n    loop {\\n        println!(\\\"Please enter an equation or \\\\\\\"q\\\\\\\" to quit: \\\");\\n\\n        let mut input = String::new();\\n        io::stdin()\\n            .read_line(&mut input)\\n            .expect(\\\"Failed to read input\\\");\\n\\n        if input.trim() == \\\"q\\\" {\\n            println!(\\\"Thanks for using this program.\\\");\\n            break;\\n        }\\n\\n        let valid_operators = vec![\\\"+\\\", \\\"-\\\", \\\"*\\\", \\\"/\\\"];\\n\\n        for operator in valid_operators {\\n            match input.find(operator) {\\n                Some(_) => {\\n                    let parts: Vec<&str> = input.split(operator).collect();\\n\\n                    if parts.len() < 2 {\\n                        panic!(\\\"Invalid equation.\\\");\\n                    }\\n\\n                    let mut number_array = vec![];\\n                    let mut counter = 0;\\n\\n                    while counter != parts.len() {\\n                        let val: f64 = parts[counter].trim().parse().ok().expect(\\\"Enter a number.\\\");\\n                        number_array.push(val);\\n                        counter += 1;\\n                    }\\n\\n                    match operator {\\n                        \\\"+\\\" => println!(\\\"{}\\\", Calc::add(number_array)),\\n                        \\\"-\\\" => println!(\\\"{}\\\", Calc::sub(number_array)),\\n                        \\\"*\\\" => println!(\\\"{}\\\", Calc::mul(number_array)),\\n                        \\\"/\\\" => println!(\\\"{}\\\", Calc::div(number_array)),\\n                        _ => println!(\\\"Only addition, subtraction, multiplication and division are supported.\\\")\\n                    }\\n                }\\n\\n                None => {\\n                    continue;\\n                }\\n            }\\n        }\\n    }\\n}\\n\"\n                }\n            ]\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "populate_annotations",
            "path": "codesense/populate_annotations",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "ignore.txt",
                    "path": "codesense/populate_annotations/ignore.txt",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "codesense/keyword_extract\ncodesense/extras/codebase_extraction/codebase.json\ncodesense/README.md\n"
                },
                {
                    "name": "populate_annotations.py",
                    "path": "codesense/populate_annotations/populate_annotations.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import os\nimport sys\n\nsys.path.insert(0, \"..\")\nfrom annotation_generate.annotation_generate import AnnotationGeneration\nfrom utilities.utility import file_to_string, obj_to_json, json_to_obj\n\n'''\nCreate a class to populate the codebase json with annotations\n- input: \n    - codebase model object\n- output:\n    - codebase model object with updated annotation fields\n'''\n\nclass PopulateAnnotations:\n    def __init__(self, model_obj, ignore_paths):\n        #ignore_paths could be a txt file or a json object containing file_paths to ignore\n        self.annotator = AnnotationGeneration()\n        self.model = model_obj\n        self.ignore_list = self.build_ignore_list(ignore_paths)\n    \n    def build_ignore_list(self, ignore_paths):\n        ignore_list = []\n        if type(ignore_paths) == str: # the input is a txt file path as a string\n            # read txt file as string\n            ignore_list = file_to_string(ignore_paths).splitlines()\n        else: # the input is an object, i.e.: {\"ignore\" : [\"path1\", \"path2\", \"path3\"]}\n            ignore_list = ignore_paths[\"ignore\"]\n        return ignore_list\n        \n    \n    def annotate(self, content_str):\n        formated_str = content_str.replace(\"\\n\", \"\") # remove newline characters\n        output = self.annotator.snippet_summary(formated_str) # comment this out to stub API call for testing purposes\n        # output = \"test\"\n        return output\n    \n    def populate_model(self):\n        self._populate(self.model, self.model[\"name\"])\n        return self.model\n        \n    def _populate(self, model, cur_path):\n        if model[\"type\"] == \"file\":\n            if model[\"content\"] not in [\"n/a\", \"\"]:\n                annotation = self.annotate(model[\"content\"])\n                model[\"annotation\"] = annotation \n                return model  \n        else:\n            for child in model[\"children\"]:\n                #build path string of traversal\n                new_path = os.path.join(cur_path, child[\"name\"])\n                if new_path not in self.ignore_list:\n                    self._populate(child, new_path)\n\nclass TestPopulateAnnotations:\n    def __init__(self):\n        self.test_model = json_to_obj(\"test_codebase_original.json\") \n        self.test_ignore_file = \"ignore.txt\"\n        self.populator = PopulateAnnotations(self.test_model, self.test_ignore_file)\n        \n    def test_populate_annotations(self):\n        print(\"Testing annotation population\")\n        updated_model = self.populator.populate_model()\n        obj_to_json(\"./\", \"test\", updated_model)\n        assert type(updated_model) == dict\n    \n        \n\nif __name__ == \"__main__\":\n    testPopulateAnnotations = TestPopulateAnnotations()\n    testPopulateAnnotations.test_populate_annotations()\n\n                \n        \n"
                },
                {
                    "name": "test.json",
                    "path": "codesense/populate_annotations/test.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"test\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"test\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"test\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "test_codebase.json",
                    "path": "codesense/populate_annotations/test_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\"test_codebase.json\\\" representing the directory structure of `self.test_path`.\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"This document outlines a project called \\\"Codesense,\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\n\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\n3. Annotation Generation: Produces text summaries for functions in the code.\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\n\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\"Once upon a time\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "test_codebase_original.json",
                    "path": "codesense/populate_annotations/test_codebase_original.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "populate_keywords",
            "path": "codesense/populate_keywords",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "populate_keywords.py",
                    "path": "codesense/populate_keywords/populate_keywords.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import sys\n\nsys.path.insert(0, \"..\")\nfrom keyword_extract.keyword_extract import KeywordExtract\nfrom utilities.utility import obj_to_json, json_to_obj\n\n'''\nCreate a class to populate the codebase json with keywords\n- input: \n    - codebase model object\n- output:\n    - codebase model object with updated keywords fields\n'''\n\nclass PopulateKeywords:\n    def __init__(self, model_obj):\n        self.model = model_obj\n    \n    def extractKeywords(self, content_str):\n        formated_str = content_str.replace(\"\\n\", \"\") # remove newline characters\n        extractor = KeywordExtract()\n        output = extractor.extract(formated_str)\n        # output = \"test\"\n        return output\n    \n    def populate_model(self):\n        self._populate(self.model)\n        return self.model\n        \n    def _populate(self, model):\n        if model[\"type\"] == \"file\":\n            annotation = model[\"annotation\"]\n            keywords = self.extractKeywords(annotation)\n            model['keywords'] = keywords\n            return model  \n        else:\n            for child in model[\"children\"]:\n                self._populate(child)\n    \n    \nclass TestPopulateKeyWords:\n    def __init__(self):\n        self.test_model = json_to_obj(\"test_codebase_original.json\")\n        self.populator = PopulateKeywords(self.test_model)\n\n    def test_populate_keywords(self):\n        print(\"Testing annotation population\")\n        updated_model = self.populator.populate_model()\n        obj_to_json(\"./\", \"test\", updated_model)\n        assert type(updated_model) == dict\n\nif __name__ == \"__main__\":\n    testPopulateKeyWords = TestPopulateKeyWords()\n    testPopulateKeyWords.test_populate_keywords()\n    "
                },
                {
                    "name": "test.json",
                    "path": "codesense/populate_keywords/test.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"annotated\",\n                        \"pieces\",\n                        \"method\",\n                        \"Python\",\n                        \"keyword\",\n                        \"query\",\n                        \"stopwords\",\n                        \"ensuring\",\n                        \"processing\",\n                        \"extracted\",\n                        \"English\",\n                        \"list\",\n                        \"tokenizes\",\n                        \"output\",\n                        \"keywords\",\n                        \"class\",\n                        \"test\",\n                        \"input\",\n                        \"text\",\n                        \"extraction\",\n                        \"selecting\",\n                        \"running\",\n                        \"Natural\",\n                        \"filters\",\n                        \"provided\",\n                        \"lists\",\n                        \"TestKeywordExtract\",\n                        \"expected\",\n                        \"description\",\n                        \"nouns\",\n                        \"filtering\",\n                        \"Language\",\n                        \"includes\",\n                        \"written\",\n                        \"tagging\",\n                        \"identifies\",\n                        \"based\",\n                        \"contains\",\n                        \"nltk\",\n                        \"script\",\n                        \"Toolkit\",\n                        \"extracts\",\n                        \"code\",\n                        \"create\"\n                    ],\n                    \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"Python\",\n                        \"damage\",\n                        \"Character\",\n                        \"named\",\n                        \"initialized\",\n                        \"health\",\n                        \"character\",\n                        \"updated\",\n                        \"game\",\n                        \"ninja\",\n                        \"output\",\n                        \"class\",\n                        \"speeds\",\n                        \"created\",\n                        \"doubled\",\n                        \"showcase\",\n                        \"attributes\",\n                        \"doubles\",\n                        \"code\",\n                        \"printed\",\n                        \"written\",\n                        \"speed\",\n                        \"using\",\n                        \"models\",\n                        \"double_speed\",\n                        \"parameters\",\n                        \"instances\",\n                        \"includes\",\n                        \"warrior\"\n                    ],\n                    \"annotation\": \"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"string\",\n                        \"directories\",\n                        \"converts\",\n                        \"method\",\n                        \"Expected\",\n                        \"Python\",\n                        \"named\",\n                        \"defines\",\n                        \"writes\",\n                        \"tree\",\n                        \"output\",\n                        \"reads\",\n                        \"class\",\n                        \"test_codebase.json\",\n                        \"model\",\n                        \"stores\",\n                        \"designed\",\n                        \"treating\",\n                        \"files\",\n                        \"traversing\",\n                        \"save_model_json\",\n                        \"leaf\",\n                        \"file_to_string\",\n                        \"provided\",\n                        \"structure\",\n                        \"content\",\n                        \"tests\",\n                        \"generates\",\n                        \"contents\",\n                        \"directory\",\n                        \"given\",\n                        \"JSON\",\n                        \"nodes\",\n                        \"representing\",\n                        \"self.test_path\",\n                        \"model_to_str\",\n                        \"file\",\n                        \"functionality\",\n                        \"code\",\n                        \"create\"\n                    ],\n                    \"annotation\": \"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\"test_codebase.json\\\" representing the directory structure of `self.test_path`.\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"project\",\n                \"involves\",\n                \"showing\",\n                \"Codesense\",\n                \"document\",\n                \"tree\",\n                \"Extraction\",\n                \"Graph\",\n                \"answer\",\n                \"keywords\",\n                \"summaries\",\n                \"Tree\",\n                \"CodeBase\",\n                \"Searches\",\n                \"Produces\",\n                \"call\",\n                \"implementations\",\n                \"report\",\n                \"outlines\",\n                \"Answering\",\n                \"called\",\n                \"codebase.2\",\n                \"Keyword\",\n                \"flows\",\n                \"structure\",\n                \"Traversal\",\n                \"aggregated\",\n                \"target\",\n                \"Creates\",\n                \"Identifies\",\n                \"annotation.7\",\n                \"serves\",\n                \"objectives\",\n                \"Generates\",\n                \"directed\",\n                \"Question\",\n                \"providing\",\n                \"codebase\",\n                \"Compiles\",\n                \"tasks\",\n                \"components\",\n                \"source\",\n                \"based\",\n                \"include\",\n                \"queries.6\",\n                \"nodes\",\n                \"matching\",\n                \"codebases\",\n                \"file.3\",\n                \"Annotation\",\n                \"Generation\",\n                \"representing\",\n                \"annotations\",\n                \"user\",\n                \"returns\",\n                \"related\",\n                \"Uses\",\n                \"Aggregation\",\n                \"functions\",\n                \"code.4\",\n                \"analyze\",\n                \"graph.5\",\n                \"code\",\n                \"Call\",\n                \"function\"\n            ],\n            \"annotation\": \"This document outlines a project called \\\"Codesense,\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\n\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\n3. Annotation Generation: Produces text summaries for functions in the code.\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\n\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Python\",\n                                \"involves\",\n                                \"installing\",\n                                \"packages\",\n                                \"certificate\",\n                                \"processing\",\n                                \"install\",\n                                \"language\",\n                                \"Extraction\",\n                                \"downloads\",\n                                \"tokenization\",\n                                \"SSL\",\n                                \"shell\",\n                                \"text\",\n                                \"Automatic\",\n                                \"instructs\",\n                                \"model\",\n                                \"use\",\n                                \"Gensim\",\n                                \"consists\",\n                                \"issue\",\n                                \"provided\",\n                                \"Keyword\",\n                                \"error\",\n                                \"command\",\n                                \"NLTK\",\n                                \"algorithm\",\n                                \"suggests\",\n                                \"downloading\",\n                                \"changing\",\n                                \"commands\",\n                                \"setting\",\n                                \"environment\",\n                                \"RAKE\",\n                                \"Rapid\",\n                                \"occurs\",\n                                \"version\",\n                                \"NLP\",\n                                \"words\",\n                                \"gensim\"\n                            ],\n                            \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"similarity\",\n                                \"embeddings\",\n                                \"Python\",\n                                \"keyword\",\n                                \"*\",\n                                \"uses\",\n                                \"Text\",\n                                \"processes\",\n                                \"texts\",\n                                \"output\",\n                                \"processing\",\n                                \"extracted\",\n                                \"language\",\n                                \"Extraction\",\n                                \"comparing\",\n                                \"tokenizes\",\n                                \"list\",\n                                \"extraction\",\n                                \"keywords\",\n                                \"Word\",\n                                \"input\",\n                                \"text\",\n                                \"reads\",\n                                \"library\",\n                                \"focuses\",\n                                \"model\",\n                                \"Gensim\",\n                                \"Comparison\",\n                                \"modeling.1\",\n                                \"Embeddings\",\n                                \"sentences\",\n                                \"Keyword\",\n                                \"lists\",\n                                \"performs\",\n                                \"verbs\",\n                                \"compare_keywords\",\n                                \"NLTK\",\n                                \"console\",\n                                \"Word2Vec\",\n                                \"written\",\n                                \"tagging\",\n                                \"create\",\n                                \"using\",\n                                \"removes\",\n                                \"calculates\",\n                                \"score\",\n                                \"libraries\",\n                                \"word\",\n                                \"Processing\",\n                                \"compare_words\",\n                                \"employs\",\n                                \"vector\",\n                                \"extract_keywords\",\n                                \"returned\",\n                                \"Similarity\",\n                                \"words.3\",\n                                \"context\",\n                                \"NLP\",\n                                \"computes\",\n                                \"extracts\",\n                                \"file\",\n                                \"code\",\n                                \"words\",\n                                \"techniques\",\n                                \"function\",\n                                \"keywords.The\"\n                            ],\n                            \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"method\",\n                                \"refers\",\n                                \"list\",\n                                \"class\",\n                                \"price\",\n                                \"profit\",\n                                \"stock\",\n                                \"increase\",\n                                \"captures\",\n                                \"iterates\",\n                                \"end\",\n                                \"sets\",\n                                \"Solution\",\n                                \"description\",\n                                \"element\",\n                                \"calculates\",\n                                \"maxP\",\n                                \"C++\",\n                                \"returns\",\n                                \"prices\",\n                                \"representing\",\n                                \"adds\",\n                                \"maxProfit\",\n                                \"difference\",\n                                \"opportunities\",\n                                \"function\"\n                            ],\n                            \"annotation\": \"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"project\",\n                                \"Transformers\",\n                                \"Python\",\n                                \"keyword\",\n                                \"generating\",\n                                \"Face\",\n                                \"Additionally\",\n                                \"generation\",\n                                \"character\",\n                                \"game\",\n                                \"scripts\",\n                                \"extraction\",\n                                \"summaries\",\n                                \"annotation\",\n                                \"text\",\n                                \"structured\",\n                                \"model\",\n                                \"detailing\",\n                                \"designed\",\n                                \"working\",\n                                \"structures\",\n                                \"utilities\",\n                                \"called\",\n                                \"provided\",\n                                \"structure\",\n                                \"attributes\",\n                                \"provide\",\n                                \"expected\",\n                                \"employing\",\n                                \"lists\",\n                                \"creating\",\n                                \"NLTK\",\n                                \"README\",\n                                \"methods\",\n                                \"codebase\",\n                                \"representations\",\n                                \"using\",\n                                \"codesense\",\n                                \"directory\",\n                                \"components\",\n                                \"tasks\",\n                                \"defined\",\n                                \"include\",\n                                \"JSON\",\n                                \"performing\",\n                                \"object\",\n                                \"codebases\",\n                                \"setting\",\n                                \"environment\",\n                                \"representing\",\n                                \"instructions\",\n                                \"annotations\",\n                                \"modeling\",\n                                \"outputs\",\n                                \"involve\",\n                                \"Hugging\",\n                                \"functionalities\",\n                                \"code\"\n                            ],\n                            \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"directories\",\n                                \"converts\",\n                                \"string\",\n                                \"read\",\n                                \"Python\",\n                                \"codebase.json\",\n                                \"navigates\",\n                                \"folder\",\n                                \"named\",\n                                \"indentation\",\n                                \"nested\",\n                                \"types\",\n                                \"create_folder_structure_json\",\n                                \"list\",\n                                \"ignored\",\n                                \"output\",\n                                \"reads\",\n                                \"folders\",\n                                \"specifies\",\n                                \"prints\",\n                                \"names\",\n                                \"children\",\n                                \"designed\",\n                                \"saves\",\n                                \"files\",\n                                \"file_path\",\n                                \"path\",\n                                \"file_to_string\",\n                                \"called\",\n                                \"structure\",\n                                \"Hidden\",\n                                \"creating\",\n                                \"contents\",\n                                \"create\",\n                                \"calls\",\n                                \"directory\",\n                                \"given\",\n                                \"JSON\",\n                                \"returns\",\n                                \"object\",\n                                \"representing\",\n                                \"starting\",\n                                \"including\",\n                                \"found\",\n                                \"script\",\n                                \"representation\",\n                                \"file\",\n                                \"code\",\n                                \"dictionary\",\n                                \"function\"\n                            ],\n                            \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Python\",\n                                \"packages\",\n                                \"install\",\n                                \"transformers\",\n                                \"CodeLlama\",\n                                \"English\",\n                                \"accelerate\",\n                                \"run\",\n                                \"snippet\",\n                                \"installed\",\n                                \"model\",\n                                \"running\",\n                                \"working\",\n                                \"required\",\n                                \"instruction\",\n                                \"command\",\n                                \"pip\",\n                                \"provides\",\n                                \"libraries\",\n                                \"environment\",\n                                \"plain\",\n                                \"code\"\n                            ],\n                            \"annotation\": \"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Transformers\",\n                                \"Python\",\n                                \"Face\",\n                                \"First\",\n                                \"uses\",\n                                \"time\",\n                                \"generation\",\n                                \"enabled\",\n                                \"language\",\n                                \"transformers\",\n                                \"library\",\n                                \"output\",\n                                \"generated\",\n                                \"prints\",\n                                \"text\",\n                                \"imports\",\n                                \"model\",\n                                \"use\",\n                                \"characters\",\n                                \"prompt\",\n                                \"load\",\n                                \"generate\",\n                                \"provided\",\n                                \"sampling\",\n                                \"expected\",\n                                \"continuation\",\n                                \"pipeline\",\n                                \"provides\",\n                                \"Llama\",\n                                \"written\",\n                                \"length\",\n                                \"Hugging\",\n                                \"code\",\n                                \"function\"\n                            ],\n                            \"annotation\": \"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\"Once upon a time\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "test_codebase_original.json",
                    "path": "codesense/populate_keywords/test_codebase_original.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\"test_codebase.json\\\" representing the directory structure of `self.test_path`.\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [],\n            \"annotation\": \"This document outlines a project called \\\"Codesense,\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\n\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\n3. Annotation Generation: Produces text summaries for functions in the code.\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\n\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [],\n                            \"annotation\": \"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\"Once upon a time\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "question_answering",
            "path": "codesense/question_answering",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "question_answer.py",
                    "path": "codesense/question_answering/question_answer.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "from openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nimport sys\n\nsys.path.insert(0, \"..\")\nfrom annotation_aggregate.annotation_aggregate import AnnotationAggregate\nfrom utilities.utility import json_to_obj\n\n'''\nCreate a class that responds to a user query given context from the codebase\n- input:\n    - traversal result object\n- output:\n    - Response to user query as string\n'''\n\n\nclass QueryAnswer:\n    def __init__(self, traverse_obj):\n        self.res = \"\"\n        self.traversal = traverse_obj\n        aggregator = AnnotationAggregate(self.traversal)\n        self.context = aggregator.aggregate_annotations()\n\n    ## Set the API Key\n    def get_response(self, query):\n        load_dotenv()\n        API_KEY = os.getenv('OPENAI_SECRET_API_KEY')\n        client = OpenAI(api_key=API_KEY)\n\n        #GPT4o REPONSE REQUEST\n        MODEL=\"gpt-4o\"\n\n        completion = client.chat.completions.create(\n        model=MODEL,\n        #Prompt modelling, grounding the model to provide a more concise and clear summary when given a piece of code\n        messages=[\n            {\"role\": \"system\", \"content\": '''\n            You are a developer assistant designed to provide detailed answers and assistance based on contextual explanations of code in a codebase. Your input consists of explanations of code files and their respective file directories within the codebase. Users will provide queries related to the codebase, seeking clarification, assistance, or suggestions. Your task is to utilize the provided context to generate clear and structured responses to the user queries. Your responses should be informative, accurate, and tailored to the specific query. Additionally, you may suggest potential actions or direct the user to relevant code files within the codebase for further reference. Your responses should solely rely on the provided context, avoiding external knowledge or assumptions. Remember to maintain clarity and coherence in your responses, ensuring that users can easily understand and follow your guidance. Make sure to keep your responses as short as possible as well so that the developer can quickly view an answer their question.\n\n            Example:\n\n            Query: How does the event-handling function handle errors during Firestore database operations?\n\n            Context:\n            The code is written in JavaScript, specifically using the async/await syntax to handle asynchronous operations with Firestore, a cloud database from Firebase. It defines an event-handling function `handleCreateEvent` meant to create and save event data into the Firestore database. When a form submission event triggers the function, it first prevents the default behavior with `e.preventDefault()`. The function checks if `isDateRange` is false and, based on this, either adds or updates single or range-dated event documents in the Firestore under the 'events' collection. It also conditionally updates the 'announcements' collection based on the existence of date ranges. After database operations, it resets multiple state variables (title, description, dateTime, etc.) and fetches user data. The function ensures newly created or modified data incorporates the current date and time and user metadata. The expected result includes adding appropriate entries in the Firestore under both 'events' and 'announcements' collections and resetting the form's state. \n            File Directory: NewsFlash/pages/events.js\n            \n\n            Response:\n            The event-handling function `handleCreateEvent` in the file events.js employs error handling mechanisms to manage errors during Firestore database operations. Within the async function, try-catch blocks are utilized to capture and handle any potential errors that may occur during asynchronous database transactions. Specifically, when performing Firestore operations such as adding or updating event documents, the try block encapsulates these operations, allowing for graceful error handling. In the event of an error, the catch block is triggered, enabling the function to handle the error appropriately, which may include logging the error, displaying a user-friendly message, or initiating corrective actions. Additionally, the function may utilize Firebase's error handling features, such as error codes or error objects, to provide more detailed information about the nature of the error and facilitate troubleshooting. Overall, the event-handling function is designed to handle errors robustly, ensuring the reliability and stability of database operations.\n            '''},\n\n            {\"role\": \"user\", \"content\": f'''With that said. The query and context is given below:\n            QUERY: {query}\n            \n            CODEBASE CONTEXT: {self.context}\n            '''}\n        ]\n        )\n        return completion.choices[0].message.content\n\n### TESTING \nclass TestQueryAnswering:\n     def __init__(self):\n        self.test_model = json_to_obj(\"top_5.json\")\n        self.responder = QueryAnswer(self.test_model)\n        print(\"Testing Query Response... \\n\")\n    \n     def test_keyword_extract_explanation(self):\n        query=\"How does keyword extraction work in this project?\"\n        output = self.responder.get_response(query)\n        print(output)\n        assert type(output) == str\n    \n\nif __name__ == \"__main__\":\n    TestQueryAnswering = TestQueryAnswering()\n    TestQueryAnswering.test_keyword_extract_explanation()"
                },
                {
                    "name": "top_5.json",
                    "path": "codesense/question_answering/top_5.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\",\n                \"word2vec\",\n                \"extract_keywords\",\n                \"function\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        },\n        {\n            \"score\": 0.5,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\",\n                \"testkeywordextract\"\n            ],\n            \"node\": {\n                \"name\": \"keyword_extract.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"English\",\n                    \"Language\",\n                    \"Natural\",\n                    \"Python\",\n                    \"TestKeywordExtract\",\n                    \"Toolkit\",\n                    \"annotated\",\n                    \"based\",\n                    \"class\",\n                    \"code\",\n                    \"contains\",\n                    \"create\",\n                    \"description\",\n                    \"ensuring\",\n                    \"expected\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"filtering\",\n                    \"filters\",\n                    \"identifies\",\n                    \"includes\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"list\",\n                    \"lists\",\n                    \"method\",\n                    \"nltk\",\n                    \"nouns\",\n                    \"output\",\n                    \"pieces\",\n                    \"processing\",\n                    \"provided\",\n                    \"query\",\n                    \"running\",\n                    \"script\",\n                    \"selecting\",\n                    \"stopwords\",\n                    \"tagging\",\n                    \"test\",\n                    \"text\",\n                    \"tokenizes\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"info.txt\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Automatic\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"RAKE\",\n                    \"Rapid\",\n                    \"SSL\",\n                    \"algorithm\",\n                    \"certificate\",\n                    \"changing\",\n                    \"command\",\n                    \"commands\",\n                    \"consists\",\n                    \"downloading\",\n                    \"downloads\",\n                    \"environment\",\n                    \"error\",\n                    \"gensim\",\n                    \"install\",\n                    \"installing\",\n                    \"instructs\",\n                    \"involves\",\n                    \"issue\",\n                    \"language\",\n                    \"model\",\n                    \"occurs\",\n                    \"packages\",\n                    \"processing\",\n                    \"provided\",\n                    \"setting\",\n                    \"shell\",\n                    \"suggests\",\n                    \"text\",\n                    \"tokenization\",\n                    \"use\",\n                    \"version\",\n                    \"words\"\n                ],\n                \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"codebase.json\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Additionally\",\n                    \"Face\",\n                    \"Hugging\",\n                    \"JSON\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"README\",\n                    \"Transformers\",\n                    \"annotation\",\n                    \"annotations\",\n                    \"attributes\",\n                    \"called\",\n                    \"character\",\n                    \"code\",\n                    \"codebase\",\n                    \"codebases\",\n                    \"codesense\",\n                    \"components\",\n                    \"creating\",\n                    \"defined\",\n                    \"designed\",\n                    \"detailing\",\n                    \"directory\",\n                    \"employing\",\n                    \"environment\",\n                    \"expected\",\n                    \"extraction\",\n                    \"functionalities\",\n                    \"game\",\n                    \"generating\",\n                    \"generation\",\n                    \"include\",\n                    \"instructions\",\n                    \"involve\",\n                    \"keyword\",\n                    \"lists\",\n                    \"methods\",\n                    \"model\",\n                    \"modeling\",\n                    \"object\",\n                    \"outputs\",\n                    \"performing\",\n                    \"project\",\n                    \"provide\",\n                    \"provided\",\n                    \"representations\",\n                    \"representing\",\n                    \"scripts\",\n                    \"setting\",\n                    \"structure\",\n                    \"structured\",\n                    \"structures\",\n                    \"summaries\",\n                    \"tasks\",\n                    \"text\",\n                    \"using\",\n                    \"utilities\",\n                    \"working\"\n                ],\n                \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"function\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Hidden\",\n                    \"JSON\",\n                    \"Python\",\n                    \"called\",\n                    \"calls\",\n                    \"children\",\n                    \"code\",\n                    \"codebase.json\",\n                    \"contents\",\n                    \"converts\",\n                    \"create\",\n                    \"create_folder_structure_json\",\n                    \"creating\",\n                    \"designed\",\n                    \"dictionary\",\n                    \"directories\",\n                    \"directory\",\n                    \"file\",\n                    \"file_path\",\n                    \"file_to_string\",\n                    \"files\",\n                    \"folder\",\n                    \"folders\",\n                    \"found\",\n                    \"function\",\n                    \"given\",\n                    \"ignored\",\n                    \"including\",\n                    \"indentation\",\n                    \"list\",\n                    \"named\",\n                    \"names\",\n                    \"navigates\",\n                    \"nested\",\n                    \"object\",\n                    \"output\",\n                    \"path\",\n                    \"prints\",\n                    \"read\",\n                    \"reads\",\n                    \"representation\",\n                    \"representing\",\n                    \"returns\",\n                    \"saves\",\n                    \"script\",\n                    \"specifies\",\n                    \"starting\",\n                    \"string\",\n                    \"structure\",\n                    \"types\"\n                ],\n                \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n            }\n        }\n    ]\n}"
                }
            ]
        },
        {
            "name": "template",
            "path": "codesense/template",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "template.py",
                    "path": "codesense/template/template.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "'''\nCreate a class to model a character in a video game\n- initialize the class with three parameters\n    - Health\n    - Damage\n    - Speed\n\n- define a mathod to double the speed of the character\n'''\n\nclass Character:\n    def __init__(self, health, damage, speed):\n        self.health = health\n        self.damage = damage\n        self.speed = speed\n    \n    def double_speed(self):\n            self.speed *= 2\n\n\n\n\nwarrior = Character(100, 50, 10)\nninja = Character(80, 40, 40)\n\nprint(f\"Warrior speed: {warrior.speed}\")\nprint(f\"Ninja speed: {ninja.speed}\")\n\nwarrior.double_speed()\n\nprint(f\"Warrior speed: {warrior.speed}\")\n  "
                }
            ]
        },
        {
            "name": "test",
            "path": "codesense/test",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "integration_test",
                    "path": "codesense/test/integration_test",
                    "type": "folder",
                    "keywords": [],
                    "children": [
                        {
                            "name": "codebase.json",
                            "path": "codesense/test/integration_test/codebase.json",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "{\n    \"name\": \"\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"Cargo.lock\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"correct\",\n                \"provided\",\n                \"code\",\n                \"dependencies\",\n                \"version\",\n                \"checksum\",\n                \"validation\",\n                \"manager\",\n                \"Cargo\",\n                \"crates.io\",\n                \"edited\",\n                \"itertools\",\n                \"generated\",\n                \"projects\",\n                \"file\",\n                \"segment\",\n                \"required\",\n                \"crates\",\n                \"calc\",\n                \"dependency\",\n                \"sourced\",\n                \"project\",\n                \"Cargo.toml\",\n                \"Rust\",\n                \"builds\",\n                \"specifies\",\n                \"lock\",\n                \"ensures\",\n                \"versions\",\n                \"lists\",\n                \"package\",\n                \"packages\",\n                \"used\"\n            ],\n            \"annotation\": \"The provided code is a segment of a `Cargo.toml` lock file used in Rust projects. It lists specific packages (crates) and their dependencies required for the project. The code specifies three packages: \\\"calc\\\" (version 0.1.0) with a dependency on \\\"itertools\\\", \\\"either\\\" (version 1.6.1) sourced from crates.io with a checksum for validation, and \\\"itertools\\\" (version 0.10.0) also from crates.io, with its own dependency on \\\"either\\\". This file is automatically generated by Cargo, Rust's package manager, and shouldn't be manually edited. The code ensures that the project builds with the correct versions of these dependencies.\",\n            \"content\": \"# This file is automatically @generated by Cargo.\\n# It is not intended for manual editing.\\n[[package]]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\ndependencies = [\\n \\\"itertools\\\",\\n]\\n\\n[[package]]\\nname = \\\"either\\\"\\nversion = \\\"1.6.1\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\\\"\\n\\n[[package]]\\nname = \\\"itertools\\\"\\nversion = \\\"0.10.0\\\"\\nsource = \\\"registry+https://github.com/rust-lang/crates.io-index\\\"\\nchecksum = \\\"37d572918e350e82412fe766d24b15e6682fb2ed2bbe018280caa810397cb319\\\"\\ndependencies = [\\n \\\"either\\\",\\n]\\n\"\n        },\n        {\n            \"name\": \"Cargo.toml\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"uses\",\n                \"code\",\n                \"dependencies\",\n                \"version\",\n                \"email\",\n                \"define\",\n                \"referenced\",\n                \"manager\",\n                \"crate\",\n                \"Cargo\",\n                \"GitHub\",\n                \"file\",\n                \"section\",\n                \"calc\",\n                \"dependency\",\n                \"project\",\n                \"Cargo.toml\",\n                \"Rust\",\n                \"compile\",\n                \"specifies\",\n                \"Boisdequin\",\n                \"manage\",\n                \"package\",\n                \"edition\",\n                \"author\",\n                \"given\",\n                \"used\",\n                \"declares\",\n                \"helps\",\n                \"Henry\",\n                \"name\"\n            ],\n            \"annotation\": \"The given code is a snippet from a Rust project's Cargo.toml file, which is used to define the project's metadata and manage its dependencies. This specific section specifies the package name as \\\"calc\\\", the version as \\\"0.1.0\\\", and the author as Henry Boisdequin with an email referenced from GitHub. The project uses the 2018 edition of Rust. In the dependencies section, it declares a dependency on the \\\"itertools\\\" crate, version \\\"0.10\\\". This file helps Cargo, Rust's package manager, to compile, package, and manage the project.\",\n            \"content\": \"[package]\\nname = \\\"calc\\\"\\nversion = \\\"0.1.0\\\"\\nauthors = [\\\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\\\"]\\nedition = \\\"2018\\\"\\n\\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\\n\\n[dependencies]\\nitertools = \\\"0.10\\\"\\n\"\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"contain\",\n                \"navigate\",\n                \"provided\",\n                \"code\",\n                \"perform\",\n                \"input\",\n                \"ensure\",\n                \"execute\",\n                \"based\",\n                \"run\",\n                \"Cargo\",\n                \"operations\",\n                \"calculator\",\n                \"directory\",\n                \"program\",\n                \"command-line\",\n                \"includes\",\n                \"project\",\n                \"installed\",\n                \"Rust\",\n                \"need\",\n                \"test\",\n                \"instructions\",\n                \"implementation\",\n                \"cargo\",\n                \"clone\",\n                \"repository\",\n                \"running\",\n                \"testing\"\n            ],\n            \"annotation\": \"The code you provided doesn't contain the actual implementation of the simple command-line calculator in Rust. However, it includes instructions for running and testing a Rust project. To run the project, you need to clone the repository, ensure Rust and Cargo are installed, navigate to the project directory, and execute `cargo run` to compile and run the program. To test it, you execute `cargo test`. The command-line calculator itself would likely perform basic arithmetic operations based on user input.\",\n            \"content\": \"Simple command-line calculator in Rust.\\n\\n## To Run\\n\\n1. Clone this repository\\n\\n2. Make sure you have Rust and cargo installed\\n\\n3. Cd into the project directory and type `cargo run`\\n\\n4. To test: run `cargo test`\\n\"\n        },\n        {\n            \"name\": \"src\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"calc.rs\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"elements\",\n                        \"test_all_operations\",\n                        \"provided\",\n                        \"using\",\n                        \"code\",\n                        \"provides\",\n                        \"multiplication\",\n                        \"tests\",\n                        \"perform\",\n                        \"subtracts\",\n                        \"struct\",\n                        \"mul\",\n                        \"subtraction\",\n                        \"crate\",\n                        \"written\",\n                        \"divides\",\n                        \"operations\",\n                        \"unit\",\n                        \"verify\",\n                        \"division\",\n                        \"method\",\n                        \"f64\",\n                        \"itertools\",\n                        \"numbers\",\n                        \"includes\",\n                        \"Expected\",\n                        \"vector\",\n                        \"Rust\",\n                        \">\",\n                        \"sub\",\n                        \"defines\",\n                        \"multiplies\",\n                        \"demonstrate\",\n                        \"div\",\n                        \"iterator\",\n                        \"addition\",\n                        \"function\",\n                        \"methods\"\n                    ],\n                    \"annotation\": \"The code is written in Rust and defines a struct `Calc` that provides basic arithmetic operations (addition, subtraction, multiplication, and division) on a vector of floating-point numbers (`Vec<f64>`). The `add` method sums all elements, `sub` method subtracts elements sequentially, `mul` method multiplies all elements, and `div` method divides the elements sequentially, each using iterator methods provided by the `itertools` crate. The `test_all_operations` function includes unit tests to verify the correctness of these methods. Expected outputs demonstrate that the methods perform standard arithmetic operations correctly.\",\n                    \"content\": \"use itertools::Itertools;\\nuse std::ops::{Div, Sub};\\n\\npub struct Calc;\\n\\nimpl Calc {\\n    pub fn add(arr: Vec<f64>) -> f64 {\\n        arr.iter().sum::<f64>()\\n    }\\n\\n    pub fn sub(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Sub::sub).unwrap_or(0.0)\\n    }\\n\\n    pub fn mul(arr: Vec<f64>) -> f64 {\\n        arr.iter().product()\\n    }\\n\\n    pub fn div(arr: Vec<f64>) -> f64 {\\n        arr.iter().map(|&x| x as f64).fold1(Div::div).unwrap_or(0.0)\\n    }\\n}\\n\\n#[test]\\nfn test_all_operations() {\\n    // addition\\n    assert_eq!(Calc::add([2.0, 4.0, 6.0].to_vec()), 12.0);\\n    assert_eq!(Calc::add([-6.0, 5.0, 10.0].to_vec()), 9.0);\\n\\n    // subtraction\\n    assert_eq!(Calc::sub([10.0, 4.0, 6.0].to_vec()), 0.0);\\n    assert_eq!(Calc::sub([100.0, 10.0, 19.0].to_vec()), 71.0);\\n\\n    // multiplication\\n    assert_eq!(Calc::mul([10.0, 10.0, 2.0].to_vec()), 200.0);\\n    assert_eq!(Calc::mul([-3.0, 2.0].to_vec()), -6.0);\\n\\n    // division\\n    assert_eq!(Calc::div([54.0, 2.0, 3.0].to_vec()), 9.0);\\n    assert_eq!(Calc::div([4.0, 2.0, 5.0].to_vec()), 0.4);\\n}\\n\"\n                },\n                {\n                    \"name\": \"main.rs\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"qThanks\",\n                        \"+\",\n                        \"q\",\n                        \"built\",\n                        \"uses\",\n                        \"code\",\n                        \"multiplication\",\n                        \"interaction\",\n                        \"perform\",\n                        \"input\",\n                        \"using\",\n                        \"program.\",\n                        \"operator\",\n                        \"calculation\",\n                        \"example\",\n                        \"result\",\n                        \"prompts\",\n                        \"subtraction\",\n                        \"module\",\n                        \"written\",\n                        \"based\",\n                        \"checks\",\n                        \"user\",\n                        \"division\",\n                        \"calculator\",\n                        \"program\",\n                        \"numbers\",\n                        \"output\",\n                        \"converts\",\n                        \"specified\",\n                        \"Calc\",\n                        \"/\",\n                        \"message\",\n                        \"Rust\",\n                        \"expected\",\n                        \"Rust.Please\",\n                        \"input.Here\",\n                        \"parts\",\n                        \"vary\",\n                        \"quit\",\n                        \"reads\",\n                        \"printed\",\n                        \"equation\",\n                        \"splits\",\n                        \"operators\",\n                        \"addition\",\n                        \"implements\",\n                        \"presence\",\n                        \"inputs\",\n                        \"basic\",\n                        \"enter\",\n                        \"split\",\n                        \"exits\"\n                    ],\n                    \"annotation\": \"The code, written in Rust, implements a simple command-line calculator. It repeatedly prompts the user to input an equation or \\\"q\\\" to quit. The program reads the input, checks for the presence of valid operators (+, -, *, /), and then splits the input based on the operator. It converts the split parts into numbers and uses the Calc module to perform the specified calculation (addition, subtraction, multiplication, division). The result of the calculation is then printed. If the user inputs \\\"q\\\", the program exits with a thank-you message. The expected output will vary based on the user's input.\\n\\nHere's an example interaction:\\n```\\nWelcome to the a basic calculator built with Rust.\\nPlease enter an equation or \\\"q\\\" to quit: \\n2 + 3\\n5.0\\nPlease enter an equation or \\\"q\\\" to quit: \\nq\\nThanks for using this program.\\n```\",\n                    \"content\": \"mod calc;\\nuse calc::Calc;\\nuse std::io;\\n\\nfn main() {\\n    println!(\\\"Welcome to the a basic calculator built with Rust.\\\");\\n\\n    loop {\\n        println!(\\\"Please enter an equation or \\\\\\\"q\\\\\\\" to quit: \\\");\\n\\n        let mut input = String::new();\\n        io::stdin()\\n            .read_line(&mut input)\\n            .expect(\\\"Failed to read input\\\");\\n\\n        if input.trim() == \\\"q\\\" {\\n            println!(\\\"Thanks for using this program.\\\");\\n            break;\\n        }\\n\\n        let valid_operators = vec![\\\"+\\\", \\\"-\\\", \\\"*\\\", \\\"/\\\"];\\n\\n        for operator in valid_operators {\\n            match input.find(operator) {\\n                Some(_) => {\\n                    let parts: Vec<&str> = input.split(operator).collect();\\n\\n                    if parts.len() < 2 {\\n                        panic!(\\\"Invalid equation.\\\");\\n                    }\\n\\n                    let mut number_array = vec![];\\n                    let mut counter = 0;\\n\\n                    while counter != parts.len() {\\n                        let val: f64 = parts[counter].trim().parse().ok().expect(\\\"Enter a number.\\\");\\n                        number_array.push(val);\\n                        counter += 1;\\n                    }\\n\\n                    match operator {\\n                        \\\"+\\\" => println!(\\\"{}\\\", Calc::add(number_array)),\\n                        \\\"-\\\" => println!(\\\"{}\\\", Calc::sub(number_array)),\\n                        \\\"*\\\" => println!(\\\"{}\\\", Calc::mul(number_array)),\\n                        \\\"/\\\" => println!(\\\"{}\\\", Calc::div(number_array)),\\n                        _ => println!(\\\"Only addition, subtraction, multiplication and division are supported.\\\")\\n                    }\\n                }\\n\\n                None => {\\n                    continue;\\n                }\\n            }\\n        }\\n    }\\n}\\n\"\n                }\n            ]\n        }\n    ]\n}"
                        },
                        {
                            "name": "ignore.txt",
                            "path": "codesense/test/integration_test/ignore.txt",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": ""
                        },
                        {
                            "name": "integration.py",
                            "path": "codesense/test/integration_test/integration.py",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "import sys\n\n\nsys.path.insert(0, \"../..\")\nfrom codebase_extract.codebase_extract import CodebaseExtract\nfrom codebase_extract.github_codebase_extract import CodeBaseExtractGithub\nfrom populate_annotations.populate_annotations import PopulateAnnotations\nfrom populate_keywords.populate_keywords import PopulateKeywords\nfrom keyword_extract.keyword_extract import KeywordExtract\nfrom tree_traverse.tree_traverse import TraverseCodebase\nfrom question_answering.question_answer import QueryAnswer\nfrom utilities.utility import obj_to_json\n\n'''\nCreate a class that can run a full integration test of codesense\n- input: Question as a string\n- output: Answer as a string\n'''\n\nclass Integration:\n    def __init__(self, code_base, ignore_paths_file):\n        self.path = code_base\n        self.ignore_paths_file = ignore_paths_file\n        self.code_base_model = {}\n        self.search_result = {}\n    \n    def model_codebase(self):\n        # Extract Codebase\n        if self.path.startswith(\"https://github.com\"):\n            codebase_extractor = CodeBaseExtractGithub(self.path)\n        else:\n            codebase_extractor = CodebaseExtract(self.path)\n        self.code_base_model = codebase_extractor.get_model()\n        # Populate Annotations\n        populate_annotations = PopulateAnnotations(self.code_base_model, self.ignore_paths_file)\n        self.code_base_model = populate_annotations.populate_model()\n        # Populate Keywords\n        populate_keywords = PopulateKeywords(self.code_base_model)\n        self.code_base_model = populate_keywords.populate_model()\n        obj_to_json(\"./\", \"codebase\", self.code_base_model)\n\n    def query(self, question):\n        # Extract Keywords\n        extract_keywords = KeywordExtract()\n        query_keywords = extract_keywords.extract(question)\n        # Traverse Tree\n        traverser = TraverseCodebase(self.code_base_model)\n        self.search_result = traverser.get_top_nodes(query_keywords, 5)\n        # Question Answer\n        responder = QueryAnswer(self.search_result)\n        response = responder.get_response(question)\n        return response\n\nclass TestIntegration:\n    def __init__(self):\n        print(\"INTEGRATION TEST\")\n        self.test_local_code_base = \"rust_calculator_project\"\n        self.test_github_repo = \"https://github.com/TravHaran/rust-calculator\"\n        self.test_ignore_file = \"ignore.txt\"\n        \n        \n    \n    def test_run_local_codebase(self):\n        integration = Integration(self.test_local_code_base, self.test_ignore_file)\n        print(f\"modelling codebase from local directory: {self.test_local_code_base}\")\n        integration.model_codebase()\n        #Q1\n        question = \"Does this project have a multiplication capability?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        #Q2\n        question = \"does it have a square operation functionality?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        #Q3\n        question = \"how would we modify the code to add a square function?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n    \n    def test_run_github_repo(self):\n        integration = Integration(self.test_github_repo, self.test_ignore_file)\n        print(f\"modelling codebase from repo: {self.test_github_repo}\")\n        integration.model_codebase()\n        #Q1\n        question = \"Does this project have a multiplication capability?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        #Q2\n        question = \"does it have a square operation functionality?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n        #Q3\n        question = \"how would we modify the code to add a square function?\"\n        print(f\"Q: {question}\")\n        print(\"querying codebase...\")\n        response = integration.query(question)\n        print(f\"RESPONSE: \\n{response}\\n\")\n    \n    def test_run_loop_prompt(self):\n        print(\"modelling codebase...\")\n        integration = Integration(self.test_github_repo, self.test_ignore_file)\n        integration.model_codebase()\n        while True:\n            question = input(\"QUESTION: \")\n            print(\"querying codebase...\")\n            response = integration.query(question)\n            print(f\"RESPONSE: \\n{response}\\n\")\n        \n        \nif __name__ == \"__main__\":\n    testIntegration = TestIntegration()\n    # testIntegration.test_run_loop_prompt()\n    # testIntegration.test_run_github_repo()\n    # testIntegration.test_run_local_codebase()"
                        },
                        {
                            "name": "rust_calculator_project",
                            "path": "codesense/test/integration_test/rust_calculator_project",
                            "type": "folder",
                            "keywords": [],
                            "children": [
                                {
                                    "name": "Cargo.lock",
                                    "path": "codesense/test/integration_test/rust_calculator_project/Cargo.lock",
                                    "type": "file",
                                    "keywords": [],
                                    "annotation": "",
                                    "content": "# This file is automatically @generated by Cargo.\n# It is not intended for manual editing.\n[[package]]\nname = \"calc\"\nversion = \"0.1.0\"\ndependencies = [\n \"itertools\",\n]\n\n[[package]]\nname = \"either\"\nversion = \"1.6.1\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\nchecksum = \"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\"\n\n[[package]]\nname = \"itertools\"\nversion = \"0.10.0\"\nsource = \"registry+https://github.com/rust-lang/crates.io-index\"\nchecksum = \"37d572918e350e82412fe766d24b15e6682fb2ed2bbe018280caa810397cb319\"\ndependencies = [\n \"either\",\n]\n"
                                },
                                {
                                    "name": "Cargo.toml",
                                    "path": "codesense/test/integration_test/rust_calculator_project/Cargo.toml",
                                    "type": "file",
                                    "keywords": [],
                                    "annotation": "",
                                    "content": "[package]\nname = \"calc\"\nversion = \"0.1.0\"\nauthors = [\"Henry Boisdequin <65845077+henryboisdequin@users.noreply.github.com>\"]\nedition = \"2018\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]\nitertools = \"0.10\"\n"
                                },
                                {
                                    "name": "README.md",
                                    "path": "codesense/test/integration_test/rust_calculator_project/README.md",
                                    "type": "file",
                                    "keywords": [],
                                    "annotation": "",
                                    "content": "Simple command-line calculator in Rust.\n\n## To Run\n\n1. Clone this repository\n\n2. Make sure you have Rust and cargo installed\n\n3. Cd into the project directory and type `cargo run`\n\n4. To test: run `cargo test`\n"
                                },
                                {
                                    "name": "src",
                                    "path": "codesense/test/integration_test/rust_calculator_project/src",
                                    "type": "folder",
                                    "keywords": [],
                                    "children": [
                                        {
                                            "name": "calc.rs",
                                            "path": "codesense/test/integration_test/rust_calculator_project/src/calc.rs",
                                            "type": "file",
                                            "keywords": [],
                                            "annotation": "",
                                            "content": "use itertools::Itertools;\nuse std::ops::{Div, Sub};\n\npub struct Calc;\n\nimpl Calc {\n    pub fn add(arr: Vec<f64>) -> f64 {\n        arr.iter().sum::<f64>()\n    }\n\n    pub fn sub(arr: Vec<f64>) -> f64 {\n        arr.iter().map(|&x| x as f64).fold1(Sub::sub).unwrap_or(0.0)\n    }\n\n    pub fn mul(arr: Vec<f64>) -> f64 {\n        arr.iter().product()\n    }\n\n    pub fn div(arr: Vec<f64>) -> f64 {\n        arr.iter().map(|&x| x as f64).fold1(Div::div).unwrap_or(0.0)\n    }\n}\n\n#[test]\nfn test_all_operations() {\n    // addition\n    assert_eq!(Calc::add([2.0, 4.0, 6.0].to_vec()), 12.0);\n    assert_eq!(Calc::add([-6.0, 5.0, 10.0].to_vec()), 9.0);\n\n    // subtraction\n    assert_eq!(Calc::sub([10.0, 4.0, 6.0].to_vec()), 0.0);\n    assert_eq!(Calc::sub([100.0, 10.0, 19.0].to_vec()), 71.0);\n\n    // multiplication\n    assert_eq!(Calc::mul([10.0, 10.0, 2.0].to_vec()), 200.0);\n    assert_eq!(Calc::mul([-3.0, 2.0].to_vec()), -6.0);\n\n    // division\n    assert_eq!(Calc::div([54.0, 2.0, 3.0].to_vec()), 9.0);\n    assert_eq!(Calc::div([4.0, 2.0, 5.0].to_vec()), 0.4);\n}\n"
                                        },
                                        {
                                            "name": "main.rs",
                                            "path": "codesense/test/integration_test/rust_calculator_project/src/main.rs",
                                            "type": "file",
                                            "keywords": [],
                                            "annotation": "",
                                            "content": "mod calc;\nuse calc::Calc;\nuse std::io;\n\nfn main() {\n    println!(\"Welcome to the a basic calculator built with Rust.\");\n\n    loop {\n        println!(\"Please enter an equation or \\\"q\\\" to quit: \");\n\n        let mut input = String::new();\n        io::stdin()\n            .read_line(&mut input)\n            .expect(\"Failed to read input\");\n\n        if input.trim() == \"q\" {\n            println!(\"Thanks for using this program.\");\n            break;\n        }\n\n        let valid_operators = vec![\"+\", \"-\", \"*\", \"/\"];\n\n        for operator in valid_operators {\n            match input.find(operator) {\n                Some(_) => {\n                    let parts: Vec<&str> = input.split(operator).collect();\n\n                    if parts.len() < 2 {\n                        panic!(\"Invalid equation.\");\n                    }\n\n                    let mut number_array = vec![];\n                    let mut counter = 0;\n\n                    while counter != parts.len() {\n                        let val: f64 = parts[counter].trim().parse().ok().expect(\"Enter a number.\");\n                        number_array.push(val);\n                        counter += 1;\n                    }\n\n                    match operator {\n                        \"+\" => println!(\"{}\", Calc::add(number_array)),\n                        \"-\" => println!(\"{}\", Calc::sub(number_array)),\n                        \"*\" => println!(\"{}\", Calc::mul(number_array)),\n                        \"/\" => println!(\"{}\", Calc::div(number_array)),\n                        _ => println!(\"Only addition, subtraction, multiplication and division are supported.\")\n                    }\n                }\n\n                None => {\n                    continue;\n                }\n            }\n        }\n    }\n}\n"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "name": "sample_test_output.txt",
                            "path": "codesense/test/integration_test/sample_test_output.txt",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "INTEGRATION TEST\nmodelling codebase...\njson file saved: ./codebase.json\nQ: Does this project have a multiplication capability?\nquerying codebase...\nRESPONSE: \nYes, this project does have multiplication capability. The `calc.rs` file defines a `Calc` struct with a method named `mul` that performs multiplication on vectors of floating-point numbers (`Vec<f64>`). This method multiplies all elements in the provided vector together. Additionally, the `main.rs` file handles user input and uses the `Calc` module to perform various arithmetic operations, including multiplication.\n\nQ: does it have a square operation functionality?\nquerying codebase...\nRESPONSE: \nBased on the provided context, the codebase does not include a square operation functionality. The `Calc` struct in `calc.rs` offers methods for addition, subtraction, multiplication, and division, but it does not mention any support for squaring a number. \n\nTo add such functionality, you would need to implement a new method, such as:\n\n```rust\nimpl Calc {\n    // Other methods...\n\n    pub fn square(&self, x: f64) -> f64 {\n        x * x\n    }\n}\n```\n\nYou would also need to modify the `main.rs` file to handle the square operation input from the user.\n\nQ: how would we modify the code to add a square function?\nquerying codebase...\nRESPONSE: \nTo add a `square` function to the existing `Calc` struct in `calc.rs`, you would define a new method called `square` in the `impl` block. This function will take a `Vec<f64>` and return a new `Vec<f64>` where each element is the square of the corresponding element in the input vector. Additionally, you will need to add a test case for this new function in the `test_all_operations` function.\n\nHere is how you can modify the `calc.rs` file:\n\n```rust\n// Adding new square method to the Calc struct\nimpl Calc {\n    // Existing methods: add, sub, mul, div\n\n    pub fn square(numbers: Vec<f64>) -> Vec<f64> {\n        numbers.into_iter().map(|x| x * x).collect()\n    }\n}\n\n// Adding test case for the new square method\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_all_operations() {\n        // Existing test cases for add, sub, mul, div\n\n        // Test case for square\n        let numbers = vec![1.0, 2.0, 3.0];\n        let squared = Calc::square(numbers.clone());\n        assert_eq!(squared, vec![1.0, 4.0, 9.0]);\n    }\n}\n```\n\nThis modification includes a new method `square` that maps each number in the input vector to its square and collects the results into a new vector. The added test case verifies that the `square` method works as expected."
                        }
                    ]
                }
            ]
        },
        {
            "name": "tree_traverse",
            "path": "codesense/tree_traverse",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "test_codebase.json",
                    "path": "codesense/tree_traverse/test_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"keywords\": [],\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"English\",\n                        \"Language\",\n                        \"Natural\",\n                        \"Python\",\n                        \"TestKeywordExtract\",\n                        \"Toolkit\",\n                        \"annotated\",\n                        \"based\",\n                        \"class\",\n                        \"code\",\n                        \"contains\",\n                        \"create\",\n                        \"description\",\n                        \"ensuring\",\n                        \"expected\",\n                        \"extracted\",\n                        \"extraction\",\n                        \"extracts\",\n                        \"filtering\",\n                        \"filters\",\n                        \"identifies\",\n                        \"includes\",\n                        \"input\",\n                        \"keyword\",\n                        \"keywords\",\n                        \"list\",\n                        \"lists\",\n                        \"method\",\n                        \"nltk\",\n                        \"nouns\",\n                        \"output\",\n                        \"pieces\",\n                        \"processing\",\n                        \"provided\",\n                        \"query\",\n                        \"running\",\n                        \"script\",\n                        \"selecting\",\n                        \"stopwords\",\n                        \"tagging\",\n                        \"test\",\n                        \"text\",\n                        \"tokenizes\",\n                        \"written\"\n                    ],\n                    \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"Character\",\n                        \"Python\",\n                        \"attributes\",\n                        \"character\",\n                        \"class\",\n                        \"code\",\n                        \"created\",\n                        \"damage\",\n                        \"double_speed\",\n                        \"doubled\",\n                        \"doubles\",\n                        \"game\",\n                        \"health\",\n                        \"includes\",\n                        \"initialized\",\n                        \"instances\",\n                        \"models\",\n                        \"named\",\n                        \"ninja\",\n                        \"output\",\n                        \"parameters\",\n                        \"printed\",\n                        \"showcase\",\n                        \"speed\",\n                        \"speeds\",\n                        \"updated\",\n                        \"using\",\n                        \"warrior\",\n                        \"written\"\n                    ],\n                    \"annotation\": \"The code is written in Python and models a character in a video game using a class named `Character`. The class is initialized with three parameters: `health`, `damage`, and `speed`. It includes a method `double_speed` that doubles the character's speed. Two instances of `Character` (warrior and ninja) are created with different attributes. The initial speed of each character is printed, then the warrior's speed is doubled, and the updated speed is printed. The output will showcase the speeds for the warrior and ninja before and after the warrior's speed is doubled.\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"test_codebase.json\",\n                    \"type\": \"file\",\n                    \"keywords\": [],\n                    \"annotation\": \"\",\n                    \"content\": \"\"\n                },\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"keywords\": [\n                        \"Expected\",\n                        \"JSON\",\n                        \"Python\",\n                        \"class\",\n                        \"code\",\n                        \"content\",\n                        \"contents\",\n                        \"converts\",\n                        \"create\",\n                        \"defines\",\n                        \"designed\",\n                        \"directories\",\n                        \"directory\",\n                        \"file\",\n                        \"file_to_string\",\n                        \"files\",\n                        \"functionality\",\n                        \"generates\",\n                        \"given\",\n                        \"leaf\",\n                        \"method\",\n                        \"model\",\n                        \"model_to_str\",\n                        \"named\",\n                        \"nodes\",\n                        \"output\",\n                        \"provided\",\n                        \"reads\",\n                        \"representing\",\n                        \"save_model_json\",\n                        \"self.test_path\",\n                        \"stores\",\n                        \"string\",\n                        \"structure\",\n                        \"test_codebase.json\",\n                        \"tests\",\n                        \"traversing\",\n                        \"treating\",\n                        \"tree\",\n                        \"writes\"\n                    ],\n                    \"annotation\": \"The provided Python code defines a `CodebaseExtract` class designed to create a JSON model of a given directory's structure, treating it as a tree. For leaf nodes (files), it stores the file content as a string if readable. The `extract` method generates this structure by recursively traversing directories. The `file_to_string` method reads file contents, the `model_to_str` method converts the model to a JSON string, and the `save_model_json` method writes the model to a JSON file. The `TestCodebaseExtract` class tests this functionality. Expected output is a JSON file named \\\"test_codebase.json\\\" representing the directory structure of `self.test_path`.\",\n                    \"content\": \"import os\\nimport json\\n\\n'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\n\\nclass CodebaseExtract:\\n    def __init__(self, path):\\n        # Initialize the output dictionary model with folder contents\\n        # name, type, keywords, and empty list for children\\n        self.path = path\\n        self.model = {}\\n\\n    def file_to_string(self, file_path):  # save file content as string\\n        with open(file_path, 'r') as file:\\n            file_content = file.read()\\n        file.close()\\n        return file_content\\n\\n    def extract(self, path):  # extracts a directory as a json object\\n        model = {'name': os.path.basename(path),\\n                 'type': 'folder', 'keywords': [], 'children': []}\\n        # Check if the path is a directory\\n        if not os.path.isdir(path):\\n            return model\\n\\n        # Iterate over the entries in the directory\\n        for entry in os.listdir(path):\\n            if not entry.startswith('.'):  # ignore hidden folders & files\\n                # Create the fill path for current entry\\n                entry_path = os.path.join(path, entry)\\n                # if the entry is a directory, recursively call the function\\n                if os.path.isdir(entry_path):\\n                    model['children'].append(self.extract(entry_path))\\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\\n                else:\\n                    content = \\\"\\\"\\n                    # save file content as string\\n                    try:\\n                        content = self.file_to_string(entry_path)\\n                    except OSError:\\n                        content = \\\"n/a\\\"\\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\\n                    ], 'annotation': \\\"\\\", 'content': content})\\n        return model\\n\\n    def model_to_str(self):  # convert codebase json to string\\n        output_str = json.dumps(self.model, indent=4)\\n        return output_str\\n\\n    def save_model_json(self, file_name):  # codebase model json file\\n        save_file = open(f\\\"{file_name}.json\\\", 'w')\\n        self.model = self.extract(self.path)\\n        json.dump(self.model, save_file, indent=4)\\n        save_file.close()\\n        print(f\\\"Codebase model saved as {file_name}\\\")\\n        return self.model\\n\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.test_path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        self.extractor = CodebaseExtract(self.test_path)\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n\\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        output = self.extractor.save_model_json(\\\"test_codebase\\\")\\n        # model_str = self.extractor.model_to_str()\\n        # print(f\\\"Codebase model: {model_str}\\\")\\n        assert type(output) == dict\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testCodebaseExtract = TestCodebaseExtract()\\n    testCodebaseExtract.test_extract_codebase()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"keywords\": [\n                \"Aggregation\",\n                \"Annotation\",\n                \"Answering\",\n                \"Call\",\n                \"CodeBase\",\n                \"Codesense\",\n                \"Compiles\",\n                \"Creates\",\n                \"Extraction\",\n                \"Generates\",\n                \"Generation\",\n                \"Graph\",\n                \"Identifies\",\n                \"Keyword\",\n                \"Produces\",\n                \"Question\",\n                \"Searches\",\n                \"Traversal\",\n                \"Tree\",\n                \"Uses\",\n                \"aggregated\",\n                \"analyze\",\n                \"annotation.7\",\n                \"annotations\",\n                \"answer\",\n                \"based\",\n                \"call\",\n                \"called\",\n                \"code\",\n                \"code.4\",\n                \"codebase\",\n                \"codebase.2\",\n                \"codebases\",\n                \"components\",\n                \"directed\",\n                \"document\",\n                \"file.3\",\n                \"flows\",\n                \"function\",\n                \"functions\",\n                \"graph.5\",\n                \"implementations\",\n                \"include\",\n                \"involves\",\n                \"keywords\",\n                \"matching\",\n                \"nodes\",\n                \"objectives\",\n                \"outlines\",\n                \"project\",\n                \"providing\",\n                \"queries.6\",\n                \"related\",\n                \"report\",\n                \"representing\",\n                \"returns\",\n                \"serves\",\n                \"showing\",\n                \"source\",\n                \"structure\",\n                \"summaries\",\n                \"target\",\n                \"tasks\",\n                \"tree\",\n                \"user\"\n            ],\n            \"annotation\": \"This document outlines a project called \\\"Codesense,\\\" which involves several computational tasks to analyze and summarize codebases. Key components include: \\n\\n1. CodeBase Tree Extraction: Generates a k-ary tree representing the directory structure of a codebase.\\n2. Call Graph Extraction: Creates a directed graph showing function call flows within a source code file.\\n3. Annotation Generation: Produces text summaries for functions in the code.\\n4. Annotation Aggregation: Compiles a report from annotations based on the call graph.\\n5. Keyword Extraction: Identifies keywords from the aggregated report and user queries.\\n6. Tree Traversal: Searches the codebase tree for nodes matching target keywords and returns the related aggregated annotation.\\n7. Question Answering: Uses the aggregated annotations to answer user queries.\\n\\nThe document serves as a high-level breakdown of the project's objectives and functions without providing specific code implementations.\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"tree_traverse\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": []\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"keywords\": [],\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Automatic\",\n                                \"Extraction\",\n                                \"Gensim\",\n                                \"Keyword\",\n                                \"NLP\",\n                                \"NLTK\",\n                                \"Python\",\n                                \"RAKE\",\n                                \"Rapid\",\n                                \"SSL\",\n                                \"algorithm\",\n                                \"certificate\",\n                                \"changing\",\n                                \"command\",\n                                \"commands\",\n                                \"consists\",\n                                \"downloading\",\n                                \"downloads\",\n                                \"environment\",\n                                \"error\",\n                                \"gensim\",\n                                \"install\",\n                                \"installing\",\n                                \"instructs\",\n                                \"involves\",\n                                \"issue\",\n                                \"language\",\n                                \"model\",\n                                \"occurs\",\n                                \"packages\",\n                                \"processing\",\n                                \"provided\",\n                                \"setting\",\n                                \"shell\",\n                                \"suggests\",\n                                \"text\",\n                                \"tokenization\",\n                                \"use\",\n                                \"version\",\n                                \"words\"\n                            ],\n                            \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"*\",\n                                \"Comparison\",\n                                \"Embeddings\",\n                                \"Extraction\",\n                                \"Gensim\",\n                                \"Keyword\",\n                                \"NLP\",\n                                \"NLTK\",\n                                \"Processing\",\n                                \"Python\",\n                                \"Similarity\",\n                                \"Text\",\n                                \"Word\",\n                                \"Word2Vec\",\n                                \"calculates\",\n                                \"code\",\n                                \"compare_keywords\",\n                                \"compare_words\",\n                                \"comparing\",\n                                \"computes\",\n                                \"console\",\n                                \"context\",\n                                \"create\",\n                                \"embeddings\",\n                                \"employs\",\n                                \"extract_keywords\",\n                                \"extracted\",\n                                \"extraction\",\n                                \"extracts\",\n                                \"file\",\n                                \"focuses\",\n                                \"function\",\n                                \"input\",\n                                \"keyword\",\n                                \"keywords\",\n                                \"keywords.The\",\n                                \"language\",\n                                \"libraries\",\n                                \"library\",\n                                \"list\",\n                                \"lists\",\n                                \"model\",\n                                \"modeling.1\",\n                                \"output\",\n                                \"performs\",\n                                \"processes\",\n                                \"processing\",\n                                \"reads\",\n                                \"removes\",\n                                \"returned\",\n                                \"score\",\n                                \"sentences\",\n                                \"similarity\",\n                                \"tagging\",\n                                \"techniques\",\n                                \"text\",\n                                \"texts\",\n                                \"tokenizes\",\n                                \"uses\",\n                                \"using\",\n                                \"vector\",\n                                \"verbs\",\n                                \"word\",\n                                \"words\",\n                                \"words.3\",\n                                \"written\"\n                            ],\n                            \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"C++\",\n                                \"Solution\",\n                                \"adds\",\n                                \"calculates\",\n                                \"captures\",\n                                \"class\",\n                                \"description\",\n                                \"difference\",\n                                \"element\",\n                                \"end\",\n                                \"function\",\n                                \"increase\",\n                                \"iterates\",\n                                \"list\",\n                                \"maxP\",\n                                \"maxProfit\",\n                                \"method\",\n                                \"opportunities\",\n                                \"price\",\n                                \"prices\",\n                                \"profit\",\n                                \"refers\",\n                                \"representing\",\n                                \"returns\",\n                                \"sets\",\n                                \"stock\"\n                            ],\n                            \"annotation\": \"The description refers to a C++ function maxProfit within a Solution class that calculates the maximum profit from a list of stock prices. Initially, it sets maxP to 0, representing the maximum profit. It then iterates through the prices from the second element to the end. For each element, if the current price is higher than the previous price, it calculates the difference (profit) and adds it to maxP. Finally, it returns maxP, representing the total accumulated profit from all consecutive buy-and-sell opportunities. This method effectively captures profit from each price increase in the stock prices list.\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Additionally\",\n                                \"Face\",\n                                \"Hugging\",\n                                \"JSON\",\n                                \"NLTK\",\n                                \"Python\",\n                                \"README\",\n                                \"Transformers\",\n                                \"annotation\",\n                                \"annotations\",\n                                \"attributes\",\n                                \"called\",\n                                \"character\",\n                                \"code\",\n                                \"codebase\",\n                                \"codebases\",\n                                \"codesense\",\n                                \"components\",\n                                \"creating\",\n                                \"defined\",\n                                \"designed\",\n                                \"detailing\",\n                                \"directory\",\n                                \"employing\",\n                                \"environment\",\n                                \"expected\",\n                                \"extraction\",\n                                \"functionalities\",\n                                \"game\",\n                                \"generating\",\n                                \"generation\",\n                                \"include\",\n                                \"instructions\",\n                                \"involve\",\n                                \"keyword\",\n                                \"lists\",\n                                \"methods\",\n                                \"model\",\n                                \"modeling\",\n                                \"object\",\n                                \"outputs\",\n                                \"performing\",\n                                \"project\",\n                                \"provide\",\n                                \"provided\",\n                                \"representations\",\n                                \"representing\",\n                                \"scripts\",\n                                \"setting\",\n                                \"structure\",\n                                \"structured\",\n                                \"structures\",\n                                \"summaries\",\n                                \"tasks\",\n                                \"text\",\n                                \"using\",\n                                \"utilities\",\n                                \"working\"\n                            ],\n                            \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Hidden\",\n                                \"JSON\",\n                                \"Python\",\n                                \"called\",\n                                \"calls\",\n                                \"children\",\n                                \"code\",\n                                \"codebase.json\",\n                                \"contents\",\n                                \"converts\",\n                                \"create\",\n                                \"create_folder_structure_json\",\n                                \"creating\",\n                                \"designed\",\n                                \"dictionary\",\n                                \"directories\",\n                                \"directory\",\n                                \"file\",\n                                \"file_path\",\n                                \"file_to_string\",\n                                \"files\",\n                                \"folder\",\n                                \"folders\",\n                                \"found\",\n                                \"function\",\n                                \"given\",\n                                \"ignored\",\n                                \"including\",\n                                \"indentation\",\n                                \"list\",\n                                \"named\",\n                                \"names\",\n                                \"navigates\",\n                                \"nested\",\n                                \"object\",\n                                \"output\",\n                                \"path\",\n                                \"prints\",\n                                \"read\",\n                                \"reads\",\n                                \"representation\",\n                                \"representing\",\n                                \"returns\",\n                                \"saves\",\n                                \"script\",\n                                \"specifies\",\n                                \"starting\",\n                                \"string\",\n                                \"structure\",\n                                \"types\"\n                            ],\n                            \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"keywords\": [],\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"CodeLlama\",\n                                \"English\",\n                                \"Python\",\n                                \"accelerate\",\n                                \"code\",\n                                \"command\",\n                                \"environment\",\n                                \"install\",\n                                \"installed\",\n                                \"instruction\",\n                                \"libraries\",\n                                \"model\",\n                                \"packages\",\n                                \"pip\",\n                                \"plain\",\n                                \"provides\",\n                                \"required\",\n                                \"run\",\n                                \"running\",\n                                \"snippet\",\n                                \"transformers\",\n                                \"working\"\n                            ],\n                            \"annotation\": \"The code snippet provides an instruction in plain English to install two Python packages, `transformers` and `accelerate`, that are required to run the CodeLlama model. The command to install these packages via pip is: `pip install transformers accelerate`. After running this command, the necessary libraries for working with the CodeLlama model will be installed on your environment.\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"keywords\": [\n                                \"Face\",\n                                \"First\",\n                                \"Hugging\",\n                                \"Llama\",\n                                \"Python\",\n                                \"Transformers\",\n                                \"characters\",\n                                \"code\",\n                                \"continuation\",\n                                \"enabled\",\n                                \"expected\",\n                                \"function\",\n                                \"generate\",\n                                \"generated\",\n                                \"generation\",\n                                \"imports\",\n                                \"language\",\n                                \"length\",\n                                \"library\",\n                                \"load\",\n                                \"model\",\n                                \"output\",\n                                \"pipeline\",\n                                \"prints\",\n                                \"prompt\",\n                                \"provided\",\n                                \"provides\",\n                                \"sampling\",\n                                \"text\",\n                                \"time\",\n                                \"transformers\",\n                                \"use\",\n                                \"uses\",\n                                \"written\"\n                            ],\n                            \"annotation\": \"The code is written in Python and utilizes the Hugging Face Transformers library to load and use the Llama 3 language model for text generation. First, it imports the `pipeline` function from the transformers library. It then initializes a text-generation pipeline with the Llama 3 model. The code provides an initial text prompt \\\"Once upon a time\\\" and uses the model to generate continuation text up to a maximum length of 50 characters with sampling enabled. Finally, it prints the generated text. The expected output will be the continuation of the provided prompt.\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                },
                {
                    "name": "top_1.json",
                    "path": "codesense/tree_traverse/top_1.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        }\n    ]\n}"
                },
                {
                    "name": "top_3.json",
                    "path": "codesense/tree_traverse/top_3.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        },\n        {\n            \"score\": 0.5,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"testkeywordextract\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"keyword_extract.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"English\",\n                    \"Language\",\n                    \"Natural\",\n                    \"Python\",\n                    \"TestKeywordExtract\",\n                    \"Toolkit\",\n                    \"annotated\",\n                    \"based\",\n                    \"class\",\n                    \"code\",\n                    \"contains\",\n                    \"create\",\n                    \"description\",\n                    \"ensuring\",\n                    \"expected\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"filtering\",\n                    \"filters\",\n                    \"identifies\",\n                    \"includes\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"list\",\n                    \"lists\",\n                    \"method\",\n                    \"nltk\",\n                    \"nouns\",\n                    \"output\",\n                    \"pieces\",\n                    \"processing\",\n                    \"provided\",\n                    \"query\",\n                    \"running\",\n                    \"script\",\n                    \"selecting\",\n                    \"stopwords\",\n                    \"tagging\",\n                    \"test\",\n                    \"text\",\n                    \"tokenizes\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"info.txt\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Automatic\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"RAKE\",\n                    \"Rapid\",\n                    \"SSL\",\n                    \"algorithm\",\n                    \"certificate\",\n                    \"changing\",\n                    \"command\",\n                    \"commands\",\n                    \"consists\",\n                    \"downloading\",\n                    \"downloads\",\n                    \"environment\",\n                    \"error\",\n                    \"gensim\",\n                    \"install\",\n                    \"installing\",\n                    \"instructs\",\n                    \"involves\",\n                    \"issue\",\n                    \"language\",\n                    \"model\",\n                    \"occurs\",\n                    \"packages\",\n                    \"processing\",\n                    \"provided\",\n                    \"setting\",\n                    \"shell\",\n                    \"suggests\",\n                    \"text\",\n                    \"tokenization\",\n                    \"use\",\n                    \"version\",\n                    \"words\"\n                ],\n                \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n            }\n        }\n    ]\n}"
                },
                {
                    "name": "top_5.json",
                    "path": "codesense/tree_traverse/top_5.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"input_keywords\": [\n        \"python\",\n        \"function\",\n        \"testkeywordextract\",\n        \"nltk\",\n        \"word2vec\",\n        \"extract_keywords\"\n    ],\n    \"results\": [\n        {\n            \"score\": 0.8333333333333334,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\",\n                \"extract_keywords\",\n                \"word2vec\",\n                \"nltk\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"*\",\n                    \"Comparison\",\n                    \"Embeddings\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Processing\",\n                    \"Python\",\n                    \"Similarity\",\n                    \"Text\",\n                    \"Word\",\n                    \"Word2Vec\",\n                    \"calculates\",\n                    \"code\",\n                    \"compare_keywords\",\n                    \"compare_words\",\n                    \"comparing\",\n                    \"computes\",\n                    \"console\",\n                    \"context\",\n                    \"create\",\n                    \"embeddings\",\n                    \"employs\",\n                    \"extract_keywords\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"file\",\n                    \"focuses\",\n                    \"function\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"keywords.The\",\n                    \"language\",\n                    \"libraries\",\n                    \"library\",\n                    \"list\",\n                    \"lists\",\n                    \"model\",\n                    \"modeling.1\",\n                    \"output\",\n                    \"performs\",\n                    \"processes\",\n                    \"processing\",\n                    \"reads\",\n                    \"removes\",\n                    \"returned\",\n                    \"score\",\n                    \"sentences\",\n                    \"similarity\",\n                    \"tagging\",\n                    \"techniques\",\n                    \"text\",\n                    \"texts\",\n                    \"tokenizes\",\n                    \"uses\",\n                    \"using\",\n                    \"vector\",\n                    \"verbs\",\n                    \"word\",\n                    \"words\",\n                    \"words.3\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and focuses on keyword extraction from text and comparing the similarity between keyword lists using natural language processing (NLP) techniques. It employs the NLTK and Gensim libraries for text processing and word vector modeling.\\n\\n1. **Keyword Extraction**: The function `extract_keywords` tokenizes input text, removes stop words, performs part-of-speech tagging, and extracts significant words (nouns and verbs). These extracted keywords are then returned as a list.\\n   \\n2. **Text Processing and Word Embeddings**: The code reads and processes a context file, tokenizes the sentences into words, and uses the Gensim library's Word2Vec model to create word embeddings for these words.\\n\\n3. **Similarity Comparison**: With the `compare_words` function, the code computes the similarity between two words using the Word2Vec model, and `compare_keywords` function calculates the cumulative similarity score between two lists of keywords.\\n\\nThe code reads two input texts, extracts keywords from each, and computes the similarity score between the two keyword lists. The final output is the similarity score printed to the console.\",\n                \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n            }\n        },\n        {\n            \"score\": 0.5,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"testkeywordextract\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"keyword_extract.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"English\",\n                    \"Language\",\n                    \"Natural\",\n                    \"Python\",\n                    \"TestKeywordExtract\",\n                    \"Toolkit\",\n                    \"annotated\",\n                    \"based\",\n                    \"class\",\n                    \"code\",\n                    \"contains\",\n                    \"create\",\n                    \"description\",\n                    \"ensuring\",\n                    \"expected\",\n                    \"extracted\",\n                    \"extraction\",\n                    \"extracts\",\n                    \"filtering\",\n                    \"filters\",\n                    \"identifies\",\n                    \"includes\",\n                    \"input\",\n                    \"keyword\",\n                    \"keywords\",\n                    \"list\",\n                    \"lists\",\n                    \"method\",\n                    \"nltk\",\n                    \"nouns\",\n                    \"output\",\n                    \"pieces\",\n                    \"processing\",\n                    \"provided\",\n                    \"query\",\n                    \"running\",\n                    \"script\",\n                    \"selecting\",\n                    \"stopwords\",\n                    \"tagging\",\n                    \"test\",\n                    \"text\",\n                    \"tokenizes\",\n                    \"written\"\n                ],\n                \"annotation\": \"The code is written in Python and utilizes the Natural Language Toolkit (nltk) library to create a class that extracts keywords from text by processing and filtering out common English stopwords, then selecting only nouns and verbs. The `KeywordExtract` class contains an `extract` method which tokenizes the input text, filters out stopwords, and identifies keywords based on part-of-speech tagging. A secondary class, `TestKeywordExtract`, is provided to test the keyword extraction on specific pieces of text, ensuring the output is a list of relevant keywords. The expected output of running the main script includes printed lists of keywords extracted from a user query and an annotated code description.\",\n                \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"info.txt\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Automatic\",\n                    \"Extraction\",\n                    \"Gensim\",\n                    \"Keyword\",\n                    \"NLP\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"RAKE\",\n                    \"Rapid\",\n                    \"SSL\",\n                    \"algorithm\",\n                    \"certificate\",\n                    \"changing\",\n                    \"command\",\n                    \"commands\",\n                    \"consists\",\n                    \"downloading\",\n                    \"downloads\",\n                    \"environment\",\n                    \"error\",\n                    \"gensim\",\n                    \"install\",\n                    \"installing\",\n                    \"instructs\",\n                    \"involves\",\n                    \"issue\",\n                    \"language\",\n                    \"model\",\n                    \"occurs\",\n                    \"packages\",\n                    \"processing\",\n                    \"provided\",\n                    \"setting\",\n                    \"shell\",\n                    \"suggests\",\n                    \"text\",\n                    \"tokenization\",\n                    \"use\",\n                    \"version\",\n                    \"words\"\n                ],\n                \"annotation\": \"The provided text consists of shell commands primarily for setting up a natural language processing (NLP) environment in Python. It involves installing the RAKE (Rapid Automatic Keyword Extraction) algorithm via `pip3 install --user rake-nltk` and downloading necessary NLTK packages for tokenization and stop words. If an SSL certificate error occurs during the downloads, it instructs changing the Python version in a specific command to fix the issue. Finally, it suggests installing the Gensim library with `pip3 install gensim` to use the Word2Vec model.\",\n                \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"nltk\",\n                \"python\"\n            ],\n            \"node\": {\n                \"name\": \"codebase.json\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Additionally\",\n                    \"Face\",\n                    \"Hugging\",\n                    \"JSON\",\n                    \"NLTK\",\n                    \"Python\",\n                    \"README\",\n                    \"Transformers\",\n                    \"annotation\",\n                    \"annotations\",\n                    \"attributes\",\n                    \"called\",\n                    \"character\",\n                    \"code\",\n                    \"codebase\",\n                    \"codebases\",\n                    \"codesense\",\n                    \"components\",\n                    \"creating\",\n                    \"defined\",\n                    \"designed\",\n                    \"detailing\",\n                    \"directory\",\n                    \"employing\",\n                    \"environment\",\n                    \"expected\",\n                    \"extraction\",\n                    \"functionalities\",\n                    \"game\",\n                    \"generating\",\n                    \"generation\",\n                    \"include\",\n                    \"instructions\",\n                    \"involve\",\n                    \"keyword\",\n                    \"lists\",\n                    \"methods\",\n                    \"model\",\n                    \"modeling\",\n                    \"object\",\n                    \"outputs\",\n                    \"performing\",\n                    \"project\",\n                    \"provide\",\n                    \"provided\",\n                    \"representations\",\n                    \"representing\",\n                    \"scripts\",\n                    \"setting\",\n                    \"structure\",\n                    \"structured\",\n                    \"structures\",\n                    \"summaries\",\n                    \"tasks\",\n                    \"text\",\n                    \"using\",\n                    \"utilities\",\n                    \"working\"\n                ],\n                \"annotation\": \"This code is structured as a JSON object representing a project directory called \\\"codesense,\\\" designed to provide various utilities for working with codebases. The primary functionalities include keyword extraction, codebase tree extraction, and annotation generation. Key components include Python scripts performing tasks such as keyword extraction from text using NLTK, creating a JSON model of a directory structure, modeling a video game character with defined attributes and methods, and employing the Hugging Face Transformers library for text generation. Additionally, there's an informative README detailing the project's breakdown and instructions for setting up the environment. The expected outputs involve generating keyword lists, JSON representations of directory structures, and textual annotations or summaries from provided codebases.\",\n                \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"import nltk\\\\nfrom nltk.tokenize import word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\n\\\\n\\\\n'''\\\\nCreate a class to extract keywords from text\\\\n- input:\\\\n    - sample text as a string\\\\n-output: \\\\n    - list of keywords\\\\n'''\\\\n\\\\n\\\\nclass KeywordExtract:\\\\n    def __init__(self):\\\\n        self.keywords = []\\\\n        # common english stopwords\\\\n        self.stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n\\\\n    def extract(self, text):\\\\n        tokens = word_tokenize(text)  # tokenize text\\\\n        filtered_tokens = [word for word in tokens if word.lower(\\\\n        ) not in self.stop_words]  # filter out stopwords\\\\n        # identify keywords with part of speech tagging\\\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\\\n        # keep only nouns, verbs\\\\n        for word, pos in pos_tags:\\\\n            if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n                self.keywords.append(word)\\\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\\\n        return self.keywords\\\\n\\\\n\\\\nclass TestKeywordExtract:\\\\n    def __init__(self):\\\\n        self.extractor = KeywordExtract()\\\\n        print(\\\\\\\"Testing Keyword Extractor...\\\\\\\\n\\\\\\\")\\\\n\\\\n    def test_extract_keywords_from_query(self):\\\\n        print(\\\\\\\"Testing keywword extraction of user query...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from query: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n    def test_extract_keywords_from_annotation(self):\\\\n        print(\\\\\\\"Testing keywword extraction of code annotation...\\\\\\\\n\\\\\\\")\\\\n        text = \\\\\\\"\\\\\\\"\\\\\\\"\\\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n            \\\\\\\"\\\\\\\"\\\\\\\"\\\\n        output = self.extractor.extract(text)\\\\n        print(f\\\\\\\"Keywords from annotation: {output}\\\\\\\\n\\\\\\\")\\\\n        assert type(output) == list\\\\n\\\\n\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    testKeywordExtract = TestKeywordExtract()\\\\n    testKeywordExtract.test_extract_keywords_from_query()\\\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\\\n\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to model a character in a video game\\\\n- initialize the class with three parameters\\\\n    - Health\\\\n    - Damage\\\\n    - Speed\\\\n\\\\n- define a mathod to double the speed of the character\\\\n'''\\\\n\\\\nclass Character:\\\\n    def __init__(self, health, damage, speed):\\\\n        self.health = health\\\\n        self.damage = damage\\\\n        self.speed = speed\\\\n    \\\\n    def double_speed(self):\\\\n            self.speed *= 2\\\\n\\\\n\\\\n\\\\n\\\\nwarrior = Character(100, 50, 10)\\\\nninja = Character(80, 40, 40)\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\nprint(f\\\\\\\"Ninja speed: {ninja.speed}\\\\\\\")\\\\n\\\\nwarrior.double_speed()\\\\n\\\\nprint(f\\\\\\\"Warrior speed: {warrior.speed}\\\\\\\")\\\\n  \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"content\\\": \\\"'''\\\\nCreate a class to extract a model of a codebase as a tree\\\\n- input: local directory path as a string\\\\n- output: \\\\n    - json file containing tree structure of directory\\\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\\\n'''\\\\n\\\\nclass CodebaseExtract:\\\\n    def __init__(self):\\\\n        self.model\\\\n    \\\\n    def extract(self, path):\\\\n        return self.model\\\\n\\\\nclass TestCodebaseExtract:\\\\n    def __init__(self):\\\\n        self.extractor = CodebaseExtract()\\\\n        print(\\\\\\\"Testing Codebase Extractor...\\\\\\\\n\\\\\\\")\\\\n    \\\\n    def test_extract_codebase(self):\\\\n        print(\\\\\\\"Testing codebase extraction of current project directory...\\\\\\\\n\\\\\\\")\\\\n        path = \\\\\\\"/Users/trav/Documents/projects/codesense\\\\\\\"\\\\n        output = self.extractor.extract(path)\\\\n        assert type(output) == json\\\\n        \\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\",\\n            \\\"content\\\": \\\"# Project Codesense\\\\n\\\\n## Breakdown\\\\n\\\\n### 1. CodeBase Tree Extraction\\\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\\\n### 2. Call Graph Extraction\\\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\\\n### 3. Annotation Generation\\\\n    - for a fucntion defined in code generate a text summarization\\\\n### 4. Annotation Aggregation\\\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\\\n### 5. Keyword Extraction\\\\n    - from the aggregated annotation report extract a list of keywords\\\\n    - from a usery query extract a list of keywords\\\\n### 6. Tree Traversal\\\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\\\n### 7. Question Answering\\\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"install RAKE\\\\n`pip3 install --user rake-nltk`\\\\n\\\\ninstall supporting nltk packages\\\\n`python3 -c \\\\\\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\\\\\"`\\\\n\\\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\\\n\\\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\\\n\\\\nto use word2vec install gensim library\\\\n`pip3 install gensim`\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import nltk\\\\nimport gensim.downloader\\\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\\\nfrom nltk.corpus import stopwords\\\\nimport warnings\\\\n\\\\ninput_text1 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nI want to modify the maxProfit function to have an initial maxP value of 10\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\ninput_text2 = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\n\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\\n#######################extract keywords#######################\\\\n\\\\n#download necessary resources\\\\n# nltk.download('averaged_perceptron_tagger')\\\\n# nltk.download(\\\\\\\"punkt\\\\\\\")\\\\n# nltk.download(\\\\\\\"stopwords\\\\\\\")\\\\n\\\\ndef extract_keywords(text):\\\\n    #tokenize the text into words\\\\n    tokens = word_tokenize(text)\\\\n    #define a set of common English stopwords\\\\n    stop_words = set(stopwords.words(\\\\\\\"english\\\\\\\"))\\\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\\\n    keywords = []\\\\n    #identify keywords using part-of-speech tagging\\\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\\\n    #keep only nouns, proper nouns, and verbs\\\\n    for word, pos in pos_tags:\\\\n        if pos.startswith(\\\\\\\"NN\\\\\\\") or pos.startswith(\\\\\\\"VB\\\\\\\"):\\\\n            keywords.append(word)\\\\n    unique_keywords = list(set(keywords))\\\\n    return unique_keywords\\\\n\\\\n# print(extract_keywords(input_text1))\\\\n\\\\n#######################compute the similarity between keywords#######################\\\\n\\\\nwarnings.filterwarnings(action='ignore')\\\\n#  Reads \\\\u2018context.txt\\\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\\\nsample = open(\\\\\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\\\\\")\\\\ns = sample.read()\\\\n# Replaces escape character with space\\\\nf = s.replace(\\\\\\\"\\\\\\\\n\\\\\\\", \\\\\\\" \\\\\\\")\\\\ndata = []\\\\n# iterate through each sentence in the file\\\\nfor i in sent_tokenize(f):\\\\n    temp = []\\\\n    # tokenize the sentence into words\\\\n    for j in word_tokenize(i):\\\\n        temp.append(j.lower())\\\\n    data.append(temp)\\\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\\\n                                vector_size=100, window=5, sg=1)\\\\n\\\\ndef compare_words(w1, w2):\\\\n    if w1 == w2:\\\\n        return 1\\\\n    if w1 in model.wv and w2 in model.wv:\\\\n        return model.wv.similarity(w1, w2)\\\\n    else:\\\\n        return 0\\\\n\\\\ndef compare_keywords(l1, l2):\\\\n    output = 0\\\\n    for word1 in l1:\\\\n        word1 = word1.lower()\\\\n        for word2 in l2:\\\\n            output += compare_words(word1, word2.lower())\\\\n    return output\\\\n\\\\nlist1 = extract_keywords(input_text1)\\\\nlist2 = extract_keywords(input_text2)\\\\nprint(compare_keywords(list1, list2))\\\\n\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\\\nNike is a sports apparel company. It's brand is recognized accross the country\\\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"{\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"codesense\\\\\\\",\\\\n    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n    \\\\\\\"children\\\\\\\": [\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"template\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"template.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extract.py\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                }\\\\n            ]\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"README.md\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n        },\\\\n        {\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"extras\\\\\\\",\\\\n            \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n            \\\\\\\"children\\\\\\\": [\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"keyword_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"context.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"codebase_extraction\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"codebase.json\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                },\\\\n                {\\\\n                    \\\\\\\"name\\\\\\\": \\\\\\\"annotation_generation\\\\\\\",\\\\n                    \\\\\\\"type\\\\\\\": \\\\\\\"folder\\\\\\\",\\\\n                    \\\\\\\"children\\\\\\\": [\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"info.txt\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        },\\\\n                        {\\\\n                            \\\\\\\"name\\\\\\\": \\\\\\\"main.py\\\\\\\",\\\\n                            \\\\\\\"type\\\\\\\": \\\\\\\"file\\\\\\\"\\\\n                        }\\\\n                    ]\\\\n                }\\\\n            ]\\\\n        }\\\\n    ]\\\\n}\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"import os\\\\nimport json\\\\n\\\\ndef create_folder_structure_json(path):\\\\n    # Initialize the result dictionary with folder\\\\n    # name, type, and an empty list for children\\\\n    result = {'name': os.path.basename(path),\\\\n              'type': 'folder', 'children': []}\\\\n    \\\\n    # Check if the path is a directory\\\\n    if not os.path.isdir(path):\\\\n        return result\\\\n    \\\\n    # Iterate over the entries in the directory\\\\n    for entry in os.listdir(path):\\\\n        if not entry.startswith('.'): # ignore hidden folders & files\\\\n            # Create the full path for current entry\\\\n            entry_path = os.path.join(path, entry)\\\\n            \\\\n            #if the entry is a directory, recursively call the function\\\\n            if os.path.isdir(entry_path):\\\\n                result['children'].append(create_folder_structure_json(entry_path))\\\\n            # if the entry is a file, create a dictionary with name and type\\\\n            else:\\\\n                try:\\\\n                    content = file_to_string(entry_path)\\\\n                except OSError:\\\\n                    content = \\\\\\\"n/a\\\\\\\"\\\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\\\n    return result\\\\n\\\\ndef file_to_string(file_path):\\\\n    with open(file_path, 'r') as file:\\\\n        file_content = file.read()\\\\n    file.close()\\\\n    return file_content\\\\n# Specify the path to the folder you want to create the JSON for\\\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\\\n\\\\n# Call the function to create the JSON representation\\\\nfolder_json = create_folder_structure_json(folder_path)\\\\n\\\\n# Convert the dictionary to a JSON string with indentation\\\\nfolder_json_str = json.dumps(folder_json, indent=4)\\\\n\\\\n# Print the JSON representation of the folder structure\\\\nprint(folder_json_str)\\\\n\\\\n# Save as a JSON file\\\\nsave_file = open(\\\\\\\"codebase.json\\\\\\\", 'w')\\\\njson.dump(folder_json, save_file, indent=4)\\\\nsave_file.close()\\\\n\\\\n\\\\n    \\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"to run codellama model install transformers\\\\n`pip install transformers accelerate`\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\",\\n                            \\\"content\\\": \\\"from transformers import pipeline\\\\n\\\\n# Load Llama 3 model from Hugging Face\\\\nllama3_model = pipeline(\\\\\\\"text-generation\\\\\\\", model=\\\\\\\"meta-llama/Meta-Llama-3-8B\\\\\\\")\\\\n\\\\n# Generate text using the Llama 3 model\\\\nprompt = \\\\\\\"Once upon a time\\\\\\\"\\\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\\\n\\\\n# Print the generated text\\\\nprint(generated_text[0]['generated_text'])\\\\n\\\\n\\\\n\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n            }\n        },\n        {\n            \"score\": 0.3333333333333333,\n            \"matched_keywords\": [\n                \"python\",\n                \"function\"\n            ],\n            \"node\": {\n                \"name\": \"main.py\",\n                \"type\": \"file\",\n                \"keywords\": [\n                    \"Hidden\",\n                    \"JSON\",\n                    \"Python\",\n                    \"called\",\n                    \"calls\",\n                    \"children\",\n                    \"code\",\n                    \"codebase.json\",\n                    \"contents\",\n                    \"converts\",\n                    \"create\",\n                    \"create_folder_structure_json\",\n                    \"creating\",\n                    \"designed\",\n                    \"dictionary\",\n                    \"directories\",\n                    \"directory\",\n                    \"file\",\n                    \"file_path\",\n                    \"file_to_string\",\n                    \"files\",\n                    \"folder\",\n                    \"folders\",\n                    \"found\",\n                    \"function\",\n                    \"given\",\n                    \"ignored\",\n                    \"including\",\n                    \"indentation\",\n                    \"list\",\n                    \"named\",\n                    \"names\",\n                    \"navigates\",\n                    \"nested\",\n                    \"object\",\n                    \"output\",\n                    \"path\",\n                    \"prints\",\n                    \"read\",\n                    \"reads\",\n                    \"representation\",\n                    \"representing\",\n                    \"returns\",\n                    \"saves\",\n                    \"script\",\n                    \"specifies\",\n                    \"starting\",\n                    \"string\",\n                    \"structure\",\n                    \"types\"\n                ],\n                \"annotation\": \"The code is a Python script designed to generate a JSON representation of a folder structure, including file contents. The `create_folder_structure_json(path)` function recursively navigates through directories and files starting from a given path, creating a nested dictionary with folder names, types, and an empty list for children. Hidden files and folders are ignored. If a directory is found, the function is called recursively; if a file is found, its content is read into the dictionary. The `file_to_string(file_path)` function reads and returns the contents of a file. The script specifies a folder path, calls the function to create the JSON structure, converts it to a JSON string with indentation, prints it, and saves it to a file named \\\"codebase.json\\\". The output will be a JSON object representing the folder structure starting from `/Users/trav/Documents/projects/codesense`.\",\n                \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                # save file content as string\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n            }\n        }\n    ]\n}"
                },
                {
                    "name": "tree_traverse.py",
                    "path": "codesense/tree_traverse/tree_traverse.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import sys\n\nsys.path.insert(0, \"..\")\nfrom utilities.utility import obj_to_json, json_to_obj\n\n'''\nCreate a class to find the most relevant node in the codebase model given some keywords\n- input:\n    - list of keywords\n    - codebase model object\n- output:\n    - object of top nodes containing: file_name, annotation, content, and matching_keywords\n'''\n\n\nclass TraverseCodebase:\n    def __init__(self, model_obj):\n        self.model = model_obj\n        self.top_nodes_with_score = []\n        self.result_file_name = \"result\"\n\n    def compute_score(self, target_keywords, input_keywords):\n        score = 0\n        if not input_keywords or not target_keywords:  # handle empty list\n            return score\n        # input keywords should already be lowered, so only lower target_keywords\n        target_keywords = self.convert_keywords_to_lowercase(target_keywords)\n        for word in input_keywords:\n            if word in target_keywords:\n                score += 1\n        return score/len(input_keywords)\n\n    def get_matched_keywords(self, target_keywords, input_keywords):\n        target_keywords_lowered = self.convert_keywords_to_lowercase(\n            target_keywords)\n        input_set = set(input_keywords)\n        target_set = set(target_keywords_lowered)\n        # find intersection\n        common_elems = input_set.intersection(target_set)\n        return list(common_elems)\n\n    def get_top_nodes(self, input_keywords, n):\n        # we need to reset the top_nodes list to empty so that multiple calls of this method don't append to it\n        self.top_nodes_with_score = []\n        # lower input keywords to compute score properly\n        input_keywords_lowered = self.convert_keywords_to_lowercase(\n            input_keywords)\n        # recursively populate top_nodes list\n        self._get_top_nodes(self.model, input_keywords_lowered)\n        # after traversal sort top_nodes list by score in descending order\n        self.top_nodes_with_score.sort(key=lambda x: x[0], reverse=True)  \n        # return result\n        return self.build_result(n, input_keywords_lowered)\n\n    def _get_top_nodes(self, model, input_keywords):\n        if model[\"type\"] == \"file\":\n            # get matching keyword score\n            score = self.compute_score(model[\"keywords\"], input_keywords)\n            self.top_nodes_with_score.append((score, model))\n            return model\n        else:\n            for child in model[\"children\"]:\n                self._get_top_nodes(child, input_keywords)\n\n    def build_result(self, n, input_keywords):\n        result = {\"input_keywords\": input_keywords, \"results\": []}\n        for entry in self.top_nodes_with_score:\n            score = entry[0]\n            node = entry[1]\n            # add matched keywords attribute\n            matched_keywords = self.get_matched_keywords(\n                node[\"keywords\"], input_keywords)\n            entry = {'score': score, 'matched_keywords': matched_keywords, 'node': node}\n            result[\"results\"].append(entry)\n        result[\"results\"] = result[\"results\"][:n]\n        return result\n\n    def convert_keywords_to_lowercase(self, keywords):\n        return [word.lower() for word in keywords]\n\n\nclass TestTraverseCodebase:\n    def __init__(self):\n        self.test_model = json_to_obj(\"test_codebase.json\")\n        self.traverser = TraverseCodebase(self.test_model)\n\n    def test_save_top_1_nodes(self):\n        print(f\"Testing Traverse Codebase to save top 1 nodes\")\n        input_keywords = [\"Python\", \"function\", \"TestKeywordExtract\",\n                          \"NLTK\", \"Word2Vec\", \"extract_keywords\"]\n        updated_model = self.traverser.get_top_nodes(input_keywords, 1)\n        obj_to_json(\"./\", \"top_1\", updated_model)\n        assert type(updated_model) == dict\n\n    def test_save_top_3_nodes(self):\n        print(f\"Testing Traverse Codebase to save top 3 nodes\")\n        input_keywords = [\"Python\", \"function\", \"TestKeywordExtract\",\n                          \"NLTK\", \"Word2Vec\", \"extract_keywords\"]\n        updated_model = self.traverser.get_top_nodes(input_keywords, 3)\n        obj_to_json(\"./\", \"top_3\", updated_model)\n        assert type(updated_model) == dict\n\n    def test_save_top_5_nodes(self):\n        print(f\"Testing Traverse Codebase to save top 5 nodes\")\n        input_keywords = [\"Python\", \"function\", \"TestKeywordExtract\",\n                          \"NLTK\", \"Word2Vec\", \"extract_keywords\"]\n        updated_model = self.traverser.get_top_nodes(input_keywords, 5)\n        obj_to_json(\"./\", \"top_5\", updated_model)\n        assert type(updated_model) == dict\n\n\nif __name__ == \"__main__\":\n    testTraverseCodebase = TestTraverseCodebase()\n    testTraverseCodebase.test_save_top_1_nodes()\n    testTraverseCodebase.test_save_top_3_nodes()\n    testTraverseCodebase.test_save_top_5_nodes()\n\n"
                }
            ]
        },
        {
            "name": "utilities",
            "path": "codesense/utilities",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "test.json",
                    "path": "codesense/utilities/test.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"data\": \"test\"\n}"
                },
                {
                    "name": "test2.json",
                    "path": "codesense/utilities/test2.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "{\n    \"data\": \"test2\"\n}"
                },
                {
                    "name": "utility.py",
                    "path": "codesense/utilities/utility.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import json\nimport os\n\n'''\nCreate a utilty methods that can be used across multiple classes\n'''\n\n\ndef obj_to_json(output_path, filename, obj):\n    # makesure filename doesn't have .json extension\n    # makesure output_path has trailing backslash\n    output_file_path = os.path.join(output_path, f\"{filename}.json\")\n    save_file = open(output_file_path, 'w')\n    json.dump(obj, save_file, indent=4)\n    save_file.close()\n    print(f\"json file saved: {output_file_path}\")\n    # return output file path for debugging purposes\n    return output_file_path\n\n\ndef json_to_obj(json_file_path):\n    d = {}\n    with open(json_file_path) as json_data:\n        d = json.load(json_data)\n    return d\n\n\ndef file_to_string(file_path):  # save file content as string\n    with open(file_path, 'r') as file:\n        file_content = file.read()\n    file.close()\n    return file_content\n\n\nclass TestUtility:\n    def __init__(self):\n        self.test_json_file = \"test.json\"\n\n    def test_json_to_obj(self):\n        test_obj = json_to_obj(self.test_json_file)\n        assert test_obj[\"data\"] == \"test\"\n\n    def test_obj_to_json(self):\n        # load object & modify it\n        test_obj = json_to_obj(self.test_json_file)\n        test_obj[\"data\"] = \"test2\"\n        # write object\n        obj_to_json(\"./\", \"test2\", test_obj)\n        # verify if object was written correctly\n        assert json_to_obj(\"test2.json\")[\"data\"] == \"test2\"\n\n    def test_file_to_string(self):\n        test_str = file_to_string(self.test_json_file)\n        # print(test_str)\n        expected_str = '''{\n    \"data\": \"test\"\n}'''\n        assert test_str == expected_str\n\n\nif __name__ == \"__main__\":\n    testUtility = TestUtility()\n    testUtility.test_json_to_obj()\n    testUtility.test_obj_to_json()\n    testUtility.test_file_to_string()\n"
                }
            ]
        }
    ]
}