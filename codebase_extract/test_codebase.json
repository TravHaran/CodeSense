{
    "name": "codesense",
    "type": "folder",
    "keywords": [],
    "children": [
        {
            "name": "keyword_extract",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "keyword_extract.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n\n'''\nCreate a class to extract keywords from text\n- input:\n    - sample text as a string\n-output: \n    - list of keywords\n'''\n\n\nclass KeywordExtract:\n    def __init__(self):\n        self.keywords = []\n        # common english stopwords\n        self.stop_words = set(stopwords.words(\"english\"))\n\n    def extract(self, text):\n        tokens = word_tokenize(text)  # tokenize text\n        filtered_tokens = [word for word in tokens if word.lower(\n        ) not in self.stop_words]  # filter out stopwords\n        # identify keywords with part of speech tagging\n        pos_tags = nltk.pos_tag(filtered_tokens)\n        # keep only nouns, verbs\n        for word, pos in pos_tags:\n            if pos.startswith(\"NN\") or pos.startswith(\"VB\"):\n                self.keywords.append(word)\n        self.keywords = list(set(self.keywords))  # remove duplicates\n        return self.keywords\n\n\nclass TestKeywordExtract:\n    def __init__(self):\n        self.extractor = KeywordExtract()\n        print(\"Testing Keyword Extractor...\\n\")\n\n    def test_extract_keywords_from_query(self):\n        print(\"Testing keywword extraction of user query...\\n\")\n        text = \"I want to modify the maxProfit function to have an initial maxP value of 10\"\n        output = self.extractor.extract(text)\n        print(f\"Keywords from query: {output}\\n\")\n        assert type(output) == list\n\n    def test_extract_keywords_from_annotation(self):\n        print(\"Testing keywword extraction of code annotation...\\n\")\n        text = \"\"\"\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\n\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\n            \"\"\"\n        output = self.extractor.extract(text)\n        print(f\"Keywords from annotation: {output}\\n\")\n        assert type(output) == list\n\n\nif __name__ == \"__main__\":\n    testKeywordExtract = TestKeywordExtract()\n    testKeywordExtract.test_extract_keywords_from_query()\n    testKeywordExtract.test_extract_keywords_from_annotation()\n"
                }
            ]
        },
        {
            "name": "template",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "template.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "'''\nCreate a class to model a character in a video game\n- initialize the class with three parameters\n    - Health\n    - Damage\n    - Speed\n\n- define a mathod to double the speed of the character\n'''\n\nclass Character:\n    def __init__(self, health, damage, speed):\n        self.health = health\n        self.damage = damage\n        self.speed = speed\n    \n    def double_speed(self):\n            self.speed *= 2\n\n\n\n\nwarrior = Character(100, 50, 10)\nninja = Character(80, 40, 40)\n\nprint(f\"Warrior speed: {warrior.speed}\")\nprint(f\"Ninja speed: {ninja.speed}\")\n\nwarrior.double_speed()\n\nprint(f\"Warrior speed: {warrior.speed}\")\n  "
                }
            ]
        },
        {
            "name": "codebase_extract",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "test_codebase.json",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": ""
                },
                {
                    "name": "codebase_extract.py",
                    "type": "file",
                    "keywords": [],
                    "annotation": "",
                    "content": "import os\nimport json\n\n'''\nCreate a class to extract a model of a codebase as a tree\n- input: local directory path as a string\n- output: \n    - json file containing tree structure of directory\n    - at leaf nodes store content of file as a string (if it's content is readable)\n'''\n\n\nclass CodebaseExtract:\n    def __init__(self, path):\n        # Initialize the output dictionary model with folder contents\n        # name, type, keywords, and empty list for children\n        self.path = path\n        self.model = {}\n\n    def file_to_string(self, file_path):  # save file content as string\n        with open(file_path, 'r') as file:\n            file_content = file.read()\n        file.close()\n        return file_content\n\n    def extract(self, path):  # extracts a directory as a json object\n        model = {'name': os.path.basename(path),\n                 'type': 'folder', 'keywords': [], 'children': []}\n        # Check if the path is a directory\n        if not os.path.isdir(path):\n            return model\n\n        # Iterate over the entries in the directory\n        for entry in os.listdir(path):\n            if not entry.startswith('.'):  # ignore hidden folders & files\n                # Create the fill path for current entry\n                entry_path = os.path.join(path, entry)\n                # if the entry is a directory, recursively call the function\n                if os.path.isdir(entry_path):\n                    model['children'].append(self.extract(entry_path))\n                # if the entry is a file, create a dictionary with name and type, keywords, annotation, and content\n                else:\n                    content = \"\"\n                    # save file content as string\n                    try:\n                        content = self.file_to_string(entry_path)\n                    except OSError:\n                        content = \"n/a\"\n                    model['children'].append({'name': entry, 'type': 'file', 'keywords': [\n                    ], 'annotation': \"\", 'content': content})\n        return model\n\n    def model_to_str(self):  # convert codebase json to string\n        output_str = json.dumps(self.model, indent=4)\n        return output_str\n\n    def save_model_json(self, file_name):  # codebase model json file\n        save_file = open(f\"{file_name}.json\", 'w')\n        self.model = self.extract(self.path)\n        json.dump(self.model, save_file, indent=4)\n        save_file.close()\n        print(f\"Codebase model saved as {file_name}\")\n        return self.model\n\n\nclass TestCodebaseExtract:\n    def __init__(self):\n        self.test_path = \"/Users/trav/Documents/projects/codesense\"\n        self.extractor = CodebaseExtract(self.test_path)\n        print(\"Testing Codebase Extractor...\\n\")\n\n    def test_extract_codebase(self):\n        print(\"Testing codebase extraction of current project directory...\\n\")\n        output = self.extractor.save_model_json(\"test_codebase\")\n        # model_str = self.extractor.model_to_str()\n        # print(f\"Codebase model: {model_str}\")\n        assert type(output) == dict\n\n\nif __name__ == \"__main__\":\n    testCodebaseExtract = TestCodebaseExtract()\n    testCodebaseExtract.test_extract_codebase()\n"
                }
            ]
        },
        {
            "name": "README.md",
            "type": "file",
            "keywords": [],
            "annotation": "",
            "content": "# Project Codesense\n\n## Breakdown\n\n### 1. CodeBase Tree Extraction\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\n### 2. Call Graph Extraction\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\n### 3. Annotation Generation\n    - for a fucntion defined in code generate a text summarization\n### 4. Annotation Aggregation\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\n### 5. Keyword Extraction\n    - from the aggregated annotation report extract a list of keywords\n    - from a usery query extract a list of keywords\n### 6. Tree Traversal\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\n    - once the target node has been found, return it's corresponding aggregated annotation report\n### 7. Question Answering\n    - given the aggregated annoation report as context, provide an answer to the user's query."
        },
        {
            "name": "tree_traverse",
            "type": "folder",
            "keywords": [],
            "children": []
        },
        {
            "name": "extras",
            "type": "folder",
            "keywords": [],
            "children": [
                {
                    "name": "keyword_extraction",
                    "type": "folder",
                    "keywords": [],
                    "children": [
                        {
                            "name": "info.txt",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "install RAKE\n`pip3 install --user rake-nltk`\n\ninstall supporting nltk packages\n`python3 -c \"import nltk; nltk.download('punkt'); nltk.download('stopwords')\"`\n\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\n\n`bash '/Applications/Python 3.9/Install Certificates.command'`\n\nto use word2vec install gensim library\n`pip3 install gensim`\n\n\n\n\n\n"
                        },
                        {
                            "name": "main.py",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "import nltk\nimport gensim.downloader\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nimport warnings\n\ninput_text1 = \"\"\"\nI want to modify the maxProfit function to have an initial maxP value of 10\n\"\"\"\n\ninput_text2 = \"\"\"\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\n\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\n\"\"\"\n\n#######################extract keywords#######################\n\n#download necessary resources\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download(\"punkt\")\n# nltk.download(\"stopwords\")\n\ndef extract_keywords(text):\n    #tokenize the text into words\n    tokens = word_tokenize(text)\n    #define a set of common English stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n    keywords = []\n    #identify keywords using part-of-speech tagging\n    pos_tags = nltk.pos_tag(filtered_tokens)\n    #keep only nouns, proper nouns, and verbs\n    for word, pos in pos_tags:\n        if pos.startswith(\"NN\") or pos.startswith(\"VB\"):\n            keywords.append(word)\n    unique_keywords = list(set(keywords))\n    return unique_keywords\n\n# print(extract_keywords(input_text1))\n\n#######################compute the similarity between keywords#######################\n\nwarnings.filterwarnings(action='ignore')\n#  Reads \u2018context.txt\u2019 file (for our application this will be the aggrgated summary report for a code file)\nsample = open(\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\")\ns = sample.read()\n# Replaces escape character with space\nf = s.replace(\"\\n\", \" \")\ndata = []\n# iterate through each sentence in the file\nfor i in sent_tokenize(f):\n    temp = []\n    # tokenize the sentence into words\n    for j in word_tokenize(i):\n        temp.append(j.lower())\n    data.append(temp)\nmodel = gensim.models.Word2Vec(data, min_count=1,\n                                vector_size=100, window=5, sg=1)\n\ndef compare_words(w1, w2):\n    if w1 == w2:\n        return 1\n    if w1 in model.wv and w2 in model.wv:\n        return model.wv.similarity(w1, w2)\n    else:\n        return 0\n\ndef compare_keywords(l1, l2):\n    output = 0\n    for word1 in l1:\n        word1 = word1.lower()\n        for word2 in l2:\n            output += compare_words(word1, word2.lower())\n    return output\n\nlist1 = extract_keywords(input_text1)\nlist2 = extract_keywords(input_text2)\nprint(compare_keywords(list1, list2))\n"
                        },
                        {
                            "name": "context.txt",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\nNike is a sports apparel company. It's brand is recognized accross the country\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity."
                        }
                    ]
                },
                {
                    "name": "codebase_extraction",
                    "type": "folder",
                    "keywords": [],
                    "children": [
                        {
                            "name": "codebase.json",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "{\n    \"name\": \"codesense\",\n    \"type\": \"folder\",\n    \"children\": [\n        {\n            \"name\": \"keyword_extract\",\n            \"type\": \"folder\",\n            \"children\": [\n                {\n                    \"name\": \"keyword_extract.py\",\n                    \"type\": \"file\",\n                    \"content\": \"import nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n\\n'''\\nCreate a class to extract keywords from text\\n- input:\\n    - sample text as a string\\n-output: \\n    - list of keywords\\n'''\\n\\n\\nclass KeywordExtract:\\n    def __init__(self):\\n        self.keywords = []\\n        # common english stopwords\\n        self.stop_words = set(stopwords.words(\\\"english\\\"))\\n\\n    def extract(self, text):\\n        tokens = word_tokenize(text)  # tokenize text\\n        filtered_tokens = [word for word in tokens if word.lower(\\n        ) not in self.stop_words]  # filter out stopwords\\n        # identify keywords with part of speech tagging\\n        pos_tags = nltk.pos_tag(filtered_tokens)\\n        # keep only nouns, verbs\\n        for word, pos in pos_tags:\\n            if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n                self.keywords.append(word)\\n        self.keywords = list(set(self.keywords))  # remove duplicates\\n        return self.keywords\\n\\n\\nclass TestKeywordExtract:\\n    def __init__(self):\\n        self.extractor = KeywordExtract()\\n        print(\\\"Testing Keyword Extractor...\\\\n\\\")\\n\\n    def test_extract_keywords_from_query(self):\\n        print(\\\"Testing keywword extraction of user query...\\\\n\\\")\\n        text = \\\"I want to modify the maxProfit function to have an initial maxP value of 10\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from query: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n    def test_extract_keywords_from_annotation(self):\\n        print(\\\"Testing keywword extraction of code annotation...\\\\n\\\")\\n        text = \\\"\\\"\\\"\\n            The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\n            Initialize Profit: It initializes maxP, the maximum profit, to 0.\\n            Iterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\n            Calculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\n            Return Profit: The function returns the accumulated maxP as the maximum profit.\\n            Overall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n            \\\"\\\"\\\"\\n        output = self.extractor.extract(text)\\n        print(f\\\"Keywords from annotation: {output}\\\\n\\\")\\n        assert type(output) == list\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    testKeywordExtract = TestKeywordExtract()\\n    testKeywordExtract.test_extract_keywords_from_query()\\n    testKeywordExtract.test_extract_keywords_from_annotation()\\n\"\n                }\n            ]\n        },\n        {\n            \"name\": \"template\",\n            \"type\": \"folder\",\n            \"children\": [\n                {\n                    \"name\": \"template.py\",\n                    \"type\": \"file\",\n                    \"content\": \"'''\\nCreate a class to model a character in a video game\\n- initialize the class with three parameters\\n    - Health\\n    - Damage\\n    - Speed\\n\\n- define a mathod to double the speed of the character\\n'''\\n\\nclass Character:\\n    def __init__(self, health, damage, speed):\\n        self.health = health\\n        self.damage = damage\\n        self.speed = speed\\n    \\n    def double_speed(self):\\n            self.speed *= 2\\n\\n\\n\\n\\nwarrior = Character(100, 50, 10)\\nninja = Character(80, 40, 40)\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\nprint(f\\\"Ninja speed: {ninja.speed}\\\")\\n\\nwarrior.double_speed()\\n\\nprint(f\\\"Warrior speed: {warrior.speed}\\\")\\n  \"\n                }\n            ]\n        },\n        {\n            \"name\": \"codebase_extract\",\n            \"type\": \"folder\",\n            \"children\": [\n                {\n                    \"name\": \"codebase_extract.py\",\n                    \"type\": \"file\",\n                    \"content\": \"'''\\nCreate a class to extract a model of a codebase as a tree\\n- input: local directory path as a string\\n- output: \\n    - json file containing tree structure of directory\\n    - at leaf nodes store content of file as a string (if it's content is readable)\\n'''\\n\\nclass CodebaseExtract:\\n    def __init__(self):\\n        self.model\\n    \\n    def extract(self, path):\\n        return self.model\\n\\nclass TestCodebaseExtract:\\n    def __init__(self):\\n        self.extractor = CodebaseExtract()\\n        print(\\\"Testing Codebase Extractor...\\\\n\\\")\\n    \\n    def test_extract_codebase(self):\\n        print(\\\"Testing codebase extraction of current project directory...\\\\n\\\")\\n        path = \\\"/Users/trav/Documents/projects/codesense\\\"\\n        output = self.extractor.extract(path)\\n        assert type(output) == json\\n        \"\n                }\n            ]\n        },\n        {\n            \"name\": \"README.md\",\n            \"type\": \"file\",\n            \"content\": \"# Project Codesense\\n\\n## Breakdown\\n\\n### 1. CodeBase Tree Extraction\\n    - for a given codebase generate a k-ary tree outlining the directories, subdirectories, all the way down to the actual code files as the leaf nodes\\n### 2. Call Graph Extraction\\n    - for a source code file generate a directed graph representing the flow of execution calls for functions defined in the code file\\n### 3. Annotation Generation\\n    - for a fucntion defined in code generate a text summarization\\n### 4. Annotation Aggregation\\n    - for a source-code file modeled as a call graph with annnotations at each node, output an aggregated report of all the annotations\\n### 5. Keyword Extraction\\n    - from the aggregated annotation report extract a list of keywords\\n    - from a usery query extract a list of keywords\\n### 6. Tree Traversal\\n    - given a list of target keywords traverse the code-base tree until you find the node with the most matching keywords\\n    - once the target node has been found, return it's corresponding aggregated annotation report\\n### 7. Question Answering\\n    - given the aggregated annoation report as context, provide an answer to the user's query.\"\n        },\n        {\n            \"name\": \"extras\",\n            \"type\": \"folder\",\n            \"children\": [\n                {\n                    \"name\": \"keyword_extraction\",\n                    \"type\": \"folder\",\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"content\": \"install RAKE\\n`pip3 install --user rake-nltk`\\n\\ninstall supporting nltk packages\\n`python3 -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"`\\n\\nif you get a ssl certificate error run following command (change python version to which ever one you're running). Then rerun the above command\\n\\n`bash '/Applications/Python 3.9/Install Certificates.command'`\\n\\nto use word2vec install gensim library\\n`pip3 install gensim`\\n\\n\\n\\n\\n\\n\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"content\": \"import nltk\\nimport gensim.downloader\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nimport warnings\\n\\ninput_text1 = \\\"\\\"\\\"\\nI want to modify the maxProfit function to have an initial maxP value of 10\\n\\\"\\\"\\\"\\n\\ninput_text2 = \\\"\\\"\\\"\\nThe maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\n\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\\n\\\"\\\"\\\"\\n\\n#######################extract keywords#######################\\n\\n#download necessary resources\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download(\\\"punkt\\\")\\n# nltk.download(\\\"stopwords\\\")\\n\\ndef extract_keywords(text):\\n    #tokenize the text into words\\n    tokens = word_tokenize(text)\\n    #define a set of common English stopwords\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    #filter out stopwords and keep significant words(i.e. nouns, verbs)\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n    keywords = []\\n    #identify keywords using part-of-speech tagging\\n    pos_tags = nltk.pos_tag(filtered_tokens)\\n    #keep only nouns, proper nouns, and verbs\\n    for word, pos in pos_tags:\\n        if pos.startswith(\\\"NN\\\") or pos.startswith(\\\"VB\\\"):\\n            keywords.append(word)\\n    unique_keywords = list(set(keywords))\\n    return unique_keywords\\n\\n# print(extract_keywords(input_text1))\\n\\n#######################compute the similarity between keywords#######################\\n\\nwarnings.filterwarnings(action='ignore')\\n#  Reads \\u2018context.txt\\u2019 file (for our application this will be the aggrgated summary report for a code file)\\nsample = open(\\\"/Users/trav/Documents/projects/codesense/keyword_extraction/context.txt\\\")\\ns = sample.read()\\n# Replaces escape character with space\\nf = s.replace(\\\"\\\\n\\\", \\\" \\\")\\ndata = []\\n# iterate through each sentence in the file\\nfor i in sent_tokenize(f):\\n    temp = []\\n    # tokenize the sentence into words\\n    for j in word_tokenize(i):\\n        temp.append(j.lower())\\n    data.append(temp)\\nmodel = gensim.models.Word2Vec(data, min_count=1,\\n                                vector_size=100, window=5, sg=1)\\n\\ndef compare_words(w1, w2):\\n    if w1 == w2:\\n        return 1\\n    if w1 in model.wv and w2 in model.wv:\\n        return model.wv.similarity(w1, w2)\\n    else:\\n        return 0\\n\\ndef compare_keywords(l1, l2):\\n    output = 0\\n    for word1 in l1:\\n        word1 = word1.lower()\\n        for word2 in l2:\\n            output += compare_words(word1, word2.lower())\\n    return output\\n\\nlist1 = extract_keywords(input_text1)\\nlist2 = extract_keywords(input_text2)\\nprint(compare_keywords(list1, list2))\\n\"\n                        },\n                        {\n                            \"name\": \"context.txt\",\n                            \"type\": \"file\",\n                            \"content\": \"The maxProfit function is part of a C++ class Solution. It calculates the maximum profit that can be made from a list of stock prices (prices). The function follows these steps:\\nNike is a sports apparel company. It's brand is recognized accross the country\\nInitialize Profit: It initializes maxP, the maximum profit, to 0.\\nIterate Through Prices: It loops through the list of prices from the second element (index 1) to the end.\\nCalculate Profit: If the current price is higher than the previous price, it calculates the profit by subtracting the previous price from the current price and adds it to maxP.\\nReturn Profit: The function returns the accumulated maxP as the maximum profit.\\nOverall, this function implements a simple algorithm for finding the total profit from multiple price increases in a stock price list, where each increase represents a buy-and-sell opportunity.\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"codebase_extraction\",\n                    \"type\": \"folder\",\n                    \"children\": [\n                        {\n                            \"name\": \"codebase.json\",\n                            \"type\": \"file\",\n                            \"content\": \"{\\n    \\\"name\\\": \\\"codesense\\\",\\n    \\\"type\\\": \\\"folder\\\",\\n    \\\"children\\\": [\\n        {\\n            \\\"name\\\": \\\"keyword_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"template\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"template.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"codebase_extract\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"codebase_extract.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                }\\n            ]\\n        },\\n        {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"extras\\\",\\n            \\\"type\\\": \\\"folder\\\",\\n            \\\"children\\\": [\\n                {\\n                    \\\"name\\\": \\\"keyword_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"context.txt\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"codebase_extraction\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"codebase.json\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                        }\\n                    ]\\n                },\\n                {\\n                    \\\"name\\\": \\\"annotation_generation\\\",\\n                    \\\"type\\\": \\\"folder\\\",\\n                    \\\"children\\\": [\\n                        {\\n                            \\\"name\\\": \\\"info.txt\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                        },\\n                        {\\n                            \\\"name\\\": \\\"main.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"content\": \"import os\\nimport json\\n\\ndef create_folder_structure_json(path):\\n    # Initialize the result dictionary with folder\\n    # name, type, and an empty list for children\\n    result = {'name': os.path.basename(path),\\n              'type': 'folder', 'children': []}\\n    \\n    # Check if the path is a directory\\n    if not os.path.isdir(path):\\n        return result\\n    \\n    # Iterate over the entries in the directory\\n    for entry in os.listdir(path):\\n        if not entry.startswith('.'): # ignore hidden folders & files\\n            # Create the full path for current entry\\n            entry_path = os.path.join(path, entry)\\n            \\n            #if the entry is a directory, recursively call the function\\n            if os.path.isdir(entry_path):\\n                result['children'].append(create_folder_structure_json(entry_path))\\n            # if the entry is a file, create a dictionary with name and type\\n            else:\\n                try:\\n                    content = file_to_string(entry_path)\\n                except OSError:\\n                    content = \\\"n/a\\\"\\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\\n    return result\\n\\ndef file_to_string(file_path):\\n    with open(file_path, 'r') as file:\\n        file_content = file.read()\\n    file.close()\\n    return file_content\\n# Specify the path to the folder you want to create the JSON for\\nfolder_path = '/Users/trav/Documents/projects/codesense'\\n\\n# Call the function to create the JSON representation\\nfolder_json = create_folder_structure_json(folder_path)\\n\\n# Convert the dictionary to a JSON string with indentation\\nfolder_json_str = json.dumps(folder_json, indent=4)\\n\\n# Print the JSON representation of the folder structure\\nprint(folder_json_str)\\n\\n# Save as a JSON file\\nsave_file = open(\\\"codebase.json\\\", 'w')\\njson.dump(folder_json, save_file, indent=4)\\nsave_file.close()\\n\\n\\n    \"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"annotation_generation\",\n                    \"type\": \"folder\",\n                    \"children\": [\n                        {\n                            \"name\": \"info.txt\",\n                            \"type\": \"file\",\n                            \"content\": \"to run codellama model install transformers\\n`pip install transformers accelerate`\"\n                        },\n                        {\n                            \"name\": \"main.py\",\n                            \"type\": \"file\",\n                            \"content\": \"from transformers import pipeline\\n\\n# Load Llama 3 model from Hugging Face\\nllama3_model = pipeline(\\\"text-generation\\\", model=\\\"meta-llama/Meta-Llama-3-8B\\\")\\n\\n# Generate text using the Llama 3 model\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n\\n\\n\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
                        },
                        {
                            "name": "main.py",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "import os\nimport json\n\ndef create_folder_structure_json(path):\n    # Initialize the result dictionary with folder\n    # name, type, and an empty list for children\n    result = {'name': os.path.basename(path),\n              'type': 'folder', 'children': []}\n    \n    # Check if the path is a directory\n    if not os.path.isdir(path):\n        return result\n    \n    # Iterate over the entries in the directory\n    for entry in os.listdir(path):\n        if not entry.startswith('.'): # ignore hidden folders & files\n            # Create the full path for current entry\n            entry_path = os.path.join(path, entry)\n            \n            #if the entry is a directory, recursively call the function\n            if os.path.isdir(entry_path):\n                result['children'].append(create_folder_structure_json(entry_path))\n            # if the entry is a file, create a dictionary with name and type\n            else:\n                # save file content as string\n                try:\n                    content = file_to_string(entry_path)\n                except OSError:\n                    content = \"n/a\"\n                result['children'].append({'name': entry, 'type': 'file', 'content': content})\n    return result\n\ndef file_to_string(file_path):\n    with open(file_path, 'r') as file:\n        file_content = file.read()\n    file.close()\n    return file_content\n# Specify the path to the folder you want to create the JSON for\nfolder_path = '/Users/trav/Documents/projects/codesense'\n\n# Call the function to create the JSON representation\nfolder_json = create_folder_structure_json(folder_path)\n\n# Convert the dictionary to a JSON string with indentation\nfolder_json_str = json.dumps(folder_json, indent=4)\n\n# Print the JSON representation of the folder structure\nprint(folder_json_str)\n\n# Save as a JSON file\nsave_file = open(\"codebase.json\", 'w')\njson.dump(folder_json, save_file, indent=4)\nsave_file.close()\n\n\n    "
                        }
                    ]
                },
                {
                    "name": "annotation_generation",
                    "type": "folder",
                    "keywords": [],
                    "children": [
                        {
                            "name": "info.txt",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "to run codellama model install transformers\n`pip install transformers accelerate`"
                        },
                        {
                            "name": "main.py",
                            "type": "file",
                            "keywords": [],
                            "annotation": "",
                            "content": "from transformers import pipeline\n\n# Load Llama 3 model from Hugging Face\nllama3_model = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B\")\n\n# Generate text using the Llama 3 model\nprompt = \"Once upon a time\"\ngenerated_text = llama3_model(prompt, max_length=50, do_sample=True)\n\n# Print the generated text\nprint(generated_text[0]['generated_text'])\n\n\n"
                        }
                    ]
                }
            ]
        }
    ]
}